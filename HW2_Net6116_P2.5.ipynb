{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79aafe63-fda7-412e-8b9e-c6f0555a3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random \n",
    "from itertools import combinations, permutations\n",
    "from math import comb\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112c66fa-f306-4bb1-b808-9848d186fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate graphs\n",
    "\n",
    "def gen_g(n):\n",
    "    G = nx.Graph()\n",
    "    eg = [(1,2)]\n",
    "    G.add_edge(1, 2)\n",
    "\n",
    "    new_node = 3\n",
    "    while new_node <= n:\n",
    "        random_e = random.choice(eg)\n",
    "        eg.append((random_e[0], new_node))\n",
    "        eg.append((random_e[1], new_node))\n",
    "        new_node += 1\n",
    "\n",
    "    G.add_edges_from(eg)\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3c16add-5442-4acb-84ff-6928d80e5790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "086289df-a045-41c5-b6ee-beb02db7953e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for graph 0 of size 50000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/116658171.py:28: RuntimeWarning: divide by zero encountered in log\n",
      "  logs = np.log(ordered_data)\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/116658171.py:29: RuntimeWarning: invalid value encountered in subtract\n",
      "  differences = logs[:-1] - logs[1:]\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/116658171.py:85: RuntimeWarning: divide by zero encountered in log\n",
      "  logs = np.log(ordered_data)\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/116658171.py:86: RuntimeWarning: invalid value encountered in subtract\n",
      "  differences = logs[:-1] - logs[1:]\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/116658171.py:58: RuntimeWarning: invalid value encountered in true_divide\n",
      "  xi_arr = gamma_pos -1. + q2/q1\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/116658171.py:121: RuntimeWarning: invalid value encountered in true_divide\n",
      "  xi_arr = gamma_pos - 1. + q2/q1\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/116658171.py:46: RuntimeWarning: invalid value encountered in subtract\n",
      "  gamma_pos = (15./(8*h_arr))*t1[max_i_vector]\\\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/116658171.py:50: RuntimeWarning: invalid value encountered in subtract\n",
      "  q1 = (15./(8*h_arr))*t4[max_i_vector]\\\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/116658171.py:54: RuntimeWarning: invalid value encountered in subtract\n",
      "  q2 = (15.*(1+alpha)/(8*h_arr))*t4[max_i_vector]\\\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/116658171.py:106: RuntimeWarning: invalid value encountered in subtract\n",
      "  gamma_pos = (35./(16*h_arr))*t1[max_i_vector]\\\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/116658171.py:111: RuntimeWarning: invalid value encountered in subtract\n",
      "  q1 = (35./(16*h_arr))*t5[max_i_vector]\\\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/116658171.py:116: RuntimeWarning: invalid value encountered in subtract\n",
      "  q2 = (35.*(1+alpha)/(16*h_arr))*t5[max_i_vector] \\\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:19: RuntimeWarning: divide by zero encountered in log\n",
      "  logs_1 = np.log(ordered_data)\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:22: RuntimeWarning: invalid value encountered in subtract\n",
      "  M1 = (1./k_vector)*logs_1_cumsum - logs_1[1:]\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:43: RuntimeWarning: divide by zero encountered in log\n",
      "  logs_1 = np.log(ordered_data)\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:44: RuntimeWarning: divide by zero encountered in log\n",
      "  logs_2 = (np.log(ordered_data))**2\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:48: RuntimeWarning: invalid value encountered in subtract\n",
      "  M1 = (1./k_vector)*logs_1_cumsum - logs_1[1:]\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:49: RuntimeWarning: invalid value encountered in subtract\n",
      "  M2 = (1./k_vector)*logs_2_cumsum - (2.*logs_1[1:]/k_vector)*logs_1_cumsum\\\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:154: RuntimeWarning: invalid value encountered in subtract\n",
      "  current_amse1 = (M2 - 2.*(M1)**2)**2\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:174: RuntimeWarning: invalid value encountered in subtract\n",
      "  current_amse2 = (M2 - 2.*(M1**2))**2\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/997918134.py:231: RuntimeWarning: invalid value encountered in true_divide\n",
      "  xi_arr = M1 + 1. - 0.5*(1. - (M1*M1)/M2)**(-1)\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:73: RuntimeWarning: divide by zero encountered in log\n",
      "  logs_1 = np.log(ordered_data)\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:74: RuntimeWarning: divide by zero encountered in log\n",
      "  logs_2 = (np.log(ordered_data))**2\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:75: RuntimeWarning: divide by zero encountered in log\n",
      "  logs_3 = (np.log(ordered_data))**3\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:80: RuntimeWarning: invalid value encountered in subtract\n",
      "  M1 = (1./k_vector)*logs_1_cumsum - logs_1[1:]\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:81: RuntimeWarning: invalid value encountered in subtract\n",
      "  M2 = (1./k_vector)*logs_2_cumsum - (2.*logs_1[1:]/k_vector)*logs_1_cumsum\\\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:83: RuntimeWarning: invalid value encountered in subtract\n",
      "  M3 = (1./k_vector)*logs_3_cumsum - (3.*logs_1[1:]/k_vector)*logs_2_cumsum\\\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:86: RuntimeWarning: invalid value encountered in true_divide\n",
      "  clean_indices = np.where((M2 <= 0) | (M3 == 0) | (np.abs(1.-(M1**2)/M2) < 1e-10)\\\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:87: RuntimeWarning: invalid value encountered in true_divide\n",
      "  |(np.abs(1.-(M1*M2)/M3) < 1e-10))\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/997918134.py:135: RuntimeWarning: invalid value encountered in true_divide\n",
      "  xi_2 = M1 + 1. - 0.5*((1. - (M1*M1)/M2))**(-1.)\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/997918134.py:136: RuntimeWarning: invalid value encountered in true_divide\n",
      "  xi_3 = np.sqrt(0.5*M2) + 1. - (2./3.)*(1. / (1. - M1*M2/M3))\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/997918134.py:155: RuntimeWarning: invalid value encountered in true_divide\n",
      "  xi_2 = M1 + 1. - 0.5*(1. - (M1*M1)/M2)**(-1.)\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/997918134.py:156: RuntimeWarning: invalid value encountered in true_divide\n",
      "  xi_3 = np.sqrt(0.5*M2) + 1. - (2./3.)*(1. / (1. - M1*M2/M3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for graph 1 of size 50000.0\n",
      "Calculating for graph 2 of size 50000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:87: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  |(np.abs(1.-(M1*M2)/M3) < 1e-10))\n",
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/3079702889.py:86: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  clean_indices = np.where((M2 <= 0) | (M3 == 0) | (np.abs(1.-(M1**2)/M2) < 1e-10)\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for graph 3 of size 50000.0\n",
      "WARNING(moments): estimated k2 is greater than k1! Re-doing bootstrap...\n",
      "Resampling...\n",
      "Calculating for graph 4 of size 50000.0\n",
      "Calculating for graph 5 of size 50000.0\n",
      "Calculating for graph 6 of size 50000.0\n",
      "Calculating for graph 7 of size 50000.0\n",
      "Calculating for graph 8 of size 50000.0\n",
      "Calculating for graph 9 of size 50000.0\n",
      "Calculating for graph 10 of size 50000.0\n",
      "Calculating for graph 11 of size 50000.0\n",
      "Calculating for graph 12 of size 50000.0\n",
      "Calculating for graph 13 of size 50000.0\n",
      "Calculating for graph 14 of size 50000.0\n",
      "Calculating for graph 15 of size 50000.0\n",
      "WARNING(moments): estimated k2 is greater than k1! Re-doing bootstrap...\n",
      "Resampling...\n",
      "Calculating for graph 16 of size 50000.0\n",
      "Calculating for graph 17 of size 50000.0\n",
      "Calculating for graph 18 of size 50000.0\n",
      "Calculating for graph 19 of size 50000.0\n",
      "Calculating for graph 0 of size 60000.0\n",
      "Calculating for graph 1 of size 60000.0\n",
      "Calculating for graph 2 of size 60000.0\n",
      "Calculating for graph 3 of size 60000.0\n",
      "Calculating for graph 4 of size 60000.0\n",
      "Calculating for graph 5 of size 60000.0\n",
      "Calculating for graph 6 of size 60000.0\n",
      "Calculating for graph 7 of size 60000.0\n",
      "Calculating for graph 8 of size 60000.0\n",
      "Calculating for graph 9 of size 60000.0\n",
      "Calculating for graph 10 of size 60000.0\n",
      "Calculating for graph 11 of size 60000.0\n",
      "Calculating for graph 12 of size 60000.0\n",
      "Calculating for graph 13 of size 60000.0\n",
      "Calculating for graph 14 of size 60000.0\n",
      "Calculating for graph 15 of size 60000.0\n",
      "Calculating for graph 16 of size 60000.0\n",
      "Calculating for graph 17 of size 60000.0\n",
      "Calculating for graph 18 of size 60000.0\n",
      "Calculating for graph 19 of size 60000.0\n",
      "Calculating for graph 0 of size 500000.0\n",
      "Calculating for graph 1 of size 500000.0\n",
      "Calculating for graph 2 of size 500000.0\n",
      "Calculating for graph 3 of size 500000.0\n",
      "Calculating for graph 4 of size 500000.0\n",
      "Calculating for graph 5 of size 500000.0\n",
      "Calculating for graph 6 of size 500000.0\n",
      "Calculating for graph 7 of size 500000.0\n",
      "Calculating for graph 8 of size 500000.0\n",
      "Calculating for graph 9 of size 500000.0\n",
      "Calculating for graph 10 of size 500000.0\n",
      "Calculating for graph 11 of size 500000.0\n",
      "Calculating for graph 12 of size 500000.0\n",
      "Calculating for graph 13 of size 500000.0\n",
      "Calculating for graph 14 of size 500000.0\n",
      "Calculating for graph 15 of size 500000.0\n",
      "Calculating for graph 16 of size 500000.0\n",
      "Calculating for graph 17 of size 500000.0\n",
      "Calculating for graph 18 of size 500000.0\n",
      "Calculating for graph 19 of size 500000.0\n",
      "Calculating for graph 0 of size 600000.0\n",
      "WARNING(moments): estimated k2 is greater than k1! Re-doing bootstrap...\n",
      "Resampling...\n",
      "Calculating for graph 1 of size 600000.0\n",
      "Calculating for graph 2 of size 600000.0\n",
      "Calculating for graph 3 of size 600000.0\n",
      "Calculating for graph 4 of size 600000.0\n",
      "Calculating for graph 5 of size 600000.0\n",
      "Calculating for graph 6 of size 600000.0\n",
      "Calculating for graph 7 of size 600000.0\n",
      "Calculating for graph 8 of size 600000.0\n",
      "Calculating for graph 9 of size 600000.0\n",
      "Calculating for graph 10 of size 600000.0\n",
      "Calculating for graph 11 of size 600000.0\n",
      "Calculating for graph 12 of size 600000.0\n",
      "Calculating for graph 13 of size 600000.0\n",
      "Calculating for graph 14 of size 600000.0\n",
      "Calculating for graph 15 of size 600000.0\n",
      "Calculating for graph 16 of size 600000.0\n",
      "Calculating for graph 17 of size 600000.0\n",
      "WARNING(moments): estimated k2 is greater than k1! Re-doing bootstrap...\n",
      "Resampling...\n",
      "Calculating for graph 18 of size 600000.0\n",
      "Calculating for graph 19 of size 600000.0\n",
      "Calculating for graph 0 of size 5000000.0\n",
      "Calculating for graph 1 of size 5000000.0\n",
      "Calculating for graph 2 of size 5000000.0\n",
      "Calculating for graph 3 of size 5000000.0\n",
      "Calculating for graph 4 of size 5000000.0\n",
      "Calculating for graph 5 of size 5000000.0\n",
      "Calculating for graph 6 of size 5000000.0\n",
      "Calculating for graph 7 of size 5000000.0\n",
      "Calculating for graph 8 of size 5000000.0\n",
      "Calculating for graph 9 of size 5000000.0\n",
      "Calculating for graph 10 of size 5000000.0\n",
      "Calculating for graph 11 of size 5000000.0\n",
      "Calculating for graph 12 of size 5000000.0\n",
      "Calculating for graph 13 of size 5000000.0\n",
      "Calculating for graph 14 of size 5000000.0\n",
      "Calculating for graph 15 of size 5000000.0\n",
      "Calculating for graph 16 of size 5000000.0\n",
      "Calculating for graph 17 of size 5000000.0\n",
      "Calculating for graph 18 of size 5000000.0\n",
      "Calculating for graph 19 of size 5000000.0\n",
      "Calculating for graph 0 of size 6000000.0\n",
      "Calculating for graph 1 of size 6000000.0\n",
      "Calculating for graph 2 of size 6000000.0\n",
      "Calculating for graph 3 of size 6000000.0\n",
      "Calculating for graph 4 of size 6000000.0\n",
      "Calculating for graph 5 of size 6000000.0\n",
      "Calculating for graph 6 of size 6000000.0\n",
      "Calculating for graph 7 of size 6000000.0\n",
      "Calculating for graph 8 of size 6000000.0\n",
      "Calculating for graph 9 of size 6000000.0\n",
      "Calculating for graph 10 of size 6000000.0\n",
      "Calculating for graph 11 of size 6000000.0\n",
      "Calculating for graph 12 of size 6000000.0\n",
      "Calculating for graph 13 of size 6000000.0\n",
      "Calculating for graph 14 of size 6000000.0\n",
      "Calculating for graph 15 of size 6000000.0\n",
      "Calculating for graph 16 of size 6000000.0\n",
      "Calculating for graph 17 of size 6000000.0\n",
      "Calculating for graph 18 of size 6000000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/2jygx_494rq5s1llg74g9nhw0000gn/T/ipykernel_16258/997918134.py:231: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  xi_arr = M1 + 1. - 0.5*(1. - (M1*M1)/M2)**(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for graph 19 of size 6000000.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "g_sizes = [5*10e3, 6*10e3, 5*10e4, 6*10e4, 5*10e5, 6*10e5]\n",
    "ke_all = {}\n",
    "he_all = {}\n",
    "me_all = {}\n",
    "\n",
    "for n in g_sizes:\n",
    "    ke = []\n",
    "    he = []\n",
    "    me = []\n",
    "    for i in list(range(20)):\n",
    "        print(\"Calculating for graph\", i, \"of size\", n)\n",
    "        G = gen_g(n)\n",
    "        ds = sorted(nx.degree_histogram(G),reverse = True)\n",
    "        for deg_cnt in ds:\n",
    "            deg += 1\n",
    "            dt = (deg, deg_cnt)\n",
    "            net.append(dt)\n",
    "        ke.append(kernel_type_estimator(ds, 100))\n",
    "        he.append(hill_estimator(ds))\n",
    "        me.append(moments_estimator(ds))\n",
    "    ke_all[n] = ke\n",
    "    he_all[n] = he\n",
    "    me_all[n] = me\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7279b5ad-0875-448c-94d3-ee5b4a766e65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50000.0</th>\n",
       "      <td>1.449636</td>\n",
       "      <td>0.023522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000.0</th>\n",
       "      <td>1.455284</td>\n",
       "      <td>0.049479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500000.0</th>\n",
       "      <td>1.391590</td>\n",
       "      <td>0.007680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600000.0</th>\n",
       "      <td>1.390844</td>\n",
       "      <td>0.013116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000000.0</th>\n",
       "      <td>1.368401</td>\n",
       "      <td>0.010804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000000.0</th>\n",
       "      <td>1.364932</td>\n",
       "      <td>0.004883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean       std\n",
       "50000.0    1.449636  0.023522\n",
       "60000.0    1.455284  0.049479\n",
       "500000.0   1.391590  0.007680\n",
       "600000.0   1.390844  0.013116\n",
       "5000000.0  1.368401  0.010804\n",
       "6000000.0  1.364932  0.004883"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ke_all = pd.DataFrame(ke_all)\n",
    "# ke_des = ke_all.describe()\n",
    "ke_dt = ke_des.loc[['mean', 'std'], :].T\n",
    "ke_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88cea32a-d286-4b09-9aac-690be8772b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50000.0</th>\n",
       "      <td>1.440563</td>\n",
       "      <td>0.025321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000.0</th>\n",
       "      <td>1.423854</td>\n",
       "      <td>0.018213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500000.0</th>\n",
       "      <td>1.382500</td>\n",
       "      <td>0.009921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600000.0</th>\n",
       "      <td>1.376129</td>\n",
       "      <td>0.012674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000000.0</th>\n",
       "      <td>1.360003</td>\n",
       "      <td>0.009855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000000.0</th>\n",
       "      <td>1.359825</td>\n",
       "      <td>0.009399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean       std\n",
       "50000.0    1.440563  0.025321\n",
       "60000.0    1.423854  0.018213\n",
       "500000.0   1.382500  0.009921\n",
       "600000.0   1.376129  0.012674\n",
       "5000000.0  1.360003  0.009855\n",
       "6000000.0  1.359825  0.009399"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# me_all = pd.DataFrame(me_all)\n",
    "# me_des = me_all.describe()\n",
    "me_dt = me_des.loc[['mean', 'std'], :].T\n",
    "me_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7db1b9e3-9c55-45a0-89d4-5ecfde287db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50000.0</th>\n",
       "      <td>1.440563</td>\n",
       "      <td>0.025321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60000.0</th>\n",
       "      <td>1.423854</td>\n",
       "      <td>0.018213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500000.0</th>\n",
       "      <td>1.382500</td>\n",
       "      <td>0.009921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600000.0</th>\n",
       "      <td>1.376129</td>\n",
       "      <td>0.012674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000000.0</th>\n",
       "      <td>1.360003</td>\n",
       "      <td>0.009855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000000.0</th>\n",
       "      <td>1.359825</td>\n",
       "      <td>0.009399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean       std\n",
       "50000.0    1.440563  0.025321\n",
       "60000.0    1.423854  0.018213\n",
       "500000.0   1.382500  0.009921\n",
       "600000.0   1.376129  0.012674\n",
       "5000000.0  1.360003  0.009855\n",
       "6000000.0  1.359825  0.009399"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# he_all = pd.DataFrame(me_all)\n",
    "# he_des = he_all.describe()\n",
    "he_dt = he_des.loc[['mean', 'std'], :].T\n",
    "he_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "997ebc12-866b-41fa-bd0f-5829b9d1e27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Compare three tail estimation on 6 graphs of different sizes')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAp8AAAKXCAYAAADEuyFlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAACH2klEQVR4nOzdd3xN9+PH8dfnZogIYtYqMRM7iE1rb2p1alW1VW3p+Hbo0E2HLtWl2qJDVUuNokWN2iUUtWeM2iNNIyKSfH5/3MsvNAiSezLez8fjPuSee+4573tzk7yd9THWWkREREREvMHldAARERERyTlUPkVERETEa1Q+RURERMRrVD5FRERExGtUPkVERETEa1Q+RURERMRrVD5FsjhjzFhjzBCnc6SFMaaXMWZ2ivvWGFMhg9fZ1BizJSPXccH6NhhjmnlrfdmNMWaBMea+DF5HbmPMz8aYf4wxP6Zh/mbGmH0p7p/7Hhu3McaYE8aYFZ5pDxpjDhljYo0xhTLqdaQ3fXbFW1Q+JdswxtxhjIn0/MI/YIz5xRjTxOlc6ckY08cYs9jpHGddafG11o6z1rbJ4EznFVpr7SJrbWgGres/r99aW9VauyAj1peejDGPGmN2GWNOGmM2GWMqOZ3Ji3oC1wGFrLU3X+mTL/geNwFaA6WstfWMMX7Ae0Aba22QtfZYeoVOC2NMlDGm1dU8N6t8diXrU/mUbMEY8z9gOPA67j8qpYFPgJscjHVZxhjf7Lw+yZw8WxbvBToCQUAn4Gg6LDerfL7KAFuttYnptKwoa+1Jz/3rgABgw9UszBjjkw6ZRDI3a61uumXpG5AfiAVuvsQ8uXCX0/2e23Agl+exZsA+4GngMHAA6Ap0ALYCx4HnUizrZWAiMAH4F1gN1Ezx+DPADs9jG4FuKR7rAywB3geOAUM82d4B9gCHgJFA7lReQ2UgHkjyvN5oz/SxwMfADM86/wDKp3ieBR4GtgG7PNM6AWuAaGApUCPF/CWAScARYBfwyEXe037AGSDBk+fnNL7+xRdkq3CJ7+uXnu/H3573ysfzWAXgd+Af3KVpgmf6Qs8yT3oy3Xr2+5tiuVHAU8A6z3xf4i4Mv3gy/wYUSDH/j8BBz7oWAlUv8/qjgFZX8Ll7gv//3N1zic9wCWAa7s/jduD+Cz6TPwBfe17DBiDiIstxAXuBlmn8+coNfAWcADbh/jm58P0c5Hk/TwO+afgMLAE+8rynm1NmARYAr3nm+ReYDRT2PBYAfIv7ZycaWAlcd5HclT3Liva8H10801/xfM/OeL5v917kNY/1vOaNns/Lha+5Fe4Cn/Jncjzuz5T13J/nmT8MmOP53m0BbkmxrLHAp8BMz3NbcYmfwUt9r4FvgGTglGf9T6fy2goD0z3vy3FgEeBK5bMb7VlGbIrXFJKG3x+DcP+8/ut5rWn6nOmWs26OB9BNt2u9Ae2ARMD3EvO8CiwHigJFPL8wX/M81szz/BcBP+B+zy/974C8QFXPL/Oynvlf9vzh6umZ/0nPHwg/z+M3e/54uHCXn5NAcc9jfTzrGoj7j3Ru3EV0GlDQs76fgTcu8jr6kKK8eaaNxf3HuJ5nmeOA71M8bnH/4SvoWV8t3GWnPuAD3O35o5PLk3mV573wB8oBO4G2F8kzFhhywbTLvf60ls/JwGdAHs/3bQXwgOex8cDznnUEAE0utkxSL5/LcRfOkp73YrXnfQkA5gEvpZi/r+f7crZIrrnM64/i//+Ap+Vz9yruz1EHII4UxfeC5S7EvTU/AAjH/RltkeIzGe9Zhg/wBrD8Issp7XmPHsVdQnfhLmSui8z/Ju6iXwAohbtkXvh+rgGux/OfpjR8BhKBxz2v+1bcJbSg5/EFuItrJdyf1wXAm57HHsD98xHoeZ11gHypZPbDXdCfw/05boG7DIWmeL++vcTvizdxl7KCnte1PpXX3CrF60n5mQ7xvL++nvt5PO/zPbh/Pmvh/g9TlRSfoX+Axp73K5BL/Axe7nudMttFXtsbuP+D6+e5NQXMpZ6Le4/SQs/8l/r9Eep5rSVSvBflL5ZFt5x7czyAbrpd6w3oBRy8zDw7gA4p7rfFvasM3CXgFP+/VS2v549H/RTzrwK6er5++YJf9i7cW62aXmTda4CbPF/3AfakeMzg/sOccktlQzxbKFNZ1nl/6DzTxgJfpLjfAdic4r7FU1I89z/FU4BSTNsC3Oj5g7LngseeBcZcJM9YLihfaXj9ly2fuIvhaVJsAQZuB+Z7vv4aGIX7OLsLn5uW8tkrxf1JwKcp7g8EplzktQR7lp//Yq+f84tJWj53vikePww0SGW91+PeupY3xbQ3gLEpPpO/pXisCnDqIq+hkec1zPC8nhDcW/jvv8j85/3nA7gvlfez7xV+BvbjKTyeaSuAuzxfLwAGp3jsIeBXz9d9uWBL20XW1xT31mpXimnjgZdTvF+XKp87gXYp7vdL5TWntXzeCiy6YPmf4fkPjucz9HWKxy75M3i57zWXL5+vAlNJ/efuP8/15I8CinjuX+r3RwXcn+FWeP4zrptuqd10zKdkB8eAwpc53qwEsDvF/d2eaeeWYa1N8nx9yvPvoRSPn8J9bNxZe89+Ya1Nxr37tASAMaa3MWaNMSbaGBMNVMO9q+s/z8W9NSwQWJVi/l8906/EwRRfx12Q9cJ1lgGeOLs+zzqv9+QvA5S44LHncJfBNEnD60+LMri3shxIsZzPcG9BBPeuXwOs8Jyh2/cKl3/h9zbV77UxxscY86YxZocxJgb3H2FI++tJy+cu5XGHqX3vzi7nuLX23wuWVTLF/Qs/AwEX+Zk4+/keZq2NttZG4X5vO1ziNaT8/OxNZZ7zpqXhM/C3tdZe8FpSvi8X+zx/A8wCvjfG7DfGDPOc4JNqZs/PZsp1lExl3tRc+Jp3X2zGNCgD1L/gZ6oXUCzFPBf+fF7uZzCt3+vUvI17q/BsY8xOY8wzF5vRGFML9+ER3ay1R1LkS/X3h7V2O/AY7oJ82BjzvTGmxH+XLDmdyqdkB8twbyXreol59uP+pXlWac+0q3X92S+MMS7cuyP3G2PKAJ8DA3CfSRuMe5edSfHclH90j+IuA1WttcGeW35rbWoF5MLnXomUz9sLDE2xvmBrbaC1drznsV0XPJbXWnuxYnJenjS+/rTYi/t7WjhFjnzW2qoA1tqD1tr7rbUlcO+K/SSDLtl0B+6T1lrhPgY1xDP97Ou53PcjvT53+4GCxpi8Fyzr76tY1hbcxzymzH6p13EA9+f7rOtTmefc89P4GShpjEl5P03vi7X2jLX2FWttFdxbcDsBvVOZdT9wvednM+U60vp+HeD811k6jc9LzV7g9wt+poKstQ+mmOfCn88r+Rm80CU/k9baf621T1hrywFdgP8ZY1peOJ8xpigwBXjYWvvnBfku9vsDa+131tomuD/3FngrjbklB1H5lCzPWvsP7uOjPjbGdDXGBBpj/Iwx7Y0xwzyzjQcGG2OKGGMKe+b/9hpWW8cY092zteEx3EVpOe7juyzu4/EwxtyDe6vPxbIn4/5D/b7nlz3GmJLGmLYXecohoJQxxv8asn8O9DfG1PdcozCPMaajp9isAP41xgwy7msh+hhjqhlj6l4iT7kU96/o9V+MtfYA7hNN3jXG5DPGuIwx5Y0xN3qWe7Mx5mwhOuFZ59mtXBdmuhZ5cX9vj+HeQv36BY9fbl3p8rmz1u7Fvbv5DWNMgDGmBu6TXa5mWXG4T5Z72hiT1/M+9sN9EkpqfgCeNcYUMMaUxF0qLyUtn4GiwCOen9ObcZ8cNPNy2Y0xzY0x1T1nhMfgPvY6OZVZ/8C9RfBpzzqaAZ2B7y+3Do+Ur7kU7kMxrtZ0oJIx5i5PFj9jTF1jTOWLzH+lP4MXuuRn0hjTyRhTwVP+/8F9OEfyBfP44j6p8ltr7Q8XLOKivz+MMaHGmBbGmFy4j0s9deGyRUDlU7IJa+27wP+Awbj/6O3F/UdyimeWIUAk7pMl/sJ9gsm1XJh9Ku5joU4AdwHdPVtlNgLv4t4aewiojvus3UsZhHs32HLPrt3fcB+4n5p5uM9uPWiMuapL41hrI3GfVPWRJ/923Met4Tn0oBPuE1p24d4y+wXurX6p+RKo4tn9NuUqX//F9MZ9wsVGT86JQHHPY3WBP4wxsbhP1nrUWrvT89jLwFeeTLdc5brP+hr3Lte/PTmWX/D4ea8/leen5+fudtxbXvfjPhnrJWvtb1e5rAG4z2Lej/t79R0w+iLzvor7sJJduD+bE3EX8lSl8TPwB1AR9+drKNDTpu16mMU864/Bfeb977h3xV+YIQF32WzvWccnQG9r7eY0rAPcJ2Dtxv2aZ6e2jrTyHCrRBrgN9/t9EPfWwFwXmf9KfwYv9Abu//BEG2OeTOXxiri/j7G4v0efWGvnXzBPKdzHzT5m3NdNPnsrfanfH57X9KYn80Hc/8l4No25JQc5e4abiKSRMeZl3Afr3+l0FhFvM8Y8CNxmrb3xKp/fB7jPs2tWRHIgbfkUEZGLMsYUN8Y09hz6EIr7uqSTnc4lIllXVhmNQkREnOGP+2z4srgvKv497t3YIiJXRbvdRURERMRrtNtdRERERLxG5VNEREREvCbbHPNZuHBhGxIS4nQMERERkRxv1apVR621qY7Wl23KZ0hICJGRkU7HEBEREcnxjDEXHZZWu91FRERExGtUPkVERETEa1Q+RURERMRrss0xnyIiInL1zpw5w759+4iPj3c6imQhAQEBlCpVCj8/vzQ/R+VTRERE2LdvH3nz5iUkJARjjNNxJAuw1nLs2DH27dtH2bJl0/w87XYXERER4uPjKVSokIqnpJkxhkKFCl3x1nKVTxEREQFQ8ZQrdjWfGZVPERERyRSCgoLOuz927FgGDBgAwMiRI/n6668B6NOnDxMnTgSgWbNm13yd7wULFrB06dJz91Ou61q9/vrr6bKc7ETlU0RERK7cuHEQEgIul/vfceMydHX9+/end+/eGbLsC8tneq7rSsuntZbk5OR0WXdmpfIpIiIiV2bcOOjXD3bvBmvd//brl6EF9OWXX+add95J8/yrVq3ixhtvpE6dOrRt25YDBw4AMGLECKpUqUKNGjW47bbbiIqKYuTIkbz//vuEh4ezaNGi89bVrFkzHn/8cSIiIqhcuTIrV66ke/fuVKxYkcGDB59bX9euXalTpw5Vq1Zl1KhRADzzzDOcOnWK8PBwevXqBcB7771HtWrVqFatGsOHDwcgKiqK0NBQevfuTbVq1di7d296vGWZls52FxERkfM99hisWXPxx5cvh9Onz58WFwf33guff576c8LDwVO2LuZsUTvr+PHjdOnSJQ2Bz3fmzBkGDhzI1KlTKVKkCBMmTOD5559n9OjRvPnmm+zatYtcuXIRHR1NcHAw/fv3JygoiCeffBKAuXPnnrc8f39/IiMj+eCDD7jppptYtWoVBQsWpHz58jz++OMUKlSI0aNHU7BgQU6dOkXdunXp0aMHb775Jh999BFrPO/lqlWrGDNmDH/88QfWWurXr8+NN95IgQIF2LZtG1999RUNGjS44teb1ah8ioiIyJW5sHhebnoa5c6d+1xRA/cxn1dzPOeWLVtYv349rVu3BiApKYnixYsDUKNGDXr16kXXrl3p2rVrmpZ3tgBXr16dqlWrnltWuXLl2Lt3L4UKFWLEiBFMnjwZgL1797Jt2zYKFSp03nIWL15Mt27dyJMnDwDdu3dn0aJFdOnShTJlyuSI4gkqnyIiInKhy2yhJCTEvav9QmXKwIIFGRDoylhrqVq1KsuWLfvPYzNmzGDhwoX8/PPPDB06lL/++uuyy8uVKxcALpfr3Ndn7ycmJrJgwQJ+++03li1bRmBgIM2aNbviyw+dLaQ5gY75FBERkSszdCgEBp4/LTDQPT0TCA0N5ciRI+fK55kzZ9iwYQPJycns3buX5s2b89Zbb/HPP/8QGxtL3rx5+ffff696ff/88w8FChQgMDCQzZs3s3z58nOP+fn5cebMGQCaNm3KlClTiIuL4+TJk0yePJmmTZte24vNglQ+RURE5Mr06gWjRrm3dBrj/nfUKPf0TMDf35+JEycyaNAgatasSXh4OEuXLiUpKYk777yT6tWrU6tWLR555BGCg4Pp3LkzkydPPnfC0ZVq164diYmJVK5cmWeeeea83ef9+vU7t6u/du3a9OnTh3r16lG/fn3uu+8+atWqlZ4vPUsw1lqnM6SLiIgIe63X+RIREcmpNm3aROXKlZ2OIVlQap8dY8wqa21EavNry6eIiIiIeI3Kp4iIiIh4jcqniIiIiHiNyqeIiIiIeI3Kp4iIiIh4jcqniIiIiHiNyqeIiIhkCkFBQee+njlzJpUqVWJ3aiMppZOxY8cyYMCA/0xfsGABS5cuzbD1Xo3hw4cTFxd37n6HDh2Ijo6+5uWuWbOGmTNnXvNyroTKp4iIiGQqc+fO5ZFHHuGXX36hTJkyaXpOUlJSuq0/K5TPmTNnEhwcfM3LvZrymZiYeE3rVPkUERGRTGPhwoXcf//9TJ8+nfLlywPw7bffUq9ePcLDw3nggQfOFc2goCCeeOIJatasybJlywgKCuL555+nZs2aNGjQgEOHDgFw5MgRevToQd26dalbty5Lliy56PqjoqIYOXIk77///rkRj8qWLXtuiMyYmJhz95s1a8ajjz5KeHg41apVY8WKFQCcPHmSvn37Uq9ePWrVqsXUqVNTXdfbb79N3bp1qVGjBi+99NK553bs2JGaNWtSrVo1JkyYwIgRI9i/fz/NmzenefPmAISEhHD06FGioqIICwujT58+VKpUiV69evHbb7/RuHFjKlaseC7TihUraNiwIbVq1aJRo0Zs2bKFhIQEXnzxRSZMmEB4eDgTJkzg+PHjdO3alRo1atCgQQPWrVsHwMsvv8xdd91F48aNueuuu67pe+x7Tc+WdNVsbDMAFvRZ4GgOERHJ2R779THWHFyTrssMLxbO8HbDLznP6dOn6dq1KwsWLCAsLAxwj54zYcIElixZgp+fHw899BDjxo2jd+/enDx5kvr16/Puu+8C7uLWoEEDhg4dytNPP83nn3/O4MGDefTRR3n88cdp0qQJe/bsoW3btmzatCnVDCEhIfTv35+goCCefPJJAJo1a8aMGTPo2rUr33//Pd27d8fPzw+AuLg41qxZw8KFC+nbty/r169n6NChtGjRgtGjRxMdHU29evVo1aoVefLkObee2bNns23bNlasWIG1li5durBw4UKOHDlCiRIlmDFjBuAeNz5//vy89957zJ8/n8KFC/8n8/bt2/nxxx8ZPXo0devW5bvvvmPx4sVMmzaN119/nSlTphAWFsaiRYvw9fXlt99+47nnnmPSpEm8+uqrREZG8tFHHwEwcOBAatWqxZQpU5g3bx69e/dmzZo1AGzcuJHFixeTO3fuNH7XU6fyKSIiIpmCn58fjRo14ssvv+SDDz4A3LvgV61aRd26dQE4deoURYsWBcDHx4cePXqce76/vz+dOnUCoE6dOsyZMweA3377jY0bN56bLyYmhtjY2DTnuu+++xg2bBhdu3ZlzJgxfP755+ceu/322wG44YYbiImJITo6mtmzZzNt2jTeeecdAOLj49mzZ895Q1DOnj2b2bNnnxvbPTY2lm3bttG0aVOeeOIJBg0aRKdOnWjatOll85UtW5bq1asDULVqVVq2bIkxhurVqxMVFQW4S+zdd9/Ntm3bMMac25J7ocWLFzNp0iQAWrRowbFjx4iJiQGgS5cu11w8QeVTRERELnC5LZQZxeVy8cMPP9CyZUtef/11nnvuOay13H333bzxxhv/mT8gIAAfH59z9/38/DDGAO5ievbYxOTkZJYvX05AQMBV5WrcuDFRUVEsWLCApKQkqlWrdu6xs+tLed9ay6RJkwgNDb3oMq21PPvsszzwwAP/eWz16tXMnDmTwYMH07JlS1588cVL5suVK9e5r10u17n7Lpfr3Hvwwgsv0Lx5cyZPnkxUVBTNmjW77Ou+UMott9dCx3yKiIhIphEYGMiMGTMYN24cX375JS1btmTixIkcPnwYgOPHj1/xGfBt2rThww8/PHf/7G7ki8mbNy///vvvedN69+7NHXfcwT333HPe9AkTJgDuLYb58+cnf/78tG3blg8//BBrLQB//vnnf9bRtm1bRo8efW4L7N9//83hw4fZv38/gYGB3HnnnTz11FOsXr36opmuxD///EPJkiUB91n+F3utTZs2Zdy4cYD7xKvChQuTL1++q15valQ+RUREJFMpWLAgv/76K0OGDGH79u0MGTKENm3aUKNGDVq3bs2BAweuaHkjRowgMjKSGjVqUKVKFUaOHHnJ+Tt37szkyZPPnXAE0KtXL06cOHFuN/tZAQEB1KpVi/79+/Pll18C7q2MZ86coUaNGlStWpUXXnjhP+to06YNd9xxBw0bNqR69er07NmTf//9l7/++uvcyVWvvPIKgwcPBqBfv360a9fu3AlHV+rpp5/m2WefpVatWuedrd68eXM2btx47oSjl19+mVWrVlGjRg2eeeYZvvrqq6ta36WYs608q4uIiLCRkZFOx7gmOuFIREScsmnTpvOOSZTzTZw4kalTp/LNN9+cm9asWTPeeecdIiIiHEzmvNQ+O8aYVdbaVN8YHfMpIiIicgkDBw7kl19+8frF2LMrlU8RERGRS0h5vGhKCxYs8G6QbELHfIqIiIiI16h8ioiIiIjXqHyKiIiIiNeofIqIiIiI16h8ioiISKZgjOHOO+88dz8xMZEiRYqcGzLT29asWZOhZ7iPHTuW/fv3n7t/3333nTcM6NWKioriu+++u+blZJQMK5/GmNHGmMPGmPUXebyZMeYfY8waz+3FFI+1M8ZsMcZsN8Y8k1EZRUREJPPIkycP69ev59SpUwDMmTPn3Kg8TvB2+fziiy+oUqXKNS/3aspnygvPZ7SM3PI5Fmh3mXkWWWvDPbdXAYwxPsDHQHugCnC7MebavxMiIiKS6XXo0IEZM2YAMH78+PNGFDp+/Dhdu3alRo0aNGjQgHXr1gHw8ssvc/fdd9O0aVPKlCnDTz/9xNNPP0316tVp164dZ86cAWDVqlXceOON1KlTh7Zt254bKalZs2YMGjSIevXqUalSJRYtWkRCQgIvvvgiEyZMODf6z++//054eDjh4eHUqlUr1eEuv/3223MjFD3wwAMkJSWRlJREnz59qFatGtWrV+f9999n4sSJREZG0qtXL8LDwzl16hTNmjXj7IA5QUFBPPXUU1StWpVWrVqxYsUKmjVrRrly5Zg2bRrgLplNmzaldu3a1K5dm6VLlwLwzDPPsGjRIsLDw3n//feJj4/nnnvuoXr16tSqVYv58+cD7vLbpUsXWrRoQcuWLTPi25mqDLvOp7V2oTEm5CqeWg/Ybq3dCWCM+R64Cbj27dAiIiJyWY89BpcZ/vyKhYfD8OGXn++2227j1VdfpVOnTqxbt46+ffueG+LypZdeolatWkyZMoV58+bRu3fvc+O079ixg/nz57Nx40YaNmzIpEmTGDZsGN26dWPGjBl07NiRgQMHMnXqVIoUKcKECRN4/vnnGT16NODe8rdixQpmzpzJK6+8wm+//carr75KZGQkH330EeAedvPjjz+mcePGxMbGEhAQcF72TZs2MWHCBJYsWYKfnx8PPfQQ48aNo2rVqvz999+sX+/eGRwdHU1wcDAfffTRRUdIOnnyJC1atODtt9+mW7duDB48mDlz5rBx40buvvtuunTpQtGiRZkzZw4BAQFs27aN22+/ncjISN58803eeecdpk+fDsC7776LMYa//vqLzZs306ZNG7Zu3QrA6tWrWbduHQULFrzSb+lVc/oi8w2NMWuB/cCT1toNQElgb4p59gH1nQgnIiIi3lWjRg2ioqIYP348HTp0OO+xxYsXM2nSJABatGjBsWPHiImJAaB9+/b4+flRvXp1kpKSaNfOvfO1evXqREVFsWXLFtavX0/r1q0BSEpKonjx4ueW3b17dwDq1KlDVFRUqtkaN27M//73P3r16kX37t0pVarUeY/PnTuXVatWUbduXQBOnTpF0aJF6dy5Mzt37mTgwIF07NiRNm3aXPZ98Pf3P+815MqV69zrO5vvzJkzDBgwgDVr1uDj43OuUF5o8eLFDBw4EICwsDDKlClzbt7WrVt7tXiCs+VzNVDGWhtrjOkATAEqXskCjDH9gH4ApUuXTveAIiIiOVFatlBmpC5duvDkk0+yYMECjh07lqbn5MqVCwCXy4Wfnx/GmHP3ExMTsdZStWpVli1bdsnn+/j4XPT4x2eeeYaOHTsyc+ZMGjduzKxZswgLCzv3uLWWu+++mzfeeOM/z127di2zZs1i5MiR/PDDD+e2uF7Mha8h5es7m+/999/nuuuuY+3atSQnJ/9nS2xa5MmT54qfc60cO9vdWhtjrY31fD0T8DPGFAb+Bq5PMWspz7TUljHKWhthrY0oUqRIhmcWERGRjNe3b19eeuklqlevft70pk2bMm7cOMA9tGXhwoXJly9fmpYZGhrKkSNHzpXPM2fOsGHDhks+J2/evOcd17ljxw6qV6/OoEGDqFu3Lps3bz5v/pYtWzJx4kQOHz4MuI9R3b17N0ePHiU5OZkePXowZMgQVq9eneryr9Q///xD8eLFcblcfPPNNyQlJaW63JTv29atW9mzZw+hoaFXvd5r5diWT2NMMeCQtdYaY+rhLsLHgGigojGmLO7SeRtwh1M5RURExLtKlSrFI4888p/pL7/8Mn379qVGjRoEBgby1VdfpXmZ/v7+TJw4kUceeYR//vmHxMREHnvsMapWrXrR5zRv3pw333yT8PBwnn32WRYvXsz8+fNxuVxUrVqV9u3bnzd/lSpVGDJkCG3atCE5ORk/Pz8+/vhjcufOzT333ENycjLAuS2jffr0oX///uTOnfuiW2Qv5aGHHqJHjx58/fXXtGvX7txWzBo1auDj40PNmjXp06cPDz30EA8++CDVq1fH19eXsWPHntuS6gRjrc2YBRszHmgGFAYOAS8BfgDW2pHGmAHAg0AicAr4n7V2qee5HYDhgA8w2lo79HLri4iIsGfPEMuqmo1tBsCCPgsczSEiIjnPpk2bqFy5stMxJAtK7bNjjFllrf3vmVRk7Nnut1/m8Y+Ajy7y2Ewg4y6sJSIiIiKO0AhHIiIiIuI1Kp8iIiIi4jUqnyIiIgK4LxUkciWu5jOj8ikiIiIEBARw7NgxFVBJM2stx44du+Lrizo9wpGIiIhkAqVKlWLfvn0cOXLE6SiShQQEBPxnpKfLUfkUERER/Pz8KFu2rNMxJAfQbncRERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8RqVTxERERHxGpVPEREREfEalU8RERER8ZoMK5/GmNHGmMPGmPWXma+uMSbRGNMzxbRhxpgNxphNxpgRxhiTUTlFRERExHsycsvnWKDdpWYwxvgAbwGzU0xrBDQGagDVgLrAjRmWUkRERES8JsPKp7V2IXD8MrMNBCYBh1M+FQgA/IFcgB9wKCMyioiIiIh3OXbMpzGmJNAN+DTldGvtMmA+cMBzm2Wt3eT9hCIiIiKS3pw84Wg4MMham5xyojGmAlAZKAWUBFoYY5qmtgBjTD9jTKQxJvLIkSMZnVdERERErpGvg+uOAL73nEtUGOhgjEkEKgLLrbWxAMaYX4CGwKILF2CtHQWMAoiIiLBeyi0iIiIiV8mxLZ/W2rLW2hBrbQgwEXjIWjsF2APcaIzxNcb44T7ZSLvdRURERLKBDNvyaYwZDzQDChtj9gEv4T55CGvtyEs8dSLQAvgL98lHv1prf86onCIiIiLiPRlWPq21t1/BvH1SfJ0EPJARmURERETEWRrhSERERES8RuVTRERERLxG5VNEREREvEblU0RERES8RuVTRERERLxG5VNEREREvEblU0RERES8RuVTRERERLxG5VNEREREvEblU0RERES8RuVTRERERLxG5VNEREREvEblU0RERES8RuVTRERERLxG5VNEREREvEblU0RERES8RuVTRERERLxG5VNEREREvEblU0RERES8RuXzKjQb24xmY5s5HUNEREQky1H5FBERERGvUfkUEREREa9R+RQRERERr1H5FBERERGvUfkUEREREa9R+RQRERERr1H5FBERERGvUfkUEREREa9R+RQRERERr1H5FBERERGvUfkUEREREa9R+RQRERERr1H5FBERERGvUfkUEREREa9R+RQRERERr1H5FBERERGvUfkUEREREa9R+RQRERERr1H5FBERERGvUfkUEREREa9R+RQRERERr1H5FBERERGvUfkUEREREa9R+RQRERERr1H5FBERERGv8XU6QFa0ZvAQ9xd9HI0hIiIikuVoy6eIiIiIeI3Kp4iIiIh4jcqniIiIiHiNyqeIiIiIeI3Kp4iIiIh4jcqniIiIiHiNyqeIiIiIeI3Kp4iIiIh4jcqniIiIiHiNyqeIiIiIeI3Kp4iIiIh4jcqniIiIiHiNyqeIiIiIeI3Kp4iIiIh4jcqniIiIiHiNyqeIiIiIeE2GlU9jzGhjzGFjzPrLzFfXGJNojOmZYlppY8xsY8wmY8xGY0xIRuUUEREREe/JyC2fY4F2l5rBGOMDvAXMvuChr4G3rbWVgXrA4YwIKCIiIiLelWHl01q7EDh+mdkGApNIUS6NMVUAX2vtHM9yYq21cRmVU0RERES8x7FjPo0xJYFuwKcXPFQJiDbG/GSM+dMY87ZnC2lqy+hnjIk0xkQeOXIkoyOLiIiIyDVy8oSj4cAga23yBdN9gabAk0BdoBzQJ7UFWGtHWWsjrLURRYoUycCoIiIiIpIefB1cdwTwvTEGoDDQwRiTCOwD1lhrdwIYY6YADYAvHcopIiIiIunEsfJprS179mtjzFhgurV2imcXe7Axpoi19gjQAoh0KKaIiIiIpKMMK5/GmPFAM6CwMWYf8BLgB2CtHXmx51lrk4wxTwJzjXuz6Crg84zKKSIiIiLek2Hl01p7+xXM2+eC+3OAGumdSUREREScpRGORERERMRrVD5FRERExGtUPkVERETEa1Q+RURERMRrVD5FRERExGtUPkVERETEa1Q+RURERMRrVD5FRERExGtUPkVERETEa1Q+RURERMRrVD4ziYVRC1m2bxn7YvY5HUVEREQkw6h8ZgLTtkyj+dfNSUhKYMeJHTw1+ymnI4mIiIhkCJVPh3215iu6ft8VgPIFyuNjfHhn2TvcMekOZ4OJiIiIZACVTwcNXz6cPlP74OPyYc5dcyiVrxT1StSjaJ6ijF8/nmZjm5GcnOx0TBEREZF0o/LpkBfmvcDjsx4nl08ult27jBZlWwDg7+vP7sd2U7FgRX7f/TvVPq1GQmKCw2lFRERE0ofKpwMenP4gQxYNIY9fHv568C8iSkSc93iAbwCbH95Mw1IN2XR0E2VHlCU6PtqZsCIiIiLpSOXTy2758RZGrhpJcEAwWwdspWKhiqnO53K5WHrvUrqFdWP/v/sJGR7C7ujdXk4rIiIikr5UPr0kOTmZ1l+35seNP1IsqBi7Ht1FiXwlLvu8n279iQF1B/DP6X8I+ziMNQfWZHxYERERkQyi8ukFycnJ1PuiHr/t+o2ywWXZMXAHwQHBaX7+hx0+5I2WbxCfGE/dL+oya/usjAsrIiIikoFUPjNYQmIClT+pzKoDq6hetDpbB24l0D/wipfzTJNn+KrrVyQlJ9F+XHu+WvNVBqQVERERyVgqnxkoJj6GciPKsfXYVppc34Q1D6zB1+V71cvrXbM3s+6chY/Lhz5T+/DGojfSMa2IiIhIxlP5zCCHYw9TbkQ5/v73b7pU6sKivotwua797W5dvjUr71tJgG8Az817jodnPpwOaUVERES8Q+UzA+yO3k2FDytw7NQx+oT3YertU9N1+eHFw9kyYAv5c+Xnk5Wf0O37bum6fBEREZGMovKZztYfWk/Yx2H8m/Av/2v4P8bcNCZD1lM6f2miHouiZN6STNkyhQZfNNBoSCIiIpLpqXymoyV7llB7VG3iE+MZ0nwI77Z5N0PXFxwQzM5HdlKlcBX++PsPQj8OJS4hLkPXKSIiInItVD7TycxtM7lx7I2cST7DyI4jef6G572yXn9ff/568C+ahzRn+/HtlP2gLEfjjnpl3SIiIiJXSuUzHYz7axydvutEsk1mQs8JPBDxgFfX73K5mHf3PO6odgeH4w5T9oOybDu2zasZRERERNJC5fMaffjHh9z50524jItZd87ilqq3OJZlXI9xPNXoKWITYqn2aTX+2PeHY1lEREREUqPyeQ1eWfAKj/z6CP4+/iy7dxmty7d2OhLDWg9jeNvhJCQl0Hh0Y6ZuTt8z7UVERESuhcrnVYprOJyXf3+Z3L65WfPAGuqWrOt0pHMebfAoE3pOwGLpNqEbn0V+5nQkEREREUDl86rENX6bM1UnkT9XfjYP2EzlIpWdjvQft1S9hfm95+Pr8qX/jP68MO8FpyOJiIiIqHxeDb+oGzAxJdj5yE5K5y/tdJyLuiHkBtb2X0ugXyBDFg2h79S+TkcSERGRHE7l8yr4/V2ffD9MoGBgQaejXFblIpXZ8cgOCuUuxJg1Y2j3bTtdjF5EREQco/KZAxQLKkbUY1GUyV+GWTtmUefzOiQmJzodS0RERHIglc8cIsg/iO2PbKdWsVqsObiGCiMqEJsQ63QsERERyWFUPnMQX5cvkfdH0r5Ce3b/s5syw8uwP2a/07FEREQkB1H5zGFcLhcze83k3lr3cvzUcSp+VJENhzc4HUtERERyCJXPHOqLLl/w0g0vEXcmjlqf1WJh1EKnI4mIiEgOoPKZg73c/GU+6/QZicmJNP+6ORPWT3A6koiIiGRzKp85XL86/Zhy2xQMhtsm3cbw5cOdjiQiIiLZmMqn0CW0C0v6LiGXTy4en/U4T81+yulIIiIikk2pfAoA9UvVZ8NDG8jrn5d3lr3DHZPucDqSiIiIZEMqn3JO+YLl2fnoTormKcr49eNpNraZRkMSERGRdKXyKecpHFiY3Y/tpmLBivy++3eqfVqNhMQEp2OJiIhINqHyKf8R4BvA5oc307BUQzYd3UTZEWWJjo92OpaIiIhkAyqfkiqXy8XSe5fSLawb+//dT8jwEHZH73Y6loiIiGRxKp9yST/d+hMD6w3kn9P/EPZxGKsPrHY6koiIiGRhKp9yWSPaj+CNlm8QnxhP/S/qM2v7LKcjiYiISBal8ilp8kyTZ/iq61ckJSfRflx7vlrzldORREREJAtS+ZQ0612zN7PunIWPy4c+U/vwxqI3nI4kIiIiWYzKp1yR1uVbs/K+lQT4BvDcvOd4eObDTkcSERGRLETlU65YePFwtgzYQnBAMJ+s/IRu33dzOpKIiIhkESqfclVK5y/Nrkd3UTJvSaZsmUKDLxpoNCQRERG5LJVPuWrBAcHsfGQnVYtU5Y+//6DSR5WIS4hzOpaIiIhkYiqfck38ff1Z138dzUOas+PEDsp+UJajcUedjiUiIiKZlMqnXDOXy8W8u+dxR7U7OBx3mLIflGXbsW1OxxIREZFMSOVT0s24HuN4qtFTxCbEUu3Tavyx7w+nI4mIiEgmo/KZiawZPIQ1g4c4HeOaDGs9jOFth5OQlEDj0Y2Zunmq05FEREQkE1H5lHT3aINH+fHmH7FYuk3oxmeRnzkdSURERDIJlU/JED2r9GR+7/n4unzpP6M/L8x7welIIiIikgmofF6pceNYd7Anx/++AUJCYNw4pxNlWjeE3MDa/msJ9AtkyKIh9J3a1+lIIiIi4rAMK5/GmNHGmMPGmPWXma+uMSbRGNPzgun5jDH7jDEfZVTGKzZuHPTrR+mkQ2ylAuzeDf36qYBeQuUildnxyA4K5S7EmDVjaPtNW12MXkREJAfLyC2fY4F2l5rBGOMDvAXMTuXh14CF6R/rGjz/PMTF8TjvUoP1bKYixMW5p8tFFQsqRtRjUZTJX4bZO2dTe1RtEpMTnY4lIiIiDsiw8mmtXQgcv8xsA4FJwOGUE40xdYDrSL2UOmfPHgBaMYcz+PMpD503XS4uyD+I7Y9sp3ax2qw9tJbyI8oTmxDrdCwRERHxMseO+TTGlAS6AZ9eMN0FvAs86USuSypdGoCO/EoYm5hPi/Omy6X5unxZef9K2ldoz55/9lBmeBn2x+x3OpaIiIh4kZMnHA0HBllrLzwA8CFgprV23+UWYIzpZ4yJNMZEHjlyJCMynm/oUAgMBKA58/iLGvzm29Y9XdLE5XIxs9dM7qt1H8dPHafiRxXZcHiD07FERETES5wsnxHA98aYKKAn8IkxpivQEBjgmf4O0NsY82ZqC7DWjrLWRlhrI4oUKZLxiXv1glGj2ONzHfcxChdJfO13r3u6XJHPu3zOSze8RNyZOGp9VosFUQucjiQiIiJeYKy1GbdwY0KA6dbaapeZb6xnvokXTO8DRFhrB1xuXRERETYyMvLqw16B4FKLAQjb78d+W5yoM6Vw+V57jz+73Oh9Ta55WVnFqFWj6D+9P8YYvuv+HbdWu9XpSCIiInKNjDGrrLURqT2WkZdaGg8sA0I9l0y61xjT3xjTP6PW6W2twvayl9J83etXp6NkWf3q9GPabdMwGG6bdBvDlw93OpKIiIhkoAzd8ulNTmz53DyzIuVrBtHKfxFTT1/yqlJXtNyctOXzrJV/r6TpmKacTjrN/xr+j3fbvOt0JBEREblKjmz5zAmK1biOJj7LWZhQn5h9MU7HydLqlqzLhoc2kNc/L+8te4/bJ97udCQRERHJACqf16hjs1iiKcAn3X9zOkqWV75geXY+upPr8lzH9xu+58YxN2o0JBERkWxG5fMa9ZvYliIc5tdVhZ2Oki0UDixM1GNRVCxYkYV7FlL106okJCY4HUtERETSicrnNQoIDuDG3CtYmtyAnfOinI6TLQT4BrD54c00KtWIzUc3E/JBCNHx0U7HEhERkXRw2fJpjMlljLnDGPOcMebFszdvhMsqbuuTmzP480lf75zwlBO4XC6W3LuE7mHdORB7gJDhIeyO3u10LBEREblGadnyORW4CUgETqa4iUe3Ec2pwDbm7i7vdJRsZ9KtkxhYbyD/nP6HsI/CWH1gtdORRERE5BqkpXyWstbeaq0dZq199+wtw5NlIS5fF80K/sUaarF4+Eqn42Q7I9qP4K1WbxGfFE+9z+sxa/sspyOJiIjIVUpL+VxqjKme4UmyuPvfKAfA6FcuOyS9XIWnGz/NN12/Idkm035ce75a85XTkUREROQqpKV8NgFWGWO2GGPWGWP+Msasy+hgWU29fuFEmEjmR4eTnKjLA2WEO2veyaw7Z+Hj8qHP1D4MXTjU6UgiIiJyhdJSPtsDFYE2QGegk+dfuUCLclFEUZYf+umanxmldfnWrOq3igDfAAbPH8xDMx5yOpKIiIhcgcuWT2vtbiAYd+HsDAR7pskFHv6mPrmI54fxiU5HydZqXFeDLQO2EBwQzKeRn9L1+65ORxIREZE0Ssullh4FxgFFPbdvjTEDMzpYVlS64fU09lnOwvh6xB2NczpOtlY6f2l2PbqLknlLMnXLVOp/Xl+jIYmIiGQBadntfi9Q31r7orX2RaABcH/Gxsq6OjQ8wTEKM7K7zsjOaMEBwex8ZCfVilRjxf4VVPqoEnEJKv0iIiKZWVrKpwGSUtxP8kyTVDw4uS0FOcaMpcFOR8kR/H39Wdt/LS1CWrDjxA5CPgjhcOxhp2OJiIjIRaSlfI4B/jDGvGyMeRlYDnyZoamysMDCgdyQ6w+WJjVgz7K9TsfJEVwuF3Pvnkuv6r04EneE8h+WZ9uxbU7HEhERkVSk5YSj94B7gOOe2z3W2uEZnCtLu/k2H+LJzSe9/3A6So7ybfdvebrR08QmxFLt02os27vM6UgiIiJygYuWT2NMPs+/BYEo4FvPbbdnmlzEbV+0JoRdzN0R4nSUHOet1m8xot0IEpISaDKmCVM3T3U6koiIiKRwqS2f33n+XQVEpridvS8X4fJ10Sx4DZE2ghWj1jgdJ8cZWH8gP978IwDdJnTj05WfOpxIREREzrpo+bTWdvL8W9ZaWy7Fray1tpz3ImZN975UCoAvntvpcJKcqWeVnvze53d8Xb48NPMhXpj3gtORREREhLRd53NuWqbJ+Zo8Vpdw1jD/WHUNt+mQJqWbsLb/WvL45WHIoiHcM/UepyOJiIjkeJc65jPAc2xnYWNMAWNMQc8tBCjptYRZWIvS29lORaY8Pt/pKDlW5SKV2f7IdgrlLsTYNWNp+01bXYxeRETEQZfa8vkA7uM7wzz/nr1NBT7K+GhZ34Nf1MaPBMZ/ecrpKDlasaBiRD0WRUhwCLN3zqb2qNokJmsIVBERESdc6pjPD6y1ZYEnUxzrWdZaW9Naq/KZBhVal6Oh6w8WnqpLfHS803FytCD/ILYN3Ead4nVYe2gt5T4oR0x8jNOxREREcpy0XOfzQ2NMNWPMLcaY3mdv3giXHbSvc4TDXMfnN2u4Taf5unyJ7BdJhwod2Buzl7IjyrI/Zr/TsURERHKUtJxw9BLwoefWHBgGdMngXNnGQz+1Ij/RTJ+fx+ko4jGj1wzur30/x08dp8KHFdhweIPTkURERHKMtAyv2RNoCRy01t4D1ATyZ2iqbCRfqXzc4L+cxUkNObjukNNxxGNU51G8fOPLnEo8Ra3ParEgaoHTkURERHKEtJTPU9baZCDRM+rRYeD6jI2VvXTrkkwcefjotkVOR5EUXmr2Ep93/pzE5ERaft2SCesnOB1JREQk20tL+Yw0xgQDn+M+2301oEGzr8Bd37ThevYwd3Mpp6PIBe6rfR/TbpuGwXDbpNt4b9l7TkcSERHJ1tJywtFD1tpoa+1IoDVwt2f3u6SRb4AvN+b9kxW2Luu+3+h0HLlAp9BOLLt3Gbl8cvHE7Cd4YvYTTkcSERHJttKy5RNjTA1jTBegNlDBGNM9Y2NlP3c/VYRkfBj5mMpnZlS3ZF02PbyJvP55eW/Ze9w28TanI4mIiGRLaTnbfTQwGugBdPbcOmVwrmyn1QuNqMZfLDhU1ekochFlC5Rl56M7uS7PdUzYMIEbxtyg0ZBERETSWVq2fDaw1kZYa++21t7jufXN8GTZUPMSW9hEZWYM+t3pKHIRhQMLE/VYFBULVmTRnkVU/bQq8YkaIEBERCS9pKV8LjPGVMnwJDlA/0+q40Mi4z7+x+kocgkBvgFsfngzjUo1YvPRzZT7oBzR8dFOxxIREckW0lI+v8ZdQLcYY9YZY/4yxqzL6GDZUZWbQmlgVrDgZASJ8RpbPDNzuVwsuXcJPSr34EDsAcoML8Pu6N1OxxIREcny0lI+vwTuAtrx/8d7ds7IUNlZm+oHOEAJRt/6i9NRJA0m3jKRR+o9QszpGMI+CmP1gdVORxIREcnS0lI+j1hrp1lrd1lrd5+9ZXiybOqhH24kLzFM/SWX01EkjT5o/wFvtXqL+KR46n1ej1+26T8OIiIiVyst5fNPY8x3xpjbjTHdz94yPFk2VTi0ME18/2DRmQYc3XLU6TiSRk83fppvun5Dsk2m43cdGfPnGKcjiYiIZElpKZ+5gdNAG3SppXRxU7t4/iUfn9y6wOkocgXurHknc+6ag4/Lh77T+jJ04VCnI4mIiGQ5xlrrdIZ0ERERYSMjI72yruBSiwGI3tfkqp6fEJtASN6jlDO7WJzcON2WK96x7tA6GnzRgFOJp3gw4kE+6fiJ05FEREQyFWPMKmttRGqPXXTLpzHmac+/HxpjRlx4y6iwOYF/kD835olkua3P5unbnI4jV6jGdTXYOnArwQHBfBr5KTeNv8npSCIiIlnGpXa7b/L8GwmsSuUm1+DOh/OThC+fPrDW6ShyFUrlK8XuR3dTMm9Jpm2dRr3P62k0JBERkTS4aPm01v7s+TLOWvtVyhsQ55142VfHt24kjE3M2x/qdBS5SvkC8rHzkZ1UK1KNlftXUvGjisQl6EdDRETkUtJywtGzaZwmV6h50Y2spzq/vbbU6Shylfx9/Vnbfy0tQlqw88ROQj4I4XDsYadjiYiIZFqXOuazvTHmQ6DkBcd7jgU0PE866Pd+KC6S+PqdI05HkWvgcrmYe/dc7qp+F0fijlBuRDm2HdOxvCIiIqm51JbP/biP94zn/GM9pwFtMz5a5hUeG0R4bNC1L+eOatQ1kSyIqUVyoo4XzOq+7v41zzR+hpNnTlLt02os27vM6UgiIiKZzqWO+VzrOb6zQopjPacB2621J7yWMJtrFbaXvZTm616/Oh1F0sEbrd5gRLsRJCQl0GRMEyZvmux0JBERkUwlLcd8zjHG5DPGFARWA58bY97P4Fw5xoDvmxLISX6akpZvhWQFA+sPZOLNEwHo8UMPPlmp64CKiIiclZbGk99aGwN0B7621tYHWmZsrJyjWI3raOKznIUJDQhM1JnS2UWPKj34vc/v+Lp8eXjmwzw/73mnI4mIiGQKaSmfvsaY4sAtwPQMzpMjdWwWyz8E88ChhRz/+wYICYFx45yOJdeoSekmrO2/ljx+eXh90ev0mdLH6UgiIiKOS0v5fBWYBeyw1q40xpQDdCpvOup3yz8U4TDzaYULC7t3Q79+KqDZQOUildn+yHYK5S7EV2u/os03bXQxehERydEuWz6ttT9aa2tYax/03N9pre2R8dFyjoDXX+RGFrCURuykjHtiXBw8r1212UGxoGJEPRZF2eCyzNk5h1qjapGYrKuViYhIznTZ8mmMqWSMmWuMWe+5X8MYMzjjo+Uge/bQi3GcwZ82/MbHPHRuumQPQf5BbB24lTrF67Du0DrKfVCOmPgYp2OJiIh4XVp2u3+Oe0SjMwDW2nXAbRkZKscpXZquTON9HiUJHwbwMTeygMX5OzidTNKRr8uXyH6RdKzYkb0xewn5IIT9MfudjiUiIuJVaSmfgdbaFRdM0z7D9DR0KAQG8hgjWEt17mcUq6hDq+iJ9M4/haNbjjqdUNLR9Dumc3/t+zkRf4IKH1Zg/aH1TkcSERHxmrSUz6PGmPKABTDG9AQOZGiqnKZXLxg1ij0+1xFEHKPKvM7v939LU5+lfBPTlZphpxlaZ5JGQcpGRnUexcs3vsypxFPUGlWLBVELnI4kIiLiFWkpnw8DnwFhxpi/gceA/hkZKkfq1YsaxSZSsORCiIqizqj+zElsweieM8lHDINX96C+/ypmDPrd6aSSTl5q9hKfd/6cpOQkWn7dkvF/jXc6koiISIZLy9nuO621rYAiQJi1tom1dnfGRxOAe37swJ8nyvLY9RPZbitw07DG9Mw9g92LdTJSdnBf7fuYfsd0DIY7frqD95a953QkERGRDJXmMR2ttSettf9mZBhJXUBwAO/v6cnK2Sfo6D+Hn+LbU6dpIIPKTyQxXoffZnUdKnZg2b3LyOWTiydmP8H/Zv3P6UgiIiIZRgOKZyEVWpdj6un2TBwwn+vZy7CdPQnPvZlxvX91Oppco7ol67Lp4U3k9c/L+8vf57aJuqCEiIhkTyqfWVD3D1uy6kxNXqw6kaMU5s5v2tHebw7rJ252Oppcg7IFyhL1aBTFgooxYcMEbhhzg0ZDEhGRbOei5dMY0/1SN2+GlP9y+bp4ZX1PVq+y3J5nGnMTb6TRzSV46LpJxB6MdTqeXKWCgQXZ9eguKhWqxKI9i6j6SVXiE+OdjiUiIpJuLrXls/Mlbp0ut2BjzGhjzOGzIyNdYr66xphEzyWcMMaEG2OWGWM2GGPWGWNuTeuLyYlK1C7Od7FdmPXGamqY9Xx6uAfhxQ/ycdtpTkeTqxTgG8CmhzbR+PrGbD62mXIflON43HGnY4mIiKSLi5ZPa+09l7j1TcOyxwLtLjWDMcYHeAuYnWJyHNDbWlvV8/zhxpjgNKwvR2v+TAMWJzfi3aaTScSXAbO7cKPPIpZ8GOl0NLkKLpeLxX0X06NyDw7EHqDsiLLsjtZFJkREJOu71G73Oz3//i+12+UWbK1dCFxuc81AYBJwOMXztlprt3m+3u95rMjlX4oA/G9hN9btLch9BX8iMrk2LR+pplGSsrCJt0zksfqPEXM6hrCPwlh9YLXTkURERK7JpXa75/H8m/cit2tijCkJdAM+vcQ89QB/YMe1ri8nyVcqH58f687CMTvOjZIUHhavUZKyqPfbvc+wVsOIT4qn3uf1+GXbL05HEhERuWqX2u3+meffV1K7pcO6hwODrLWptiFjTHHgG+CeS8zTzxgTaYyJPHLkSDpEyl7q9KnBnMQWfNljBkHEMnh1Dxr4r+KX5xY5HU2u0FONn+Lb7t+SbJPp+F1Hxvw5xulIIiIiV+Wyl1oyxgQYYx42xnziOYlotDFmdDqsOwL43hgTBfQEPjHGdPWsMx8wA3jeWrv8Yguw1o6y1kZYayOKFNGe+YvpO7Eja06E8Nj1E9lmK9D5jYb0zD1doyRlMb2q92LOXXPwcfnQd1pfhiwc4nQkERGRK5aW63x+AxQD2gK/A6WAax7pyFpb1lobYq0NASYCD1lrpxhj/IHJwNfW2onXuh5xOztK0h+/HKOj/2/8FN+BiKa5GVRBoyRlJS3LtWRVv1Xk9s3NC/Nf4MHpDzodSURE5Ipc6oQjX8+XFay1LwAnrbVfAR2B+pdbsDFmPLAMCDXG7DPG3GuM6W+M6X+Zp94C3AD0Mcas8dzC0/Ji5PIqtavA1NPtmDhgPqXYx7Ad7lGSvuszy+lokkY1rqvB1oFbKRBQgJGrRnLT+JucjiQiIpJml9ryucLz7xnPv9HGmGpAfqDo5RZsrb3dWlvcWutnrS1lrf3SWjvSWjsylXn7nN3Kaa391vOc8BS3NVf2suRyLhwlqddXbemgUZKyjFL5ShH1aBSl8pVi2tZp1Pu8HonJ2oItIiKZX1p2u48yxhQABgPTgI24r80pWdyFoyT9dnaUpGITiTsa53Q8uYx8AfnYMXAH1YtWZ+X+lYR+FEpcgr5vIiKSuV2qfBb1XM8zH3AP7hOEPsZdPPNc4nmSxZwdJemXIZFUNxv49FBPahTZz8ftpjodTS7D39efNQ+soWXZluw8sZOQD0I4HHv48k8UERFxyKXKpw8QxPnX9gxKcZNspuXzjViUUP//R0madRPNfBZqlKRMzuVy8Vvv37ir+l0ciTtCuRHl2HZsm9OxREREUmWstak/YMxqa21tL+e5ahERETYy0jslqVnwGgAWRIen63KDSy0GIHpfk3Rd7tWIjormyTrzGH+8Lcm4uCXfLN5d0YTCoYWdjiaX8Oxvz/Lmkjfx9/Fnwd0LaHh9Q6cjiYhIDmSMWWWtjUjtsUtt+TQZlEeygOCQYL7wjJLUxGc5X2uUpCzhjVZv8GH7D0lISqDJmCZM3jTZ6UgiIiLnuVT5bOm1FJJpuUdJas7n3aaTh5MaJSkLGFBvABNvdl8it8cPPfhk5ScOJxIREfl/lxpe87g3g0jmdt9PnVh7ogyPltIoSVlBjyo9WNhnIX4uPx6e+TDPz3ve6UgiIiJA2i61JAK4R0kavlejJGUVjUs3Zt2D68jjl4fXF71Onyl9nI4kIiKi8ilXTqMkZR2hhUPZ+chOCgcW5qu1X9HmmzYkJ+uYXRERcY7Kp1y1s6MkvVDl/FGSNvykUZIyk6JBRdn96G7KBpdlzs451BpVS6MhiYiIY1Q+5Zq4fF28usE9StJtZ0dJ6lFcoyRlMoH+gWwduJU6xeuw7tA6yn1Qjpj4GKdjiYhIDqTyKemiRO3ijI/twsxXI6lqNmqUpEzI1+VLZL9IOlbsyN6YvYR8EMK+mH1OxxIRkRxG5fMqLIgOT/cLzGcXrV5oxOKE+rzTZDJn8Ds3StKyj1c7HU08pt8xnQdqP8CJ+BNU+rAS6w+tdzqSiIjkICqfku5cvi6eWNSNtbvyc2/Bn1iZXIcWAypzd/4pHN1y1Ol4AozsPJJXmr3CqcRT1BpVi3m75jkdSUREcgiVT8kwZ0dJWvDF9vNGSXq9rkZJygxevPFFvuj8BUnJSbT+pjXj/xrvdCQREckBVD4lw9W9t+Z5oyQ9H9mDhv6RGiUpE7i39r1Mv2M6BsMdP93Bu0vfdTqSiIhkcyqf4jUpR0naaivS+Y2G3ByoUZKc1qFiB5bft5xcPrl4cs6T/G/W/5yOJCIi2ZjKp3jVhaMkTTqlUZIyg4gSEWx6eBP5cuXj/eXvc8uPtzgdSUREsimVT3HE2VGSfnxwPqX4m2E7elIr9yaNkuSgsgXKsuuRXRQLKsaPG3+k6eimGg1JRETSncqnOKrHJy1ZdaYGL1SZyBGKaJQkhxUMLMiuR3cRWiiUxXsXU/WTqsQnxjsdS0REshGVT3FcylGSbk0xStLDxTVKkhMCfAPY+NBGmlzfhM3HNlPug3IcjzvudCwREckmVD4l0yhRuzjfpxgl6ZOD7lGSPu0wzeloOY7L5WJR30X0rNKTA7EHKDuiLLujdzsdS0REsgGVT8l0Lhwl6aFfutBcoyQ54sebf+Sx+o8RczqG0I9CWX1A3wMREbk2Kp+SKV04StIKzyhJfYKncHy7dgF70/vt3mdYq2GcTjpNvc/r8cu2X5yOJCIiWZjKp2RqKUdJauyznK/+6UrNiic1SpKXPdX4Kb7t/i3JNpmO33VkzJ9jnI4kIiJZlMqnZAl1763Jb55RkgKJ0yhJDuhVvRe/9f4NH5cPfaf1ZcjCIU5HEhGRLEjlU7KUs6MkPVJyIltsJY2S5GUtyrbgz35/kts3Ny/Mf4EHpz/odCQREcliVD4lywkIDuCDfT1Z8ctROpwbJSmAZypqlCRvqHZdNbYO3EqBgAKMXDWSzt91djqSiIhkISqfkmVValeBaedGSdrPW9vdoyR933e209GyvVL5ShH1aBSl8pVi+rbp1B1Vl8RkFX8REbk8lU/J8s6OkvR85Ykcpii3j2lDR//ZGiUpg+ULyMeOgTuoXrQ6kQciCf0wlLgEDQogIiKXpvIp2YLL18WQjT35c1Uyt+aZxpwzzTRKkhf4+/qz5oE1tCrbip3ROwn5IITDsYedjiUiIpmYyqdkK6mNklSzyN+M7Piz09GyLZfLxZzec+hdozdH4o5QbkQ5thzd4nQsERHJpFQ+JVs6O0rSsEY/kYA/D87srFGSMthX3b7i2SbPcvLMSWp8WoMle5Y4HUlERDIhlU/Jtly+Lp5a0t0zStLkc6Mk3VNAoyRllNdbvs6H7T/kTPIZbhh7A5M3TXY6koiIZDIqn5LtuUdJ6nZulKSx0RolKSMNqDeAH2/+EYAeP/Tgk5WfOJxIREQyE5VPyTHOjZLUdcb5oyQ9r1GS0luPKj1Y2Gchfi4/Hp75MM/Pe97pSCIikkmofEqOc9/kjuePkvS6e5SkPcv2Oh0tW2lcujHrHlxHHr88vL7ode6efLfTkUREJBNQ+ZQc6ewoSct/PnJulKQ6jXLxbCWNkpSeQguHsvORnRQOLMzX676m9detSU7WoQ4iIjmZyqfkaGGdKjLtdDt+6D+PkhzgzW09qa1RktJV0aCi7H50N2WDy/Lbrt8I/yxcoyGJiORgKp8iQM9PW7H6THWerzyRQxolKd0F+geydeBWIopH8Nfhvyj3QTli4mOcjiUiIg5Q+RTxuNgoSQM0SlK68HX5srLfSjpW7MjemL2EfBDCvph9TscSEREvU/kUuUDKUZKqmE18rFGS0tX0O6bzQO0HOBF/gkofVmL9ofVORxIRES9S+RS5iFYvNGJJQr0LRkn6XaMkpYORnUfyWvPXOJV4ilqjajFv1zynI4mIiJeofIpcwoWjJP2RXFejJKWTwTcM5ovOX5CUnESrr1sx/q/xTkcSEREvUPkUSYOzoyT9/sU2GqUYJenNej9plKRrcG/te5lxxwxcxsUdP93Bu0vfdTqSiIhkMJXPTCQ8Nojw2CCnY8gl1L23JnNTjJL07MruNPRfqVGSrkH7iu1Zft9ycvnk4sk5T/L4r487HUlERDKQsdY6nSFdRERE2MjISKdjSA4SHx3PoGrT+ervVpwkD11zz+LduTUp3fB6p6NlSbtO7CL8s3BiTsdwc5Wb+eHmH5yOJCIiV8kYs8paG5HaY9ryKXKVUo6S1N5/rkZJukZlC5Rl1yO7KBZUjB83/kjT0U01GpKISDak8ilyjVIbJalO7o1MuG+O09GynIKBBdn16C7CCoWxeO9iqnxShfjEeKdjiYhIOlL5FEknPT9tReSpqjxfeSIHuY7bvmxNR//ZbJy6xeloWUqAbwAbHtpA09JN2XJsC2U/KMvxOF1ZQEQku1D5FElHvgG+DNnYk1UrErk18Gdmn2lOw67XaZSkK+RyuVh4z0JurnIzB2MPUnZEWXad2OV0LBERSQcqnyIZoFTdknx/sjMzX15BFbNZoyRdpR9u/oHHGzxOzOkYKn9cmcj9OqnwrGZjm9FsbDOnY4iIXDGVT5EM1Pqlxu5RkhpO5jS5zo2StHykRklKq/favsc7rd/hdNJpGnzRgJnbZjodSUREroHKp0gGc/m6eGppN9btykffAu5Rkpo/qFGSrsQTjZ7gu+7fYbF0+q4TX67+0ulIIiJylVQ+RbwkOCSYL493Y8FnW8+NkhReMZY362uUpLS4vfrtzLlrDj4uH+77+T6GLBzidCQREbkKKp8iXlavX/i5UZJyc4pnV7hHSZr14mKno2V6Lcq24M9+f5LbNzcvzH+B/j/3dzqSiIhcIZVPEYfcN7kja0+U4ZESE9liQ+n0Wn1uCfyZPcv2Oh0tU6t2XTW2DtxKgYACfLb6Mzp/19npSCIicgVUPkUcFBAcwAd/u0dJauc3l4mnOhLRKBfPhWqUpEspla8UUY9GcX2+65m+bTp1R9UlMVnvl4hIVqDyKZIJhHWqyM8J7fj+/rkU5wBvbNUoSZeTLyAfOx/dSfWi1Yk8EEnoh6HEJehaqiIimZ3Kp0gmcsuo1qw6VZXnQydygGLc/mVLOvnP0ihJF+Hr8mXNA2toXa41O6N3EvJBCIdjDzsdS0RELkHlUyST8Q3wZcjmnqxecYZbAmcw60wLGna9joElNEpSalwuF7Pvmk3vGr05EneEciPKseWoyrqISGal8imSSV04StJHB3oSXmQfn3XWKEmp+arbVzzX9DlOnjlJjU9rsGTPEqcjiYhIKjKsfBpjRhtjDhtj1l9mvrrGmERjTM8U0+42xmzz3O7OqIwiWUHKUZLiCaD/9M600ChJqRraYigfd/iYM8lnuGHsDUzeNNnpSCIicoGM3PI5Fmh3qRmMMT7AW8DsFNMKAi8B9YF6wEvGmAIZF1Mk87twlKTlyXVp8WAY9xSYrFGSLvBQ3YeYdMskAHr80IOPVnzkcCIREUkpw8qntXYhcLm/igOBSUDKMwTaAnOstcettSeAOVymxIrkFClHSWro8wdjo7tplKRUdKvcjYV9FuLn8mPgLwN5bu5zTkcSEREPx475NMaUBLoBn17wUEkg5VW293mmpbaMfsaYSGNM5JEjRzImqEgmdHaUpM+6/HxulKRGGiXpPI1LN2bdg+vI45eHNxa/wd2TdQSPiEhm4OQJR8OBQdbaq95cY60dZa2NsNZGFClSJP2SiWQR/aZ2PjdK0uYUoyTtW/m309EyhdDCoex8ZCdFAovw9bqvafV1K5KTtYVYRMRJTpbPCOB7Y0wU0BP4xBjTFfgbuD7FfKU800QkFSlHSWrrN4+JpzpSu56fRknyKBpUlKhHoygXXI65u+YS/lk4CYkJTscSEcmxHCuf1tqy1toQa20IMBF4yFo7BZgFtDHGFPCcaNTGM01ELiGsU0WmJ7TVKEmpCPQPZMvALdQtUZe/Dv9F+Q/LExMf47X1NxvbjGZjm3ltfSIimVlGXmppPLAMCDXG7DPG3GuM6W+M6X+p51lrjwOvASs9t1c900QkDc6OkvRcJY2SlJKvy5cV96+gU8VO7IvZR8gHIeyL2ed0LBGRHCcjz3a/3Vpb3FrrZ60tZa390lo70lo7MpV5+1hrJ6a4P9paW8FzG5NRGUWyK98AX4Zu0ShJqfn5jp/pX6c/J+JPUOnDSqw7tM7pSCIiOYpGOBLJxjRKUuo+7fQprzV/jVOJp6gzqg7zds1zOpKISI6h8imSA1xslKQVo9Y4Hc0xg28YzBedvyApOYlWX7di3F/jnI4kIpIjqHyK5BBnR0lasy2Ie4LdoyQ1e6BSjh4l6d7a9zLjjhm4jIs7f7qTt5e87XQkEZFsT+VTJIcpWKEgo0+cHSVpxblRkoY1zJmjJLWv2J7l9y0nl08unv7taR7/9XGnI4mIZGsqnyI5lHuUpGaM7PQzAcQzaHl3GvuvYM4rS5yO5nURJSLYMmAL+XLlY/gfw7n5x5udjiQikm2pfIrkcA/83Jl1J0ozsMRENtowOrxcj1vz5LxRksoEl2HXI7soHlSciRsn0mR0E42GJCKSAVQ+RYSA4ABG/N2TZVMO0dZvHj/G5cxRkgoGFmTnozsJKxTGkr1LqPxJZeIT452OJSKSrah8isg5VW4KZXpCW8bfm3KUpA380C/njJIU4BvAhoc20LR0U7Ye20rZD8pyPC5nnpAlIpIRVD5F5D9u/SLlKEnFue3znDVKksvlYuE9C7m5ys0cjD1I2RFl2XVil9OxRESyBZVPEUlVylGSbk4xStIjJScSH50zdkX/cPMPPN7gcWJOx1D548pE7o90OpKISJan8ikil1SqbkkmnOzM9Bf+oLLZwof7e1KjwB5G3ZQzRkl6r+17vNP6HU4nnabBFw2YuW2m05FERLI0lU8RSZO2rzZhaUJd3mrwE/EE8MC0zrT0XZAjRkl6otETfNf9OyyWTt914svVXzodSUQky1L5FJE0c/m6eHpZ93OjJC1LqkezByrRt8BkoqOinY6XoW6vfjtz7pqDj8uH+36+j1d/f9XpSCIiWZLKp4hcsbOjJM37dDMNfVYwJrobNcrGZPtRklqUbcGf/f4kt29uXlrwEv1+7ud0JBGRLEflU0SuWoP+tXPcKEnVrqvG1oFbKRBQgM9Xf06n7zo5HUlEJEtR+RSRa/bAz51Zc6RUjhklqVS+UkQ9GsX1+a5nxrYZRIyKIDE551yMX0TkWqh8iki6CCwcmOooSc9n01GS8gXkY+ejO6lxXQ1WHVhFpQ8rEZcQ53QsEZFMT+VTRNLV+aMkHeT1bDxKkq/Llz/7/Unrcq3ZFb2LMh+U4WDsQadjiYhkaiqfIpIh3KMkVfnPKEmbp29zOlq6crlczL5rNnfXvJujcUepMKICW47mjJGgRESuhsqniGSYs6MkRS49fW6UpPqdi2TLUZLGdh3Lc02f4+SZk9T4tAZL9mTfk65ERK6FyqeIZLjSDa//zyhJNQvsznajJA1tMZSPO3zMmeQz3DD2BiZtnOR0JBGRTEflU0S8JuUoSafInS1HSXqo7kNMusVdOnv+2JOPVnzEmsFDWDN4iMPJREQyB5VPEfGqlKMk9QmezNKk+tlulKRulbux+J7F+Lv8GfjLQE5FjHQ6kohIpmGstU5nSBcRERE2MjLS6RgicoWWj1zNcw//y/zkG7mePQxoEMmTi7ri8s36/zfecnQLdUbV4WTCSUgIokLxYpTKW4ryBctTrWg1IkpEULtYbQL9A6942c3GNgNgQZ8F6RtaRCQdGGNWWWsjUn1M5VNEMoPPOv/Mu9ND2UYlGpjlvPpSEq1faux0rGt2OPYwxZ9tTHK+/bhyxZNs/zv8qI/xIY9/HgrnLkzJfCUpV6AcVYtUpXbx2tQtUZd8Afn+8xyVTxHJzFQ+RSRLiDsaxzM1Z/LV/lbEkYfugb/y7oLalKpb0ulo1yS41GIAovc1ITYhlhV/r2DV/lVsPLqRHcd38HfM3xw7dYzYhFiSbNJ/nu8yLvL45aFg7oKUyFuCcgXKsWTPEgoHFmZlv5XefjkiIpel8ikiWcrGqVt4+uYoZp5pTRGOcH/oIl5e0xXfAF+no12VlOXzcuIS4lh9cDWR+yPZcHgD249vZ9+/+zgad5TYhNj/DOOZL1c+moc058lGT9Kk9OWXLyLiDSqfIpIlTbhvDq9/eR3rqEFN1jK4/xF6ftrK6VhX7ErK5+UkJCbw58E/6fVTL47GHeV00mniE93XTM3tm5tG1zfikfqP0KliJ1yurH/crIhkTZcqn/rNJCKZ1tlRkp6tNJG/Kc4tI1vQ2f/XbDdK0pXw9/Wnfqn6lMpXivBi4Zx6/hRze8+lY8WO+Lh8mLtrLjd9fxO5h+am0ZeN+GrNVyQn//c4UxERp6h8ikim5hvgy+tberJq6Wl65p7Br2da0qBzER7NhqMkXa0WZVsw/Y7p/Pvsv6zqt4rbqt5GUK4glu1bRp+pffAf4k/4yHA+WP4BCYkJTscVkRxO5VNEsoTSDa/nhzj3KEmhZgsjPKMkfdFthtPRMpXaxWszvud4jj19jO0Dt3NfrfsokqcIaw+t5bFZjxEwNIDQj0J5ZcErxMTHOB1XRHIglU8RyVLavtqEZQl1eaOee5Sk+6d0pKXv/Gw1SlJ6KV+wPJ93+ZwDTxzgwBMH+F/D/1E6f2m2HtvKy7+/TP638lNmeBn+N+t/HIw96HRcEckhVD5FJMtx+bp45o+UoyQ1oNkDlbi3YPYZJSm9FQsqxrtt3iXqsSj+GfQPrzR7hdBCoez9Zy/vL3+f4u8Wp/i7xblv2n1sO5Zzj6kVkYyn8ikiWVbBCgUZc6Ib8z/dRAPXSkaf6EaNsjG83WgyyYk6yeZi8gXk48UbX2TzgM3EPx/PiHYjCC8WzpGTR/jyzy+p9FElCr5VkNsm3sbqA6udjisi2YwutSQi2cbIjj/z7swwtlMxU42SlJ6XWsrIZSYnJ/PdX9/xaeSnRO6PJCHZfXJSkH8QN5S+gf81/B8ty7VMt/WJSPal63yKSI4RdzSOQTVm8vUB9yhJPQJ/5R2HR0nKKuXzQtO3TOeDPz5g6b6lxJ2JAyDAJ4D6peozoN4Auod117VERSRVus6niOQYgYUD+XB/T5ZNOURbv3n8ENeROvV8GRw2kcT4xMsvQM7pFNqJOb3ncPK5kyy+ZzFdQ7uSyzcXv+/+nZt/vJlcQ3NRd1RdRq0a9Z+Rl0RELkblU0SypSo3hTI9oS3j753LdRxm6JaeROTewMQHf3M6WpbUuHRjJt82mehnovmr/1/cVf0uggOCiTwQyQPTHyDXkFxU+6Qaw5YMIy4hzum4IpKJabe7iGR7ifGJvFhzCp9vvYFjFKaj32ze/qk8YZ0qemX9WXW3e1rs+WcPby15i6mbp/L3v38DYDCUK1COW6rewpMNn6RgYEFHM4qI9+mYTxERYM+yvTzZcg2TT7UjDye5u8RvvLWhEwHBARm63uxcPlM6Hnecd5a9w4QNE9h1YhcW99+XknlL0i2sG081forS+Us7nFJEvEHlU0QkhVkvLubFIX6ssPWpxBae6rqd+yZ3zLD15ZTymVJcQhwjVozg23XfsunoJpKt+9JXRQKL0L5Ce55u/DRVi1Z1OKWIZBSVTxGRCyQnJjOs8RQ+WRHBXkrTwmc+b35WkLr31kz3deXE8plSYnIiX67+ki9Wf8GaQ2vOnZyUP1d+WpRtwVONnqLh9Q0dTiki6UnlU0TkIo5vP84TdX/n++h2GCy3F5jFu6ubExwSnG7ryOnlM6Xk5GQmbprIxys+ZsXfK4hPigcg0C+Qxtc35rEGj9GhYgeHU4rItdKllkRELiLlKEn1NUpShnO5XNxS9RZ+v+d3Tg0+xew7Z9O+QnsMhjk759Dxu47kGpKLJqOb8O3ab0lO1vdAJLtR+RQRARr0r838pBv5tMPP5OI0Ty/rRmP/Ffz22lKno2Vrrcu3ZmavmcQ+F8vK+1dyc5WbyeOXhyV7l3DXlLvwH+JP7c9q8+EfH5KQmOB0XBFJB9rtLiJygbijcTxdfSbfHGxNHIHXPEqSdrtfuW3HtvHWkreYvnU6h04eAtyXcAotFEqvGr14rMFjBPkHOZxSRC5Gx3yKiFyFjVO38NTNu5l5pg1FOcT9lRfx8uqu+Ab4XtFyVD6vzf6Y/by99G1+2vwTe/7Zc256mfxluLnKzTzV6CmKBhV1MKGIXEjlU0TkGnzfdzavjynOX1SnJmt54cGj9PikZZqfr/KZfqLjoxm+bDjjN4xn27Ft564lWiyoGJ0rdebZJs9StkBZh1OKiMqniMg1unCUpE7+sxk2KW2jJKl8Zoz4xHhGRo5kzJoxbDi8gSSbBEDB3AVpU64NgxoPIrx4uLMhRXIolU8RkXSyZ9lenmi5limn2qZ5lCSVz4yXnJzMN+u+YWTkSFYdWMWZ5DMA5PXPyw1lbuDJRk/SLKSZsyFFchBdaklEJJ2Ubng9P8Z1Ytpzywk1Wxixvyc1C+zmi24znI6Wo7lcLu4Ov5tl9y0j/vl4pt42lZZlW5KYnMiMbTNo/lVzcg/NTYuvWjB502RdwknEQdryKSJylZITkxnWaAofr6zLPq6/6ChJzYLXALAgOjzd1q0tn2m3MGoh7y1/j/lR84k5HQOAn8uPWsVq0a9OP+4Ovxtf15WdRCYil6bd7iIiGcg9StJCvo9ui4tkbi84m3dW/f8oSSqfmce6Q+t4a/FbzNoxi2OnjgHgMi6qFqnK3TXv5uF6DxPge/FDKEQkbVQ+RUS8YNnHq3nukX9ZkHwjpdnNgEareOKBOA73eZqi9iCuMqVh6FDo1eua16Xyee12ndjFsCXDmLplKgdiDwDua4mWL1ie26rexhONniA4INjZkCJZlMqniIgXfdphGu/+UoUdVKAhS3mVF2jFPPeDgYEwatQ1F1CVz/R1NO4oby95mx83/siu6F3npl+f73q6hnXl6cZPUypfKQcTimQtKp8iIl4WdzSOp68byzfJvThFbh5lOG8zyP1gmTIQFXVNy1f5zDixCbF8+MeHfLPuG7Yc20KydZ+cVDSwKB0qdeCZxs8QWjjU4ZQimZvKp4iIE1wuNtpK3MeXLKchg3mNV3kZjIFrPNta5dM7EhIT+PLPL/nyzy9Ze2gticmJAAQHBNOybEueavQU9UvVdzilSOaj8iki4oSQENi9m2jy0obfWE1thvAcz5T5QVs+s6Dk5GR+3PgjH6/8mBV/r+B00mkA8vjloUnpJjze4HHaVmjrcEqRzEHX+RQRccLQoRAYSDD/Mo1OVGU9LzKED/M+43QyuQoul4tbq93KwnsWEj84npl3zKRteXfZnLVjFu3GtSNgSABNRzdl/F/jdS1RkYvQlk8RkYw0bhwH73qKovYge4rWpcPhMUQRwkc95tN3YserXqy2fGYuf+z7g7eXvs3cXXOJjo8GwNflS83ratK3Vl/uq3Uf/r7+zoYU8SLtdhcRcVDK63xunLqFLl19OExRvrj/D24Z1fqqlqnymXltOrKJYUuHMXPrTA7HHQbc1xINLRRKr+q9eLTBowT5BzmcUiRjObLb3Rgz2hhz2Biz/iKP32SMWWeMWWOMiTTGNEnx2DBjzAZjzCZjzAhjjMmonCIi3lTlplAmjIkjmGj6f16HGYN+dzqSpLPKRSoz5qYxHHrqEHsf38sj9R6hZN6SbDq6icHzB5P3jbyU+6Acg+YM4mjcUafjinhdRh7zORZod4nH5wI1rbXhQF/gCwBjTCOgMVADqAbUBW7MwJwiIl5Vp08Nvnv/ELk4Td9hYcx/c7nTkSSDlMpXig/af8Cex/dwYtAJBjcdTMWCFYmKjmLY0mEUebsIJd4twYPTH2R39G6n44p4RYaVT2vtQuD4JR6Ptf+/zz8PcPZrCwQA/kAuwA84lFE5RUSc0OSxuox9YQdJ+HDXs6VYMWqN05EkgwUHBPNai9fYOnArcc/H8U7rd6hetDqHTh5i5KqRhHwQQuFhhek1qRfrDq1zOq5IhnH0bHdjTDdjzGZgBu6tn1hrlwHzgQOe2yxr7SbnUoqIZIy2rzZh1IC1/EsQtz6Qn/UTNzsdSbwkwDeAJxo9wboH13F68Gm+6PwF9UrUI+Z0DN+t/46aI2uS/8383DT+JhZGLXQ6rki6crR8WmsnW2vDgK7AawDGmApAZaAUUBJoYYxpmtrzjTH9PMeLRh45csRLqUVE0k/3D1vy0R3LOUxRetxs2DkvyulI4mW+Ll/urX0vf9z/B/HPx/PTLT/RPKQ5Z5LOMG3rNG786kYChwbS6utWTN08VZdwkiwvU1zn07OLvpwxpjDQDVju2S0fC/wCNLzI80ZZayOstRFFihTxYmIRkfRz17h2vNN+LlGEcFPLf9m38m+nI4lDXC4X3Sp3Y97d84h7Po75d8+nU8VO+Lp8mbtrLl0ndCVgaAANv2jImD/HqIhKluRY+TTGVDh7Frsxpjbu4zuPAXuAG40xvsYYP9wnG2m3u4hkaw/O7MKQRjPYRGVuqneQo1t0FrRAs5Bm/HzHz8Q8G8Of/f7k9mq3kzdXXpb/vZy+0/riP8SfmiNrMnz5cOIT452OK+mg2dhmNBvbzOkYGSojL7U0HlgGhBpj9hlj7jXG9DfG9PfM0gNYb4xZA3wM3Oo5AWkisAP4C1gLrLXW/pxROUVEMounlnTnhepT+ZNa3FR5G7EHY52OJJlIePFwvuvxHceePsbOR3byQO0HKJqnKOsOrePxWY8TODSQSh9W4uX5LxMTH+N0XJGL0kXmRUQyWMqLzKfFoPITGbazJ81cv/PLsfoEBAf8Zx5dZF7OOhx7mLeXvs3ETROJio46N/36fNfTo3IPnmr0FCXylXAuoFyRs1s9F/RZ4GiOa6Wx3UVEspC3dvTk4WITWZB8I92KLCYxPtHpSJKJFQ0qyttt3mbXo7v499l/ea35a4QVDmNfzD6G/zGcku+XpNg7xeg7tS/bjm1zOq6IyqeISGb00YGe3BM8mV8TW3Fz8BySE3ViiVxekH8Qg28YzKaHNxH/fDwftv+QWsVqcTTuKGPWjKHSR5Uo+FZBbvnxFiL3a2+hOEPlU0Qkk/riyE3cmmcaU063587g6SqgckX8ff0ZUG8Aqx9YTcLgBL7t/i2Nr2/MyTMn+XHjj9T9vC5BrwfRYVwH5uyY43RcyUFUPkVEMimXr4vvojvRxf8Xxp/sQr/rpp57LDw2iPDYIAfTSVbicrnoVb0Xi/su5vTg08y4YwZtyrXBYvll+y+0+bYNAUMCuHHMjfyw4QddwkkylMqniEgm5vJ1Memf1rTx+Y0vj3djYImJTkeSbKBDxQ7MumsWJ587ydK+S+ke1p0A3wAW7lnIrRNvJdfQXESMiuCzyM9ITNYxx5K+VD5FRDI53wBfph5two2uhXx0oCfPXDea72M6MO+f2hASAuPGOR1RsrCG1zdk0q2TiH4mmvUPrqd3jd4UCCjAqgOr6D+jP7mG5KLaJ9V4c/GbxCXEOR1XsgGVTxGRLCAgOIBpu8NpwDLePnw3n9u+uLCwezf066cCKumiatGqfNXtKw4/dZjdj+1mQN0BFA8qzoYjG3h27rMEvRFE+RHleW7ucxyPO+50XMmiVD5FRLKIfKXy8XOx+wlnDa/yEgP5gLk0IzkuDp5/3ul4ks2Uzl+aDzt8yL7/7ePYU8d4rulzlCtQjl0ndvHG4jco9HYhSr1XiodnPsyef/Y4HVeyEF1kXkQkg13pReYvyeViny1GJ2ayFvfyCnGUKmykcuEjNGqVm5veaERwSPC1r0skFXEJcXy88mO+Xvs1G49uJNm6T04qHFiY9uXbM6jJIKoWrepwyqwrJ1xkXuVTRCSDpWv5DAmB3btJxDCH1iygBX9RjU1UIYqyAPhzmspsJixwD7VrnKHTM1WpclPota9b5AKJyYmMXTOWUatG8efBP8+dnJQvVz5ahLTgyUZP0rh0Y4dTZi0qn1mIyqeIZFbpWj7HjXMf4xmX4sSPwEAYNYqNQRHMeGsjq9b5sOVkaTZSmQRyARDCLsJ8t1Pt+mha3F6M1i80xDfA99rziHgkJyfz0+af+HjFxyz/eznxifEABPoF0qhUIx6t/yidQjs5nDLzU/nMQlQ+RSSzStfyCTBuHAfveoqi9iCuMqVh6FDo1es/s8Xsi2HqoKUsmXOSTceKsjE5jKMUASCYE1Q1mwgrcJAGN/rT9Y0GFA4tnD75RIC5O+fy3rL3WLhnIbEJsQD4+/gTUTyCByIe4M7qd+Jy6dSTC6l8ZiEqnyKSWaV7+bzKZSYnJvP7OyuYM2Yf63blZcuZcmynIgC+nCGMzYTl3k145dN0fCKU8DuqpVteydki90fyztJ3mL1jNifiTwDgY3yofl11+tTsw4MRD+Lv6+9wysxB5TMLUfkUkcwqs5TP1OycF8XUV9YQudqwJbYUG6nMKQIBuJ49VPbZRpUSJ2jWvRDthzTGP0gFQa7NtmPbGLZkGNO3Tedg7EEADIZKhSpxR/U7eKz+Y+QLyOdwSueofGYhKp8iklll5vJ5odiDsUx/bimLf4lh4+EibEwO5RDFAMhLDFXMJirnP0C9hi5uGlKXErWLp+v6JWc5GHuQt5e+zaSNk9j9z+5z08vkL0OPKj14qtFTFAsq5mBC71P5zEJUPkUks8pK5fNCyYnJLPt0NbM/i2Lt9jxsPl2WrVTC4sJFEqFsJTTXLsIrnaL9gPJE9K2By1fH8cmVi4mP4f3l7zN+/Xi2HtuKxd1PigUVo1PFTjzT5BnKFyzvcMqMp/KZhah8ikhmlZXLZ2r2LNvLzy//yYrlyWz+twQbbBVOEgRAcfZT2WcLVa87xg1dgun0RhMCggO8nlGytvjEeD6L/Iwxa8aw/vB6kmwSAAVzF6R1udY83fhpahev7XDKjKHymYWofIpIZpXdyueF4qPjmfnCEn6ffIKNBwuxMakS+ykJQCAnqWI2UjnvfiLqwk0v16JMk9IOJ5asJDk5mW//+paRkSNZtX8VCckJAAT5B3FD6Rt4otETtCjbwuGU6UflMwtR+RSRzCq7l8/UrPxyLb98uI21W3KzJT6EzYSShC+GZCqyjTD/XVQvG0ub+0vT5NEI7aqXNJu+ZTof/PEBS/Yu4VTiKQACfANoULIBA+oNoFtYtyx9CSeVzyxE5VNEMqucWD4vdHDdIaY+v4I/Fiey+Z9ibLCViSEYgCIcpqprM5ULH6Fx2zzcNKwJQcWCnA0sWcLiPYt5d+m7zIuaR8zpGAB8Xb7UKlaLfnX60Se8D76urDWYgspnFqLyKSKZlcrnfyXGJ/LLC4tZ8MNRNuwPZlNiRfZQBoAATlGFTYTm2UedWknc9GJNKrQu53BiyezWH1rPsCXD+GXHLxyNOwqAy7ioUrgKvWv25uG6DxPoH+hwystT+cxCVD5FJCfJ6uUzNeu+38j0dzexZoM/W06VYRNhnMF9XdFy7CDMbwfVy8TQqncJWjzbQLvq5aJ2R+/mrSVvMW3LNP7+92/AfS3RcgXKcWu1W3miwRMUDCzocMrUqXxmISqfIpKTZMfyeaHj248z5ZllLF9wmk3Hr2ODrcwJ3IWhIMeoYjZRpdBhGrYIoOtbjQgOCXY2sGRKR+OO8u7Sd/lh4w/sOrHr3CWcSuUrRbewbjzd+GlK5SvlcMr/p/KZhah8ikhOkhPK54US4xOZ/9Yf/PbNAdbvycemMxXYhXt3vB8JVGYTYYF7qFXtDJ0HVaFq9zCHE0tmE5cQx4gVI/hm7TdsPraZZJsMQJHAInSo2IFBjQdRuUhlRzOqfGYhKp8ikpPkxPKZmq2/bmfqa+tYvdaXzSevZxOVOY37uqJliKKy73aqljpB81uK0vaVxvgGZK2TTyTjJCYn8uXqL/l89eesPbSWxOREAPLnyk/Lsi15uvHT1C9V3+u5VD6zEJVPEclJVD5TF7MvhmnPLGXJ7JNsPFaUjclhHKUIAPmJpqrZRFjwQeo39aXr6/UpWrWow4klM0hOTubHjT/yycpP+OPvPziddBqAQL9AmlzfhMcaPEb7iu29kkXlMwtR+RSRnETlM22SE5NZ9P5KZn+xl3W7gthyphzbqASAD4mEsZmwgN3UrBxPx8cqUbt3dYcTS2YwZ8cc3l/+Pgt3L+TkmZMA5PLJRUSJCB6s+yC3V709w64lqvKZhah8ikhOovJ59aIW7mHKi6tZtdqwObYkG2wVTuG+BE9J9lHZZytVix+nWfdCdBjaGP8gf4cTi5NW/r2SYUuHMXfnXE7EnwDAx/hQ87qa3FPrHvrV7oe/b/p9RlQ+sxCVTxHJSVQ+00/c0Th+fmYJi2f+w8bDhdmYFMpBigMQxL/u4UHzHaBuA8NNr0VQqm5JhxOLU7Yc3cJbS95ixrYZHD55GHBfwim0cCi9qvfisQaPEeR/bQMkqHxmISqfIpKTqHxmnOTEZP74bA2/jtzJum152Hw6hK1UIhkfXCRRia2E5YqiRsWTtOtfjvoPhOuaoznQ/pj9vL30bSZtmsTemL3npocEh3BzlZt5suGTFA268mOKVT6zEJVPEclJVD69a9/Kv5n6QiQrl1s2xRRno61CLHkBKMYBqvhsoUrRozTpkJ/ObzYmsHDmH0lH0k90fDTDlw3nu/Xfsf349nPXEi0eVJwulbowqMkgyhYom6ZlBZda7F7mvibpls+JQqvyKSKSzah8Ois+Op5fX1rGgp+OseFAQTYnVWQf1wMQyEmqmE2E5vmbiDqWrq/WJuSG0g4nFm+JT4xnZORIxqwZw/rD689dS7RQ7kK0Kd+GZ5o8Q43ralz0+SqfWYjKp4jkJCqfmc+qseuYOWIbazcFsDm+DJsJIwn3dUUrspVQv53UKBtLm/uup+njdbWrPgdITE7km7XfMGrVKFYdWMWZ5DMA5PXPy41lbuSJRk/QLKTZec9R+cxCVD5FJCdR+cz8Dm84zJTn/uCPRYlsji7GeluZGIIBKMxhqri2UKXQYRq3yUOXNxuRr1Q+ZwNLhkpOTubnrT8z4o8RLNu3jFOJpwDI7ZubhqUaMqDeAG4KvYmCpZcC6Vs+M6LQXo7Kp4hINqPymfUkxicy66UlzJ9whA1/B7MpsQK7CQEgF/Hu4UHz7KV2zUQ6P1udsE4VnQ0sGWph1ELeXfYu86Pm82/CvwD4ufxIPlAR/7W3E7dkcLqtS+Uzg6h8ikhOovKZPayfuJnpwzby5wY/NseVZhOVOYP7mpFl2Ullv+1UKx1DizuK0frFRtpVn02tO7SOtxa/xawdszgWdwyAioUqMvGWiZc8PjStVD4ziMqniOQkKp/Z0/Htx5n2/HKWzYt3Dw9qK3OcQgAU4DhVzSYqFzxEg2a56PpmQwpWKOhwYklveStPIK7FyyQX3QxA2/Jt+b7n9wQHBF/1MlU+M4jKp4jkJCqfOUNyYjLz3ljOb1/v56/d+dhyphw7qACAHwlUZjOhuXcTXjWBLoOqUq1nmMOJ5VqdLYqTFp7mzsl3cjD2IL4uXx5v8Dhvtnzzqob1VPnMICqfIpKTqHzmXFt/3c7PQ/9i1Z8+bDlZio1UJp7cAJRmN2E+26hWMppmtxSm/WtN8A3wdTixXIkLi+LbS97mhfkvcDrpNMEBwXzW6TNuqXrLNS3TG1Q+RUSyGZVPOSv2YCxTn17Mklkn2XS0CBuSwziCe2SdfERTxWymcv4D1G/iy01D61GsxnUOJ5ZLSa0oxifGc8+Ue5iwYQIWS1jhMCbePJGqRate9TIzmsqniEg2o/IpF5OcmMziDyKZ/fke/toVxOaEsmyjIhYXPiQSyhbCAqKoGXqK9gMrUvfemk5HlhQuVRR3ndhFjx968OfBPwHoVLET43uOv+x48iqfGUTlU0RyEpVPuRK7F+9hyourWbXKsOnfEmy0VYgjDwAl+JsqPlupUuwYN3QpQMfXGxMQHOBw4pwrLUVx5raZ9JnShyNxR/Bz+TGo8SBeafbKRY8HVfnMICqfIpKTqHzKtYg7GsfMF5aycFo0Gw4VYlNSKAcoAUAQ/1LFbCIs734i6hluerU2pRte73DinONKiuKQhUN4beFrJCQlUDB3Qb7o/AXdKne7pmWmF5VPEZFsRuVT0lNyYjKRo9fxy0c7WLM1N1tOl2ULlUjGB0MyldhKWK5d1Ch/krb9Q2j4YG1dczSDXGlRjEuI467JdzF582QslmpFqjHxlomEFg696mWmB5VPEZFsRuVTMtr+1QeYOnglK5Yls+mf4my0lfkX9xCg13GQyq4tVCl6hKbt89FlWBMCCwc6nDh7uNqiuO3YNnr80IO/Dv+FwdA1rCvfdvuWQP9Alc+MovIpIjmJyqd4W0JsAr8MXsKCn46xYX9BNidVYC+lAchNnHt40KB9RNS23PRSOOVahDgbOIu61qI4dfNU7p12L8dOHcPfx5/nmz7Pe71aXNMyr4bKp4hINqPyKZnBmu/WM+PdLazZlIvNp8qwmTAS8QOgPNsJ89tBjbL/0rJ3CZoPaqBd9WmQHlspk5OTefn3l3lz8ZucST6DORVM7gXPcXL5U+kV87JUPkVEshmVT8mMjm45ypRn/mD5wtNsPlGM9bYK/xAMQGGOUMW1mcqFDtOwZW5ueqMRwSHBjubNjNJzF3lsQiy9JvVi2pZpAEy9fSpdQrtc83LTQuVTRCSbUfmUrCAxPpE5ry5j3vcHWb83mM2JFYiiLAD+nKYKmwjNs4fa1RPp8nx1wjpVdDix8zLi+Mygqt9yuu7nnB49/6qG57waKp8iItmMyqdkVRt+2syMtzeyep0fm+NKs4kwEsgFQAi7qOy7narXR9Pi9mK0fqFhjhseNCPKZ2Y74ShnfUdFRETEUVW7h1G1e9i5+9FR0Ux9di5LfzvFpuPXsSKxFr/sKsw7r0Pw6yeoajYSVuAQDW70p+sbDSgcWtjB9JIeVD5FRETEMcEhwdw9vgN3e+4nJyYz/62l/PbVfv6KysvmM+VZcrwxX06GByefIYy/CMu9m/DKp+n8VGVq3FbF0fxy5VQ+RUREJNNw+bpo+XwjWj7//9O2z9nJtNfWsupPHzbHlmT6qZZMXJ2bwbfD9bfvobLPNqqWPMGN3QrRfkhj/IP8nXsBclk65lNERESylNiDsUx7ZgmLf/2XTUeKsDE5jMNcB0A+/nEPD5r/APUa+nDTkLqUqF3c4cRpp2M+RURERDKZoGJB3DG2LXd47icnJrP0o5X8Omo3f+3Iw+aEcnwVXY+xv7gY8EsSoWwkLCCKmhVP0X5Aeer1C3cyfo6nLZ8iIiKS7exevIdpr/zJyhWWzf+WZKOtzEmCACjOfqr4bKHKdce4oUswnd5oQkBwgMOJ3bTlU0RERCQLKtOkNAPnlD53Pz46nhnPzWXh1BNsOFSIjUmhzN1fgg9HQp6RsVQxKwnLu5+IutD1tdqUbni9g+mzN235FBERkRxp5Zdr+WXEdtZszc3m+BC2EEoyPhiSqcg2wvx3Ub18LO36laHRgDpeGR40J2z5VPkUERERAQ6uO8SUZ1fwx9JENv9TnI22MjHkB6Aoh6ji2kLlIodp0i4vXd5sTFCxoHTPoPKZhah8ioiISHpKiE3g15eWsmDiUTb8HcympErsxb0rP4BTVGETYUH7qB2exE0v1qRC63LXvE6VzyxE5VNEREQy2rrvN/Lz25tYsykXW06VYRNhJOIHQHm2E+a3g2pl/qVV7xK0eLbBFe+qV/m8tpWOBjoBh6211VJ5/CbgNSAZSAQes9Yu9jxWGvgCuB6wQAdrbdSl1qfyKSIiIt52dMtRpj3/B8vmn2bTievYYKsQTQEACnGUKq7NVC54mIYtAuj6ViOCQ4IvuTyVz2tb6Q1ALPD1RcpnEHDSWmuNMTWAH6y1YZ7HFgBDrbVzPPMlW2vjLrU+lU8RERFxWmJ8InNfX868cQf5a09+NiVWIIqyAPhzmspsJjRwD7WqnaHzoCrnjXPPuHHsufsJSiUdxlWmNAwdCr16XXOmHFM+PSsOAaanVj4vmK8hMNpaW9kYUwUYZa29ondI5VNEREQyo41TtzDjrY2sWufDlpOl2UhlEsgFQAi7CPPdQdXgfbQ8MYnWSTPwxdPNAgNh1KhrK6AZVGgvJ9OWT2NMN+ANoCjQ0Vq7zBjTFbgPSADKAr8Bz1hrky61LpVPERERyQpi9sUwddBSlsw5yaZjRdmYHMZRigCQn2hCiMKPBPw5gy9n8Pex+JpE/F1J+LkS8fNJxt8nGT/fZPz9kvH3g1y5ICAA/HMZcudxkSvQReCJ/eRetYTApGjycJLGLCEg0O/aC20aZNrymWK+G4AXrbWtjDE9gS+BWsAeYAIw01r7ZSrP6wf0AyhdunSd3bt3p/MrEBEREclYyYnJ/O7Xgjm0Zh01OEJRzuBHAn6cwZ+Eczc/EsjFaXJxmisfkelPavB/7d1/rF93Xcfx52ursnVzQ+yG4raWAZm4H1y3oSBMxw9xLhCY02yxmCDqLGxBgxMzh0qU/mEWXQyZYKdLNdRFozAHKBkZLHVIYT+8ZR37YYC2G7BfsrKVFra1b//4nku+u73f297eez/n28vzkSz5ns8533Pe5/PJbV77nHO+Z4K7YOVK2Lp14U9kyNi/4aiqNiY5OckK4EFgsqq+ApDkBuAVDALp9O+tA9bBYOazXcWSJEkL47Blh/GalVt5zbb37rtyRFDc+8xedj60kx0PfIudj+xi58O7efLRXeza8RS7Hn+a3U8+ze5/uZHvcAS7OZLv8oOsotvP9u2Lej7701v4TPJi4MvdA0dnAs8B/g94HHhukuOq6lHgtYDX0yVJ0tK1di1ccgnsGnq+evnyQfsMDlt2GMeccAzHnHDM6H1+/p0w01Xhk07at62hRXtPVJLrgc8BpyR5MMlvJlmTZE23yYXAliSTwDXARTWwB7gcuDnJXUCAaxerTkmSpN6tXg3r1rH98OezlwxmPOd7b+batYMAO2yWQNuKPzIvSZI0Jhb8Z5E2bOChX/8Djq+HxuZp97G451OSJEmLYPVqLr70VABu2TrRby2dRbvsLkmSJE1n+JQkSVIzXnaXJEkaExM7j+67hEXnzKckSZKaMXxKkiSpGcOnJEmSmjF8SpIkqRnDpyRJkpoxfEqSJKkZw6ckSZKaMXxKkiSpGcOnJEmSmjF8SpIkqRnDpyRJkpoxfEqSJKkZw6ckSZKaMXxKkiSpGcOnJEmSmjF8SpIkqRnDpyRJkpoxfEqSJKkZw6ckSZKaMXxKkiSpGcOnJEmSmjF8SpIkqRnDpyRJkpoxfEqSJKkZw6ckSZKaWdZ3AZIkSVo8t+yY6LuEZzF8SpIkjYlxC4qLwcvukiRJasbwKUmSpGYMn5IkSWrG8ClJkqRmDJ+SJElqxvApSZKkZgyfkiRJasbwKUmSpGYMn5IkSWrG8ClJkqRmDJ+SJElqxvApSZKkZgyfkiRJasbwKUmSpGYMn5IkSWrG8ClJkqRmDJ+SJElqxvApSZKkZgyfkiRJasbwKUmSpGYMn5IkSWrG8ClJkqRmDJ+SJElqxvApSZKkZgyfkiRJasbwKUmSpGZSVX3XsCCSPAps67uOeVoBPNZ3EVpQjunS4nguPY7p0uOYjoeVVXXcTCuWTPhcCpLcXlVn912HFo5jurQ4nkuPY7r0OKbjz8vukiRJasbwKUmSpGYMn+NlXd8FaME5pkuL47n0OKZLj2M65rznU5IkSc048ylJkqRmDJ89SXJdkkeSbBlq+9UkdyfZm8Qn9Q4xI8b0qiT3Jvliko8meW6PJWoORoznn3djOZnkpiQv6LNGzc1MYzq07veTVJIVfdSmuRvxN/q+JF/r/kYnk5zfZ42ameGzP+uB86a1bQF+GdjYvBothPXsO6afAk6rqjOA+4ErWhelg7aefcfzqqo6o6omgI8Df9K6KM3LevYdU5KcCLwB2N66IM3LemYYT+Dqqpro/vuPxjXpABg+e1JVG4FvTmu7p6ru66kkzdOIMb2pqp7pFjcBJzQvTAdlxHg+MbR4FOBN84eQmca0czXwHhzPQ8os46kxZ/iU2nk78J99F6H5SbI2yQPAapz5POQleTPwtara3HctWjCXdbfHXJfkh/suRvsyfEoNJLkSeAbY0Hctmp+qurKqTmQwlpf1XY8OXpLlwB/h/0QsJR8EXgRMAN8A/rLXajQjw6e0yJK8DXgjsLr8bbOlZANwYd9FaF5eBLwQ2JxkK4PbYu5M8qO9VqWDVlUPV9WeqtoLXAv8dN81aV/L+i5AWsqSnMfgXrKfr6pdfdej+Unykqr6327xzcC9fdaj+amqu4Djp5a7AHp2VT3WW1GalyQ/VlXf6BYvYPAgr8aM4bMnSa4HzgVWJHkQ+FMGN05/ADgO+ESSyar6xf6q1FyMGNMrgOcAn0oCsKmq1vRWpA7YiPE8P8kpwF5gG+BYHkJmGtOq+vt+q9LBGvE3em6SCQYPj20Ffqev+jSabziSJElSM97zKUmSpGYMn5IkSWrG8ClJkqRmDJ+SJElqxvApSZKkZgyfkiRJasbwKUmSpGYMn5LGTpLnJ/mnJF9JckeSzyW5YIH2vSrJQb/1JMl/L0QdczzmqiS7k0wOte1coP3O2BctzjPJ+5JcnuTIJJNJnkqyYrGPK6lfhk9JYyWDV0HdAGysqpOr6izgYgbv3X7Wdkma/xtWVT/b+pidL1fVRKuDtTzPqtrdndvXWx1TUn8Mn5LGzWuBp6rqQ1MNVbWtqj7QzdTdl+QfGbyz+cQkN3Szo3cnuQS+N6N3b5INSe5J8q9Jlg8d4/Ak13bfuSnJkcMFJDkqySeSbE6yJclFQ+t2JlnTzdRNJvlqks8MrX9rki906/42yeGznWySjyR5f5KNSbYnef1cOivJu7satyT5vaH2P+766tYk1ye5fMQuls3UT1Mzq11f3jO9v0a1768fklyZ5P4ktwKnzOVcJS0Nhk9J4+ZU4M5Z1r8E+JuqOrWqtgFv72ZHzwbeleRHuu1O6bZ7KfAE8M5p+7imqk4FdgAXTjvGecDXq+plVXUa8MnhlVX1oW6m7uXAg8BfASR5KXAR8Kpu/R5g9X7O93RgR1X9HPC7B7D99yQ5C/gN4GeAVwC/neSnkry8O6eXAb/EoG9Gma2fpozqrxnbR/VDV+/FwARwPoP+k/R9xvApaawluaabgbyta9pWVZuGNnlXks3AJuBEBoEI4IGq+mz3+cPAq4e+89Wqmuw+3wGsmnbYu4BfSPIXSc6pqm+NKO+vgU9X1ce65dcBZwG3dfdnvg44eZZzWw4cC1zdNf0AgxB3oF4NfLSqvl1VO4GPAOcArwL+vaq+U1VPAh+bZR+z9dOUUf01qn1UP5zT1burqp4AbjzwU5W0VCzruwBJmuZuhmYiq+rS7iGU27umb0+tS3Iu8HrglVW1K8ktwBFTX5223+Hl7w593gM867J7Vd2f5EwGs3PvT3JzVf3Z8DZJ3gasBC4bbgb+oaqu2O9ZDvwkcEdV7emWz2BwO0FLs/XTlFH9Nap9xn4Yvi1A0vcvZz4ljZtPA0ckecdQ2/IR2x4LPN4Fz59gcOl5yklJXtl9/jXg1gMtIMkLgF1V9WHgKuDMaevPAi4H3lpVe4dW3Qz8SpLju+2el2Rl9/nmJD8+7VCnA5NDy2cAXzzQOoH/At6SZHmSo4ALurbPAm9KckSSo4E3zrKPg+6nWYzqh41dvUcm+SHgTQtwLEmHGGc+JY2VqqokbwGuTvIe4FEGs51/OMPmnwTWJLkHuI/Bpfcp9wGXJrkO+BLwwTmUcTpwVZK9wNPAO6atvwx4HvCZJAC3V9VvVdWXkrwXuKl7Ev/proYHgBcD35zhOJ8fWj6NOcx8VtWdSdYDX+ia/q6q/gcgyY0MguzDDG4jGHXrwHz6aVRdM/ZDVW1K8s/AZuAR4LbZ9iNpaUrVTFdYJOnQlWQV8PHuYaHeJTmNwYNR7z7I769ijueT5Oiq2tndV7oRuKSqZnuQq3dJtgJnV9VjfdciafF42V2SFllVbTnY4NnZAxyboR+ZPwDruu3vBP5tnINndxl+ksEDV3v3s7mkQ5wzn5IkSWrGmU9JkiQ1Y/iUJElSM4ZPSZIkNWP4lCRJUjOGT0mSJDVj+JQkSVIzhk9JkiQ1Y/iUJElSM/8PnOeuiDF8EO4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 792x792 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(11,11))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.log(g_sizes), he_dt['mean'].values, color = 'r', marker = 'o', label = 'Hill estimator')\n",
    "plt.errorbar(np.log(g_sizes), he_dt['mean'].values, yerr=he_dt['std'].values, color = 'r')\n",
    "ax.plot(np.log(g_sizes), ke_dt['mean'].values, color = 'g', label = 'Kernel type estimator')\n",
    "plt.errorbar(np.log(g_sizes), ke_dt['mean'].values, yerr=ke_dt['std'].values, color = 'g')\n",
    "ax.plot(np.log(g_sizes), me_dt['mean'].values, color = 'b', label = 'Moments estimator')\n",
    "plt.errorbar(np.log(g_sizes), me_dt['mean'].values, yerr=me_dt['std'].values, color = 'b')\n",
    "ax.set_xlabel('Graph size, $n$ [log binned]')\n",
    "ax.set_ylabel('Tail estimation')\n",
    "ax.legend()\n",
    "plt.title('Compare three tail estimation on 6 graphs of different sizes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01a5b3-b208-4bae-9b86-280c45ba4fcc",
   "metadata": {},
   "source": [
    "auxiliary codes hidden vvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69e9c30d-286d-4a40-bcab-c26eb7e18ce0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# sequence_file_path = os.getcwd()+'sth.txt'\n",
    "# output_file_path = os.getcwd()+'sth.pdf'\n",
    "\n",
    "# =========================================\n",
    "# ========== Auxiliary Functions ==========\n",
    "# =========================================\n",
    "\n",
    "def add_uniform_noise(data_sequence, p = 1):\n",
    "    \"\"\"\n",
    "    Function to add uniform random noise to a given dataset.\n",
    "    Uniform noise in range [-5*10^(-p), 5*10^(-p)] is added to each\n",
    "    data entry. For integer-valued sequences, p = 1.\n",
    "\n",
    "    Args:\n",
    "        data_sequence: numpy array of data to be processed.\n",
    "        p: integer parameter controlling noise amplitude.\n",
    "\n",
    "    Returns:\n",
    "        numpy array with noise-added entries.\n",
    "    \"\"\"\n",
    "    if p < 1:\n",
    "        print(\"Parameter p should be greater or equal to 1.\")\n",
    "        return None\n",
    "    noise = np.random.uniform(-5.*10**(-p), 5*10**(-p), size = len(data_sequence))\n",
    "    randomized_data_sequence = data_sequence + noise\n",
    "    # ensure there are no negative entries after noise is added\n",
    "    randomized_data_sequence = randomized_data_sequence[np.where(randomized_data_sequence > 0)]\n",
    "    return randomized_data_sequence\n",
    "\n",
    "\n",
    "def get_distribution(data_sequence, number_of_bins = 30):\n",
    "    \"\"\"\n",
    "    Function to get a log-binned distribution of a given dataset.\n",
    "\n",
    "    Args:\n",
    "        data_sequence: numpy array with data to calculate\n",
    "                       log-binned PDF on.\n",
    "        number_of_bins: number of logarithmic bins to use.\n",
    "\n",
    "    Returns:\n",
    "        x, y: numpy arrays containing midpoints of bins\n",
    "              and corresponding PDF values.\n",
    "\n",
    "    \"\"\"\n",
    "    # define the support of the distribution\n",
    "    lower_bound = min(data_sequence)\n",
    "    upper_bound = max(data_sequence)\n",
    "    # define bin edges\n",
    "    log = np.log10\n",
    "    lower_bound = log(lower_bound) if lower_bound > 0 else -1\n",
    "    upper_bound = log(upper_bound)\n",
    "    bins = np.logspace(lower_bound, upper_bound, number_of_bins)\n",
    "    \n",
    "    # compute the histogram using numpy\n",
    "    y, __ = np.histogram(data_sequence, bins = bins, density = True)\n",
    "    # for each bin, compute its midpoint\n",
    "    x = bins[1:] - np.diff(bins) / 2.0\n",
    "    # if bin is empty, drop it from the resulting list\n",
    "    drop_indices = [i for i,k in enumerate(y) if k == 0.0]\n",
    "    x = [k for i,k in enumerate(x) if i not in drop_indices]\n",
    "    y = [k for i,k in enumerate(y) if i not in drop_indices]\n",
    "    return x, y\n",
    "\n",
    "def get_ccdf(degree_sequence):\n",
    "    \"\"\"\n",
    "    Function to get CCDF of the list of degrees.\n",
    "    \n",
    "    Args:\n",
    "        degree_sequence: numpy array of nodes' degrees.\n",
    "\n",
    "    Returns:\n",
    "        uniques: unique degree values met in the sequence.\n",
    "        1-CDF: CCDF values corresponding to the unique values\n",
    "               from the 'uniques' array.\n",
    "    \"\"\"\n",
    "    uniques, counts = np.unique(degree_sequence, return_counts=True)\n",
    "    cumprob = np.cumsum(counts).astype(np.double) / (degree_sequence.size)\n",
    "    return uniques[::-1], (1. - cumprob)[::-1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb7cf2-7696-43da-b55a-a7a61a482f91",
   "metadata": {},
   "source": [
    "##### Hill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9efdfed4-980d-4dc6-b926-81826c2889dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# ========== Hill Tail Index Estimation ==========\n",
    "# ================================================\n",
    "def get_moments_estimates_1(ordered_data):\n",
    "    \"\"\"\n",
    "    Function to calculate first moments array given an ordered data\n",
    "    sequence. Decreasing ordering is required.\n",
    "\n",
    "    Args:\n",
    "        ordered_data: numpy array of ordered data for which\n",
    "                      the 1st moment (Hill estimator)\n",
    "                      is calculated.\n",
    "    Returns:\n",
    "        M1: numpy array of 1st moments (Hill estimator)\n",
    "            corresponding to all possible order statistics\n",
    "            of the dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    logs_1 = np.log(ordered_data)\n",
    "    logs_1_cumsum = np.cumsum(logs_1[:-1])\n",
    "    k_vector = np.arange(1, len(ordered_data))\n",
    "    M1 = (1./k_vector)*logs_1_cumsum - logs_1[1:]\n",
    "    return M1\n",
    "\n",
    "def get_moments_estimates_2(ordered_data):\n",
    "    \"\"\"\n",
    "    Function to calculate first and second moments arrays\n",
    "    given an ordered data sequence. \n",
    "    Decreasing ordering is required.\n",
    "\n",
    "    Args:\n",
    "        ordered_data: numpy array of ordered data for which\n",
    "                      the 1st (Hill estimator) and 2nd moments \n",
    "                      are calculated.\n",
    "    Returns:\n",
    "        M1: numpy array of 1st moments (Hill estimator)\n",
    "            corresponding to all possible order statistics\n",
    "            of the dataset.\n",
    "        M2: numpy array of 2nd moments corresponding to all \n",
    "            possible order statistics of the dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    logs_1 = np.log(ordered_data)\n",
    "    logs_2 = (np.log(ordered_data))**2\n",
    "    logs_1_cumsum = np.cumsum(logs_1[:-1])\n",
    "    logs_2_cumsum = np.cumsum(logs_2[:-1])\n",
    "    k_vector = np.arange(1, len(ordered_data))\n",
    "    M1 = (1./k_vector)*logs_1_cumsum - logs_1[1:]\n",
    "    M2 = (1./k_vector)*logs_2_cumsum - (2.*logs_1[1:]/k_vector)*logs_1_cumsum\\\n",
    "         + logs_2[1:]\n",
    "    return M1, M2\n",
    "\n",
    "def get_moments_estimates_3(ordered_data):\n",
    "    \"\"\"\n",
    "    Function to calculate first, second and third moments \n",
    "    arrays given an ordered data sequence. \n",
    "    Decreasing ordering is required.\n",
    "\n",
    "    Args:\n",
    "        ordered_data: numpy array of ordered data for which\n",
    "                      the 1st (Hill estimator), 2nd and 3rd moments \n",
    "                      are calculated.\n",
    "    Returns:\n",
    "        M1: numpy array of 1st moments (Hill estimator)\n",
    "            corresponding to all possible order statistics\n",
    "            of the dataset.\n",
    "        M2: numpy array of 2nd moments corresponding to all \n",
    "            possible order statistics of the dataset.\n",
    "        M3: numpy array of 3rd moments corresponding to all \n",
    "            possible order statistics of the dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    logs_1 = np.log(ordered_data)\n",
    "    logs_2 = (np.log(ordered_data))**2\n",
    "    logs_3 = (np.log(ordered_data))**3\n",
    "    logs_1_cumsum = np.cumsum(logs_1[:-1])\n",
    "    logs_2_cumsum = np.cumsum(logs_2[:-1])\n",
    "    logs_3_cumsum = np.cumsum(logs_3[:-1])\n",
    "    k_vector = np.arange(1, len(ordered_data))\n",
    "    M1 = (1./k_vector)*logs_1_cumsum - logs_1[1:]\n",
    "    M2 = (1./k_vector)*logs_2_cumsum - (2.*logs_1[1:]/k_vector)*logs_1_cumsum\\\n",
    "         + logs_2[1:]\n",
    "    M3 = (1./k_vector)*logs_3_cumsum - (3.*logs_1[1:]/k_vector)*logs_2_cumsum\\\n",
    "         + (3.*logs_2[1:]/k_vector)*logs_1_cumsum - logs_3[1:]\n",
    "    # cleaning exceptional cases\n",
    "    clean_indices = np.where((M2 <= 0) | (M3 == 0) | (np.abs(1.-(M1**2)/M2) < 1e-10)\\\n",
    "                             |(np.abs(1.-(M1*M2)/M3) < 1e-10))\n",
    "    M1[clean_indices] = np.nan\n",
    "    M2[clean_indices] = np.nan\n",
    "    M3[clean_indices] = np.nan\n",
    "    return M1, M2, M3\n",
    "\n",
    "def hill_dbs(ordered_data, t_bootstrap = 0.5,\n",
    "            r_bootstrap = 500, eps_stop = 1.0,\n",
    "            verbose = False, diagn_plots = False):\n",
    "    \"\"\"\n",
    "        Function to perform double-bootstrap procedure for\n",
    "        Hill estimator.\n",
    "\n",
    "        Args:\n",
    "            ordered_data: numpy array for which double-bootstrap\n",
    "                          is performed. Decreasing ordering is required.\n",
    "            t_bootstrap:  parameter controlling the size of the 2nd\n",
    "                          bootstrap. Defined from n2 = n*(t_bootstrap).\n",
    "            r_bootstrap:  number of bootstrap resamplings for the 1st and 2nd\n",
    "                          bootstraps.\n",
    "            eps_stop:     parameter controlling range of AMSE minimization.\n",
    "                          Defined as the fraction of order statistics to consider\n",
    "                          during the AMSE minimization step.\n",
    "            verbose:      flag controlling bootstrap verbosity. \n",
    "            diagn_plots:  flag to switch on/off generation of AMSE diagnostic\n",
    "                          plots.\n",
    "\n",
    "        Returns:\n",
    "            k_star:     number of order statistics optimal for estimation\n",
    "                        according to the double-bootstrap procedure.\n",
    "            x1_arr:     array of fractions of order statistics used for the\n",
    "                        1st bootstrap sample.\n",
    "            n1_amse:    array of AMSE values produced by the 1st bootstrap\n",
    "                        sample.\n",
    "            k1_min:     value of fraction of order statistics corresponding\n",
    "                        to the minimum of AMSE for the 1st bootstrap sample.\n",
    "            max_index1: index of the 1st bootstrap sample's order statistics\n",
    "                        array corresponding to the minimization boundary set\n",
    "                        by eps_stop parameter.\n",
    "            x2_arr:     array of fractions of order statistics used for the\n",
    "                        2nd bootstrap sample.\n",
    "            n2_amse:    array of AMSE values produced by the 2nd bootstrap\n",
    "                        sample.\n",
    "            k2_min:     value of fraction of order statistics corresponding\n",
    "                        to the minimum of AMSE for the 2nd bootstrap sample.\n",
    "            max_index2: index of the 2nd bootstrap sample's order statistics\n",
    "                        array corresponding to the minimization boundary set\n",
    "                        by eps_stop parameter.\n",
    "\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Performing Hill double-bootstrap...\")\n",
    "    n = len(ordered_data)\n",
    "    eps_bootstrap = 0.5*(1+np.log(int(t_bootstrap*n))/np.log(n))\n",
    "    n1 = int(n**eps_bootstrap)\n",
    "    samples_n1 = np.zeros(n1-1)\n",
    "    good_counts1 = np.zeros(n1-1)\n",
    "    k1 = None\n",
    "    k2 = None\n",
    "    min_index1 = 1\n",
    "    min_index2 = 1\n",
    "    while k2 == None:\n",
    "        # first bootstrap with n1 sample size\n",
    "        for i in range(r_bootstrap):\n",
    "            sample = np.random.choice(ordered_data, n1, replace = True)\n",
    "            sample[::-1].sort()\n",
    "            M1, M2 = get_moments_estimates_2(sample)\n",
    "            current_amse1 = (M2 - 2.*(M1)**2)**2\n",
    "            samples_n1 += current_amse1\n",
    "            good_counts1[np.where(current_amse1 != np.nan)] += 1\n",
    "        averaged_delta = samples_n1 / good_counts1\n",
    "        \n",
    "        max_index1 = (np.abs(np.linspace(1./n1, 1.0, n1) - eps_stop)).argmin()\n",
    "        k1 = np.nanargmin(averaged_delta[min_index1:max_index1]) + 1 + min_index1 #take care of indexing\n",
    "        if diagn_plots:\n",
    "            n1_amse = averaged_delta\n",
    "            x1_arr = np.linspace(1./n1, 1.0, n1)\n",
    "        \n",
    "        # second bootstrap with n2 sample size\n",
    "        n2 = int(n1*n1/float(n))\n",
    "        samples_n2 = np.zeros(n2-1)\n",
    "        good_counts2 = np.zeros(n2-1)\n",
    "    \n",
    "        for i in range(r_bootstrap):\n",
    "            sample = np.random.choice(ordered_data, n2, replace = True)\n",
    "            sample[::-1].sort()\n",
    "            M1, M2 = get_moments_estimates_2(sample)\n",
    "            current_amse2 = (M2 - 2.*(M1**2))**2\n",
    "            samples_n2 += current_amse2\n",
    "            good_counts2[np.where(current_amse2 != np.nan)] += 1\n",
    "        max_index2 = (np.abs(np.linspace(1./n2, 1.0, n2) - eps_stop)).argmin()\n",
    "        averaged_delta = samples_n2 / good_counts2\n",
    "        \n",
    "        \n",
    "        k2 = np.nanargmin(averaged_delta[min_index2:max_index2]) + 1 + min_index2 #take care of indexing\n",
    "        if diagn_plots:\n",
    "            n2_amse = averaged_delta\n",
    "            x2_arr = np.linspace(1./n2, 1.0, n2)\n",
    "\n",
    "        if k2 > k1:\n",
    "#             print(\"Warning (Hill): k2 > k1, AMSE false minimum suspected, resampling...\")\n",
    "            # move left AMSE boundary to avoid numerical issues\n",
    "            min_index1 = min_index1 + int(0.005*n)\n",
    "            min_index2 = min_index2 + int(0.005*n)\n",
    "            k2 = None\n",
    "    \n",
    "    '''\n",
    "    # this constant is provided in the Danielsson's paper\n",
    "    # use instead of rho below if needed\n",
    "    rho = (np.log(k1)/(2.*np.log(n1) - np.log(k1)))\\\n",
    "          **(2.*(np.log(n1) - np.log(k1))/(np.log(n1)))\n",
    "    '''\n",
    "    \n",
    "    # this constant is provided in Qi's paper\n",
    "    rho = (1. - (2*(np.log(k1) - np.log(n1))/(np.log(k1))))**(np.log(k1)/np.log(n1) - 1.)\n",
    "    \n",
    "    k_star = (k1*k1/float(k2)) * rho\n",
    "    k_star = int(np.round(k_star))\n",
    "    \n",
    "    # enforce k_star to pick 2nd value (rare cases of extreme cutoffs)\n",
    "    if k_star == 0:\n",
    "        k_star = 2\n",
    "    if int(k_star) >= len(ordered_data):\n",
    "#         print(\"WARNING: estimated threshold k is larger than the size of data\")\n",
    "        k_star = len(ordered_data)-1\n",
    "    if verbose:\n",
    "        print(\"--- Hill double-bootstrap information ---\")\n",
    "        print(\"Size of the 1st bootstrap sample n1:\", n1)\n",
    "        print(\"Size of the 2nd bootstrap sample n2:\", n2)\n",
    "        print(\"Estimated k1:\", k1)\n",
    "        print(\"Estimated k2:\", k2)\n",
    "        print(\"Estimated constant rho:\", rho)\n",
    "        print(\"Estimated optimal k:\", k_star)\n",
    "        print(\"-----------------------------------------\")\n",
    "    if not diagn_plots:\n",
    "        x1_arr, x2_arr, n1_amse, n2_amse = None, None, None, None\n",
    "    return k_star, x1_arr, n1_amse, k1/float(n1), max_index1, x2_arr, n2_amse, k2/float(n2), max_index2\n",
    "\n",
    "def hill_estimator(ordered_data,\n",
    "                   bootstrap = True, t_bootstrap = 0.5,\n",
    "                   r_bootstrap = 500, verbose = False,\n",
    "                   diagn_plots = False, eps_stop = 0.99):\n",
    "    \"\"\"\n",
    "    Function to calculate Hill estimator for a given dataset.\n",
    "    If bootstrap flag is True, double-bootstrap procedure\n",
    "    for estimation of the optimal number of order statistics is\n",
    "    performed.\n",
    "\n",
    "    Args:\n",
    "        ordered_data: numpy array for which tail index estimation\n",
    "                      is performed. Decreasing ordering is required.\n",
    "        bootstrap:    flag to switch on/off double-bootstrap procedure.\n",
    "        t_bootstrap:  parameter controlling the size of the 2nd\n",
    "                      bootstrap. Defined from n2 = n*(t_bootstrap).\n",
    "        r_bootstrap:  number of bootstrap resamplings for the 1st and 2nd\n",
    "                      bootstraps.\n",
    "        eps_stop:     parameter controlling range of AMSE minimization.\n",
    "                      Defined as the fraction of order statistics to consider\n",
    "                      during the AMSE minimization step.\n",
    "        verbose:      flag controlling bootstrap verbosity. \n",
    "        diagn_plots:  flag to switch on/off generation of AMSE diagnostic\n",
    "                      plots.\n",
    "\n",
    "    Returns:\n",
    "        results: list containing an array of order statistics,\n",
    "                 an array of corresponding tail index estimates,\n",
    "                 the optimal order statistic estimated by double-\n",
    "                 bootstrap and the corresponding tail index,\n",
    "                 an array of fractions of order statistics used for\n",
    "                 the 1st bootstrap sample with an array of corresponding\n",
    "                 AMSE values, value of fraction of order statistics\n",
    "                 corresponding to the minimum of AMSE for the 1st bootstrap\n",
    "                 sample, index of the 1st bootstrap sample's order statistics\n",
    "                 array corresponding to the minimization boundary set\n",
    "                 by eps_stop parameter; and the same characteristics for the\n",
    "                 2nd bootstrap sample.\n",
    "    \"\"\"\n",
    "    k_arr = np.arange(1, len(ordered_data))\n",
    "    xi_arr = get_moments_estimates_1(ordered_data)\n",
    "    if bootstrap:\n",
    "        results = hill_dbs(ordered_data,\n",
    "                           t_bootstrap = t_bootstrap,\n",
    "                           r_bootstrap = r_bootstrap,\n",
    "                           verbose = verbose, \n",
    "                           diagn_plots = diagn_plots,\n",
    "                           eps_stop = eps_stop)\n",
    "        k_star, x1_arr, n1_amse, k1, max_index1, x2_arr, n2_amse, k2, max_index2 = results\n",
    "        while k_star == None:\n",
    "            print(\"Resampling...\")\n",
    "            results = hill_dbs(ordered_data,\n",
    "                           t_bootstrap = t_bootstrap,\n",
    "                           r_bootstrap = r_bootstrap,\n",
    "                           verbose = verbose, \n",
    "                           diagn_plots = diagn_plots,\n",
    "                           eps_stop = eps_stop)\n",
    "            k_star, x1_arr, n1_amse, k1, max_index1, x2_arr, n2_amse, k2, max_index2 = results\n",
    "        xi_star = xi_arr[k_star-1]\n",
    "#         print(\"Adjusted Hill estimated gamma:\", 1 + 1./xi_star)\n",
    "#         print(\"**********\")\n",
    "    else:\n",
    "        k_star, xi_star = None, None\n",
    "        x1_arr, n1_amse, k1, max_index1 = 4*[None]\n",
    "        x2_arr, n2_amse, k2, max_index2 = 4*[None]\n",
    "    results = [k_arr, xi_arr, k_star, xi_star, x1_arr, n1_amse, k1, max_index1,\\\n",
    "               x2_arr, n2_amse, k2, max_index2]\n",
    "    return 1 + 1./xi_star\n",
    "\n",
    "def smooth_hill_estimator(ordered_data, r_smooth = 2):\n",
    "    \"\"\"\n",
    "    Function to calculate smooth Hill estimator for a\n",
    "    given ordered dataset.\n",
    "\n",
    "    Args:\n",
    "        ordered_data: numpy array for which tail index estimation\n",
    "                      is performed. Decreasing ordering is required.\n",
    "        r_smooth:     integer parameter controlling the width\n",
    "                      of smoothing window. Typically small\n",
    "                      value such as 2 or 3.\n",
    "    Returns:\n",
    "        k_arr:  numpy array of order statistics based on the data provided.\n",
    "        xi_arr: numpy array of tail index estimates corresponding to \n",
    "                the order statistics array k_arr.\n",
    "    \"\"\"\n",
    "    n = len(ordered_data)\n",
    "    M1 = get_moments_estimates_1(ordered_data)\n",
    "    xi_arr = np.zeros(int(np.floor(float(n)/r_smooth)))\n",
    "    k_arr = np.arange(1, int(np.floor(float(n)/r_smooth))+1)\n",
    "    xi_arr[0] = M1[0]\n",
    "    bin_lengths = np.array([1.]+[float((r_smooth-1)*k) for k in k_arr[:-1]])\n",
    "    cum_sum = 0.0\n",
    "    for i in range(1, r_smooth*int(np.floor(float(n)/r_smooth))-1):\n",
    "        k = i\n",
    "        cum_sum += M1[k]\n",
    "        if (k+1) % (r_smooth) == 0:\n",
    "            xi_arr[int(k+1)//int(r_smooth)] = cum_sum\n",
    "            cum_sum -= M1[int(k+1)//int(r_smooth)]\n",
    "    xi_arr = xi_arr/bin_lengths\n",
    "    return k_arr, xi_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e9aa5a-6464-4687-abb0-c45969f35f6a",
   "metadata": {},
   "source": [
    "##### Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04270c54-f797-4e02-9d15-6d7d20dfeb29",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================\n",
    "# ========== Moments Tail Index Estimation ==========\n",
    "# ===================================================\n",
    "\n",
    "def moments_dbs_prefactor(xi_n, n1, k1):\n",
    "    \"\"\" \n",
    "    Function to calculate pre-factor used in moments\n",
    "    double-bootstrap procedure.\n",
    "\n",
    "    Args:\n",
    "        xi_n: moments tail index estimate corresponding to\n",
    "              sqrt(n)-th order statistic.\n",
    "        n1:   size of the 1st bootstrap in double-bootstrap\n",
    "              procedure.\n",
    "        k1:   estimated optimal order statistic based on the 1st\n",
    "              bootstrap sample.\n",
    "\n",
    "    Returns:\n",
    "        prefactor: constant used in estimation of the optimal\n",
    "                   stopping order statistic for moments estimator.\n",
    "    \"\"\"\n",
    "    def V_sq(xi_n):\n",
    "        if xi_n >= 0:\n",
    "            V = 1. + (xi_n)**2\n",
    "            return V\n",
    "        else:\n",
    "            a = (1.-xi_n)**2\n",
    "            b = (1-2*xi_n)*(6*((xi_n)**2)-xi_n+1)\n",
    "            c = (1.-3*xi_n)*(1-4*xi_n)\n",
    "            V = a*b/c\n",
    "            return V\n",
    "\n",
    "    def V_bar_sq(xi_n):\n",
    "        if xi_n >= 0:\n",
    "            V = 0.25*(1+(xi_n)**2)\n",
    "            return V\n",
    "        else:\n",
    "            a = 0.25*((1-xi_n)**2)\n",
    "            b = 1-8*xi_n+48*(xi_n**2)-154*(xi_n**3)\n",
    "            c = 263*(xi_n**4)-222*(xi_n**5)+72*(xi_n**6)\n",
    "            d = (1.-2*xi_n)*(1-3*xi_n)*(1-4*xi_n)\n",
    "            e = (1.-5*xi_n)*(1-6*xi_n)\n",
    "            V = a*(b+c)/(d*e)\n",
    "            return V\n",
    "    \n",
    "    def b(xi_n, rho):\n",
    "        if xi_n < rho:\n",
    "            a1 = (1.-xi_n)*(1-2*xi_n)\n",
    "            a2 = (1.-rho-xi_n)*(1.-rho-2*xi_n)\n",
    "            return a1/a2\n",
    "        elif xi_n >= rho and xi_n < 0:\n",
    "            return 1./(1-xi_n)\n",
    "        else:\n",
    "            b = (xi_n/(rho*(1.-rho))) + (1./((1-rho)**2))\n",
    "            return b\n",
    "\n",
    "    def b_bar(xi_n, rho):\n",
    "        if xi_n < rho:\n",
    "            a1 = 0.5*(-rho*(1-xi_n)**2)\n",
    "            a2 = (1.-xi_n-rho)*(1-2*xi_n-rho)*(1-3*xi_n-rho)\n",
    "            return a1/a2\n",
    "        elif xi_n >= rho and xi_n < 0:\n",
    "            a1 = 1-2*xi_n-np.sqrt((1-xi_n)*(1-2.*xi_n))\n",
    "            a2 = (1.-xi_n)*(1-2*xi_n)\n",
    "            return a1/a2\n",
    "        else:\n",
    "            b = (-1.)*((rho + xi_n*(1-rho))/(2*(1-rho)**3))\n",
    "            return b\n",
    "\n",
    "    rho = np.log(k1)/(2*np.log(k1) - 2.*np.log(n1))\n",
    "    a = (V_sq(xi_n)) * (b_bar(xi_n, rho)**2)\n",
    "    b = V_bar_sq(xi_n) * (b(xi_n, rho)**2)\n",
    "    prefactor = (a/b)**(1./(1. - 2*rho))\n",
    "    return prefactor\n",
    "\n",
    "def moments_dbs(ordered_data, xi_n, t_bootstrap = 0.5,\n",
    "                r_bootstrap = 500, eps_stop = 1.0,\n",
    "                verbose = False, diagn_plots = False):\n",
    "    \"\"\"\n",
    "    Function to perform double-bootstrap procedure for \n",
    "    moments estimator.\n",
    "\n",
    "    Args:\n",
    "        ordered_data: numpy array for which double-bootstrap\n",
    "                      is performed. Decreasing ordering is required.\n",
    "        xi_n:         moments tail index estimate corresponding to\n",
    "                      sqrt(n)-th order statistic.\n",
    "        t_bootstrap:  parameter controlling the size of the 2nd\n",
    "                      bootstrap. Defined from n2 = n*(t_bootstrap).\n",
    "        r_bootstrap:  number of bootstrap resamplings for the 1st and 2nd\n",
    "                      bootstraps.\n",
    "        eps_stop:     parameter controlling range of AMSE minimization.\n",
    "                      Defined as the fraction of order statistics to consider\n",
    "                      during the AMSE minimization step.\n",
    "        verbose:      flag controlling bootstrap verbosity. \n",
    "        diagn_plots:  flag to switch on/off generation of AMSE diagnostic\n",
    "                      plots.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        k_star:     number of order statistics optimal for estimation\n",
    "                    according to the double-bootstrap procedure.\n",
    "        x1_arr:     array of fractions of order statistics used for the\n",
    "                    1st bootstrap sample.\n",
    "        n1_amse:    array of AMSE values produced by the 1st bootstrap\n",
    "                    sample.\n",
    "        k1_min:     value of fraction of order statistics corresponding\n",
    "                    to the minimum of AMSE for the 1st bootstrap sample.\n",
    "        max_index1: index of the 1st bootstrap sample's order statistics\n",
    "                    array corresponding to the minimization boundary set\n",
    "                    by eps_stop parameter.\n",
    "        x2_arr:     array of fractions of order statistics used for the\n",
    "                    2nd bootstrap sample.\n",
    "        n2_amse:    array of AMSE values produced by the 2nd bootstrap\n",
    "                    sample.\n",
    "        k2_min:     value of fraction of order statistics corresponding\n",
    "                    to the minimum of AMSE for the 2nd bootstrap sample.\n",
    "        max_index2: index of the 2nd bootstrap sample's order statistics\n",
    "                    array corresponding to the minimization boundary set\n",
    "                    by eps_stop parameter.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Performing moments double-bootstrap...\")\n",
    "    n = len(ordered_data)\n",
    "    eps_bootstrap = 0.5*(1+np.log(int(t_bootstrap*n))/np.log(n))\n",
    "\n",
    "    # first bootstrap with n1 sample size\n",
    "    n1 = int(n**eps_bootstrap)\n",
    "    samples_n1 = np.zeros(n1-1)\n",
    "    good_counts1 = np.zeros(n1-1)\n",
    "    for i in range(r_bootstrap):\n",
    "        sample = np.random.choice(ordered_data, n1, replace = True)\n",
    "        sample[::-1].sort()\n",
    "        M1, M2, M3 = get_moments_estimates_3(sample)\n",
    "        xi_2 = M1 + 1. - 0.5*((1. - (M1*M1)/M2))**(-1.)\n",
    "        xi_3 = np.sqrt(0.5*M2) + 1. - (2./3.)*(1. / (1. - M1*M2/M3))\n",
    "        samples_n1 += (xi_2 - xi_3)**2\n",
    "        good_counts1[np.where((xi_2 - xi_3)**2 != np.nan)] += 1\n",
    "    max_index1 = (np.abs(np.linspace(1./n1, 1.0, n1) - eps_stop)).argmin()\n",
    "    averaged_delta = samples_n1 / good_counts1\n",
    "    k1 = np.nanargmin(averaged_delta[:max_index1]) + 1 #take care of indexing\n",
    "    if diagn_plots:\n",
    "        n1_amse = averaged_delta\n",
    "        x1_arr = np.linspace(1./n1, 1.0, n1)\n",
    "    \n",
    "\n",
    "    #r second bootstrap with n2 sample size\n",
    "    n2 = int(n1*n1/float(n))\n",
    "    samples_n2 = np.zeros(n2-1)\n",
    "    good_counts2 = np.zeros(n2-1)\n",
    "    for i in range(r_bootstrap):\n",
    "        sample = np.random.choice(ordered_data, n2, replace = True)\n",
    "        sample[::-1].sort()\n",
    "        M1, M2, M3 = get_moments_estimates_3(sample)\n",
    "        xi_2 = M1 + 1. - 0.5*(1. - (M1*M1)/M2)**(-1.)\n",
    "        xi_3 = np.sqrt(0.5*M2) + 1. - (2./3.)*(1. / (1. - M1*M2/M3))\n",
    "        samples_n2 += (xi_2 - xi_3)**2\n",
    "        good_counts2[np.where((xi_2 - xi_3)**2 != np.nan)] += 1\n",
    "    max_index2 = (np.abs(np.linspace(1./n2, 1.0, n2) - eps_stop)).argmin()\n",
    "    averaged_delta = samples_n2 / good_counts2\n",
    "    k2 = np.nanargmin(averaged_delta[:max_index2]) + 1 #take care of indexing\n",
    "    if diagn_plots:\n",
    "        n2_amse = averaged_delta\n",
    "        x2_arr = np.linspace(1./n2, 1.0, n2)\n",
    "    \n",
    "    if k2 > k1:\n",
    "        print(\"WARNING(moments): estimated k2 is greater than k1! Re-doing bootstrap...\") \n",
    "        return 9*[None]\n",
    "    \n",
    "    #calculate estimated optimal stopping k\n",
    "    prefactor = moments_dbs_prefactor(xi_n, n1, k1)\n",
    "    k_star = int((k1*k1/float(k2)) * prefactor)\n",
    "\n",
    "    if int(k_star) >= len(ordered_data):\n",
    "        print(\"WARNING: estimated threshold k is larger than the size of data\")\n",
    "        k_star = len(ordered_data)-1\n",
    "    if verbose:\n",
    "        print(\"--- Moments double-bootstrap information ---\")\n",
    "        print(\"Size of the 1st bootstrap sample n1:\", n1)\n",
    "        print(\"Size of the 2nd bootstrap sample n2:\", n2)\n",
    "        print(\"Estimated k1:\", k1)\n",
    "        print(\"Estimated k2:\", k2)\n",
    "        print(\"Estimated constant:\", prefactor)\n",
    "        print(\"Estimated optimal k:\", k_star)\n",
    "        print(\"--------------------------------------------\")\n",
    "    if not diagn_plots:\n",
    "        x1_arr, x2_arr, n1_amse, n2_amse = None, None, None, None\n",
    "    return k_star, x1_arr, n1_amse, k1/float(n1), max_index1, x2_arr, n2_amse, k2/float(n2), max_index2\n",
    "\n",
    "def moments_estimator(ordered_data,\n",
    "                      bootstrap = True, t_bootstrap = 0.5,\n",
    "                      r_bootstrap = 500, verbose = False,\n",
    "                      diagn_plots = False, eps_stop = 0.99):\n",
    "    \"\"\"\n",
    "    Function to calculate moments estimator for a given dataset.\n",
    "    If bootstrap flag is True, double-bootstrap procedure\n",
    "    for estimation of the optimal number of order statistics is\n",
    "    performed.\n",
    "\n",
    "    Args:\n",
    "        ordered_data: numpy array for which tail index estimation\n",
    "                      is performed. Decreasing ordering is required.\n",
    "        bootstrap:    flag to switch on/off double-bootstrap procedure.\n",
    "        t_bootstrap:  parameter controlling the size of the 2nd\n",
    "                      bootstrap. Defined from n2 = n*(t_bootstrap).\n",
    "        r_bootstrap:  number of bootstrap resamplings for the 1st and 2nd\n",
    "                      bootstraps.\n",
    "        eps_stop:     parameter controlling range of AMSE minimization.\n",
    "                      Defined as the fraction of order statistics to consider\n",
    "                      during the AMSE minimization step.\n",
    "        verbose:      flag controlling bootstrap verbosity. \n",
    "        diagn_plots:  flag to switch on/off generation of AMSE diagnostic\n",
    "                      plots.\n",
    "\n",
    "    Returns:\n",
    "        results: list containing an array of order statistics,\n",
    "                 an array of corresponding tail index estimates,\n",
    "                 the optimal order statistic estimated by double-\n",
    "                 bootstrap and the corresponding tail index,\n",
    "                 an array of fractions of order statistics used for\n",
    "                 the 1st bootstrap sample with an array of corresponding\n",
    "                 AMSE values, value of fraction of order statistics\n",
    "                 corresponding to the minimum of AMSE for the 1st bootstrap\n",
    "                 sample, index of the 1st bootstrap sample's order statistics\n",
    "                 array corresponding to the minimization boundary set\n",
    "                 by eps_stop parameter; and the same characteristics for the\n",
    "                 2nd bootstrap sample.\n",
    "    \"\"\"\n",
    "    n =  len(ordered_data)\n",
    "    M1, M2 = get_moments_estimates_2(ordered_data)\n",
    "    xi_arr = M1 + 1. - 0.5*(1. - (M1*M1)/M2)**(-1)\n",
    "    k_arr = np.arange(1, len(ordered_data))\n",
    "    if bootstrap:\n",
    "        xi_n = xi_arr[int(np.floor(n**0.5))-1]\n",
    "        results = moments_dbs(ordered_data, xi_n,\n",
    "                              t_bootstrap = t_bootstrap,\n",
    "                              r_bootstrap = r_bootstrap,\n",
    "                              verbose = verbose, \n",
    "                              diagn_plots = diagn_plots,\n",
    "                              eps_stop = eps_stop)\n",
    "        while results[0] == None:\n",
    "            print(\"Resampling...\")\n",
    "            results = moments_dbs(ordered_data, xi_n,\n",
    "                                  t_bootstrap = t_bootstrap,\n",
    "                                  r_bootstrap = r_bootstrap,\n",
    "                                  verbose = verbose, \n",
    "                                  diagn_plots = diagn_plots,\n",
    "                                  eps_stop = eps_stop)\n",
    "        k_star, x1_arr, n1_amse, k1, max_index1, x2_arr, n2_amse, k2, max_index2 = results\n",
    "        xi_star = xi_arr[k_star-1]\n",
    "#         if xi_star <= 0:\n",
    "#             print (\"Moments estimated gamma: infinity (xi <= 0).\")\n",
    "#         else:\n",
    "#             print (\"Moments estimated gamma:\", 1 + 1./xi_star)\n",
    "#         print(\"**********\")\n",
    "    else:\n",
    "        k_star, xi_star = None, None\n",
    "        x1_arr, n1_amse, k1, max_index1 = 4*[None]\n",
    "        x2_arr, n2_amse, k2, max_index2 = 4*[None]\n",
    "    results = [k_arr, xi_arr, k_star, xi_star, x1_arr, n1_amse, k1, max_index1,\\\n",
    "              x2_arr, n2_amse, k2, max_index2]\n",
    "    return 1 + 1./xi_star\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970ef4db-c343-4618-8396-bdef14df4ea1",
   "metadata": {},
   "source": [
    "##### Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63afd49a-3a28-4c3e-92ed-a4b55c73ed80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# =======================================================\n",
    "# ========== Kernel-type Tail Index Estimation ==========\n",
    "# =======================================================\n",
    "\n",
    "def get_biweight_kernel_estimates(ordered_data, hsteps, alpha):\n",
    "    \"\"\"\n",
    "    Function to calculate biweight kernel-type estimates for tail index.\n",
    "    Biweight kernel is defined as:\n",
    "    phi(u) = (15/8) * (1 - u^2)^2\n",
    "\n",
    "    Args:\n",
    "        ordered_data: numpy array for which tail index estimation\n",
    "                      is performed. Decreasing ordering is required.\n",
    "        hsteps:       parameter controlling number of bandwidth steps\n",
    "                      of the kernel-type estimator.\n",
    "        alpha:        parameter controlling the amount of \"smoothing\"\n",
    "                      for the kernel-type estimator. Should be greater\n",
    "                      than 0.5.\n",
    "\n",
    "    Returns:\n",
    "        h_arr:  numpy array of fractions of order statistics included\n",
    "                in kernel-type tail index estimation.\n",
    "        xi_arr: numpy array with tail index estimated corresponding\n",
    "                to different fractions of order statistics included\n",
    "                listed in h_arr array.\n",
    "    \"\"\"\n",
    "    n = len(ordered_data)\n",
    "    logs = np.log(ordered_data)\n",
    "    differences = logs[:-1] - logs[1:]\n",
    "    \n",
    "    i_arr = np.arange(1, n)/float(n)\n",
    "    \n",
    "    i3_arr = i_arr**3\n",
    "    i5_arr = i_arr**5\n",
    "    i_alpha_arr = i_arr**alpha\n",
    "    i_alpha2_arr = i_arr**(2.+alpha)\n",
    "    i_alpha4_arr = i_arr**(4.+alpha)\n",
    "    t1 = np.cumsum(i_arr*differences)\n",
    "    t2 = np.cumsum(i3_arr*differences)\n",
    "    t3 = np.cumsum(i5_arr*differences)\n",
    "    t4 = np.cumsum(i_alpha_arr*differences)\n",
    "    t5 = np.cumsum(i_alpha2_arr*differences)\n",
    "    t6 = np.cumsum(i_alpha4_arr*differences)\n",
    "    h_arr = np.logspace(np.log10(1./n), np.log10(1.0), hsteps)\n",
    "    max_i_vector = (np.floor((n*h_arr))-2.).astype(int)\n",
    "    gamma_pos = (15./(8*h_arr))*t1[max_i_vector]\\\n",
    "                - (15./(4*(h_arr**3)))*t2[max_i_vector]\\\n",
    "                + (15./(8*(h_arr**5)))*t3[max_i_vector]\n",
    "\n",
    "    q1 = (15./(8*h_arr))*t4[max_i_vector]\\\n",
    "         + (15./(8*(h_arr**5)))*t6[max_i_vector]\\\n",
    "         - (15./(4*(h_arr**3)))*t5[max_i_vector]\n",
    "\n",
    "    q2 = (15.*(1+alpha)/(8*h_arr))*t4[max_i_vector]\\\n",
    "         + (15.*(5+alpha)/(8*(h_arr**5)))*t6[max_i_vector]\\\n",
    "         - (15.*(3+alpha)/(4*(h_arr**3)))*t5[max_i_vector]\n",
    "\n",
    "    xi_arr = gamma_pos -1. + q2/q1\n",
    "    return h_arr, xi_arr\n",
    "\n",
    "\n",
    "def get_triweight_kernel_estimates(ordered_data, hsteps, alpha):\n",
    "    \"\"\"\n",
    "    Function to calculate triweight kernel-type estimates for tail index.\n",
    "    Triweight kernel is defined as:\n",
    "    phi(u) = (35/16) * (1 - u^2)^3\n",
    "\n",
    "    Args:\n",
    "        ordered_data: numpy array for which tail index estimation\n",
    "                      is performed. Decreasing ordering is required.\n",
    "        hsteps:       parameter controlling number of bandwidth steps\n",
    "                      of the kernel-type estimator.\n",
    "        alpha:        parameter controlling the amount of \"smoothing\"\n",
    "                      for the kernel-type estimator. Should be greater\n",
    "                      than 0.5.\n",
    "\n",
    "    Returns:\n",
    "        h_arr:  numpy array of fractions of order statistics included\n",
    "                in kernel-type tail index estimation.\n",
    "        xi_arr: numpy array with tail index estimated corresponding\n",
    "                to different fractions of order statistics included\n",
    "                listed in h_arr array.\n",
    "    \"\"\"\n",
    "    n = len(ordered_data)\n",
    "    logs = np.log(ordered_data)\n",
    "    differences = logs[:-1] - logs[1:]\n",
    "    i_arr = np.arange(1, n)/float(n)\n",
    "    i3_arr = i_arr**3\n",
    "    i5_arr = i_arr**5\n",
    "    i7_arr = i_arr**7\n",
    "    i_alpha_arr = i_arr**alpha\n",
    "    i_alpha2_arr = i_arr**(2.+alpha)\n",
    "    i_alpha4_arr = i_arr**(4.+alpha)\n",
    "    i_alpha6_arr = i_arr**(6.+alpha)\n",
    "    t1 = np.cumsum(i_arr*differences)\n",
    "    t2 = np.cumsum(i3_arr*differences)\n",
    "    t3 = np.cumsum(i5_arr*differences)\n",
    "    t4 = np.cumsum(i7_arr*differences)\n",
    "    t5 = np.cumsum(i_alpha_arr*differences)\n",
    "    t6 = np.cumsum(i_alpha2_arr*differences)\n",
    "    t7 = np.cumsum(i_alpha4_arr*differences)\n",
    "    t8 = np.cumsum(i_alpha6_arr*differences)\n",
    "    h_arr = np.logspace(np.log10(1./n), np.log10(1.0), hsteps)\n",
    "    max_i_vector = (np.floor((n*h_arr))-2.).astype(int)\n",
    "\n",
    "    gamma_pos = (35./(16*h_arr))*t1[max_i_vector]\\\n",
    "                - (105./(16*(h_arr**3)))*t2[max_i_vector]\\\n",
    "                + (105./(16*(h_arr**5)))*t3[max_i_vector]\\\n",
    "                - (35./(16*(h_arr**7)))*t4[max_i_vector]\n",
    "\n",
    "    q1 = (35./(16*h_arr))*t5[max_i_vector]\\\n",
    "         + (105./(16*(h_arr**5)))*t7[max_i_vector]\\\n",
    "         - (105./(16*(h_arr**3)))*t6[max_i_vector]\\\n",
    "         - (35./(16*(h_arr**7)))*t8[max_i_vector]\n",
    "\n",
    "    q2 = (35.*(1+alpha)/(16*h_arr))*t5[max_i_vector] \\\n",
    "        + (105.*(5+alpha)/(16*(h_arr**5)))*t7[max_i_vector] \\\n",
    "        - (105.*(3+alpha)/(16*(h_arr**3)))*t6[max_i_vector] \\\n",
    "        - (35.*(7+alpha)/(16*(h_arr**7)))*t8[max_i_vector]\n",
    "\n",
    "    xi_arr = gamma_pos - 1. + q2/q1\n",
    "    return h_arr, xi_arr\n",
    "\n",
    "def kernel_type_dbs(ordered_data, hsteps, t_bootstrap = 0.5,\n",
    "                    r_bootstrap = 500, alpha = 0.6, eps_stop = 1.0,\n",
    "                    verbose = False, diagn_plots = False):\n",
    "    \"\"\"\n",
    "    Function to perform double-bootstrap procedure for \n",
    "    moments estimator.\n",
    "\n",
    "    Args:\n",
    "        ordered_data: numpy array for which double-bootstrap\n",
    "                      is performed. Decreasing ordering is required.\n",
    "        hsteps:       parameter controlling number of bandwidth steps\n",
    "                      of the kernel-type estimator.\n",
    "        t_bootstrap:  parameter controlling the size of the 2nd\n",
    "                      bootstrap. Defined from n2 = n*(t_bootstrap).\n",
    "        r_bootstrap:  number of bootstrap resamplings for the 1st and 2nd\n",
    "                      bootstraps.\n",
    "        alpha:        parameter controlling the amount of \"smoothing\"\n",
    "                      for the kernel-type estimator. Should be greater\n",
    "                      than 0.5.\n",
    "        eps_stop:     parameter controlling range of AMSE minimization.\n",
    "                      Defined as the fraction of order statistics to consider\n",
    "                      during the AMSE minimization step.\n",
    "        verbose:      flag controlling bootstrap verbosity. \n",
    "        diagn_plots:  flag to switch on/off generation of AMSE diagnostic\n",
    "                      plots.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        h_star:       fraction of order statistics optimal for estimation\n",
    "                      according to the double-bootstrap procedure.\n",
    "        x1_arr:       array of fractions of order statistics used for the\n",
    "                      1st bootstrap sample.\n",
    "        n1_amse:      array of AMSE values produced by the 1st bootstrap\n",
    "                      sample.\n",
    "        h1:           value of fraction of order statistics corresponding\n",
    "                      to the minimum of AMSE for the 1st bootstrap sample.\n",
    "        max_k_index1: index of the 1st bootstrap sample's order statistics\n",
    "                      array corresponding to the minimization boundary set\n",
    "                      by eps_stop parameter.\n",
    "        x2_arr:       array of fractions of order statistics used for the\n",
    "                      2nd bootstrap sample.\n",
    "        n2_amse:      array of AMSE values produced by the 2nd bootstrap\n",
    "                      sample.\n",
    "        h2:           value of fraction of order statistics corresponding\n",
    "                      to the minimum of AMSE for the 2nd bootstrap sample.\n",
    "        max_k_index2: index of the 2nd bootstrap sample's order statistics\n",
    "                      array corresponding to the minimization boundary set\n",
    "                      by eps_stop parameter.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Performing kernel double-bootstrap...\")\n",
    "    n = len(ordered_data)\n",
    "    eps_bootstrap = 0.5*(1+np.log(int(t_bootstrap*n))/np.log(n))\n",
    "    \n",
    "    # first bootstrap with n1 sample size\n",
    "    n1 = int(n**eps_bootstrap)\n",
    "    samples_n1 = np.zeros(hsteps)\n",
    "    good_counts1 = np.zeros(hsteps)\n",
    "    for i in range(r_bootstrap):\n",
    "        sample = np.random.choice(ordered_data, n1, replace = True)\n",
    "        sample[::-1].sort()\n",
    "        _, xi2_arr = get_biweight_kernel_estimates(sample, hsteps, alpha)\n",
    "        _, xi3_arr = get_triweight_kernel_estimates(sample, hsteps, alpha)\n",
    "        samples_n1 += (xi2_arr - xi3_arr)**2\n",
    "        good_counts1[np.where((xi2_arr - xi3_arr)**2 != np.nan)] += 1\n",
    "    max_index1 = (np.abs(np.logspace(np.log10(1./n1), np.log10(1.0), hsteps) - eps_stop)).argmin()\n",
    "    x1_arr = np.logspace(np.log10(1./n1), np.log10(1.0), hsteps)\n",
    "    averaged_delta = samples_n1 / good_counts1\n",
    "    h1 = x1_arr[np.nanargmin(averaged_delta[:max_index1])] \n",
    "    if diagn_plots:\n",
    "        n1_amse = averaged_delta\n",
    "        \n",
    "    \n",
    "    # second bootstrap with n2 sample size\n",
    "    n2 = int(n1*n1/float(n))\n",
    "    if n2 < hsteps:\n",
    "        sys.exit(\"Number of h points is larger than number \"+\\\n",
    "                 \"of order statistics! Please either increase \"+\\\n",
    "                 \"the size of 2nd bootstrap or decrease number \"+\\\n",
    "                 \"of h grid points.\")\n",
    "    samples_n2 = np.zeros(hsteps)\n",
    "    good_counts2 = np.zeros(hsteps)\n",
    "    for i in range(r_bootstrap):\n",
    "        sample = np.random.choice(ordered_data, n2, replace = True)\n",
    "        sample[::-1].sort()\n",
    "        _, xi2_arr = get_biweight_kernel_estimates(sample, hsteps, alpha)\n",
    "        _, xi3_arr = get_triweight_kernel_estimates(sample, hsteps, alpha)\n",
    "        samples_n2 += (xi2_arr - xi3_arr)**2\n",
    "        good_counts2[np.where((xi2_arr - xi3_arr)**2 != np.nan)] += 1\n",
    "    max_index2 = (np.abs(np.logspace(np.log10(1./n2), np.log10(1.0), hsteps) - eps_stop)).argmin()\n",
    "    x2_arr = np.logspace(np.log10(1./n2), np.log10(1.0), hsteps)\n",
    "    averaged_delta = samples_n2 / good_counts2\n",
    "    h2 = x2_arr[np.nanargmin(averaged_delta[:max_index2])]\n",
    "    if diagn_plots:\n",
    "        n2_amse = averaged_delta\n",
    "    \n",
    "    A = (143.*((np.log(n1) + np.log(h1))**2)/(3*(np.log(n1) - 13. * np.log(h1))**2))\\\n",
    "        **(-np.log(h1)/np.log(n1))\n",
    "    \n",
    "    h_star = (h1*h1/float(h2)) * A\n",
    "\n",
    "    if h_star > 1:\n",
    "        print(\"WARNING: estimated threshold is larger than the size of data!\")\n",
    "        print(\"WARNING: optimal h is set to 1...\")\n",
    "        h_star = 1.\n",
    "\n",
    "    if verbose:\n",
    "        print(\"--- Kernel-type double-bootstrap information ---\")\n",
    "        print(\"Size of the 1st bootstrap sample n1:\", n1)\n",
    "        print(\"Size of the 2nd bootstrap sample n2:\", n2)\n",
    "        print(\"Estimated h1:\", h1)\n",
    "        print(\"Estimated h2:\", h2)\n",
    "        print(\"Estimated constant A:\", A)\n",
    "        print(\"Estimated optimal h:\", h_star)\n",
    "        print(\"------------------------------------------------\")\n",
    "    if not diagn_plots:\n",
    "        x1_arr, x2_arr, n1_amse, n2_amse = None, None, None, None\n",
    "    if x1_arr is not None:\n",
    "        max_k_index1 = x1_arr[max_index1]\n",
    "    else:\n",
    "        max_k_index1 = None\n",
    "    if x2_arr is not None:\n",
    "        max_k_index2 = x2_arr[max_index2]\n",
    "    else:\n",
    "        max_k_index2 = None\n",
    "    return h_star, x1_arr, n1_amse, h1, max_k_index1, x2_arr, n2_amse, h2, max_k_index2\n",
    "\n",
    "def kernel_type_estimator(ordered_data, hsteps, alpha = 0.6,\n",
    "                         bootstrap = True, t_bootstrap = 0.5,\n",
    "                         r_bootstrap = 500, verbose = False,\n",
    "                         diagn_plots = False, eps_stop = 0.99):\n",
    "    \"\"\"\n",
    "    Function to calculate kernel-type estimator for a given dataset.\n",
    "    If bootstrap flag is True, double-bootstrap procedure\n",
    "    for estimation of the optimal number of order statistics is\n",
    "    performed.\n",
    "\n",
    "    Args:\n",
    "        ordered_data: numpy array for which tail index estimation\n",
    "                      is performed. Decreasing ordering is required.\n",
    "        hsteps:       parameter controlling number of bandwidth steps\n",
    "                      of the kernel-type estimator.\n",
    "        alpha:        parameter controlling the amount of \"smoothing\"\n",
    "                      for the kernel-type estimator. Should be greater\n",
    "                      than 0.5.\n",
    "        bootstrap:    flag to switch on/off double-bootstrap procedure.\n",
    "        t_bootstrap:  parameter controlling the size of the 2nd\n",
    "                      bootstrap. Defined from n2 = n*(t_bootstrap).\n",
    "        r_bootstrap:  number of bootstrap resamplings for the 1st and 2nd\n",
    "                      bootstraps.\n",
    "        eps_stop:     parameter controlling range of AMSE minimization.\n",
    "                      Defined as the fraction of order statistics to consider\n",
    "                      during the AMSE minimization step.\n",
    "        verbose:      flag controlling bootstrap verbosity. \n",
    "        diagn_plots:  flag to switch on/off generation of AMSE diagnostic\n",
    "                      plots.\n",
    "\n",
    "    Returns:\n",
    "        results: list containing an array of fractions of order statistics,\n",
    "                 an array of corresponding tail index estimates,\n",
    "                 the optimal order statistic estimated by double-\n",
    "                 bootstrap and the corresponding tail index,\n",
    "                 an array of fractions of order statistics used for\n",
    "                 the 1st bootstrap sample with an array of corresponding\n",
    "                 AMSE values, value of fraction of order statistics\n",
    "                 corresponding to the minimum of AMSE for the 1st bootstrap\n",
    "                 sample, index of the 1st bootstrap sample's order statistics\n",
    "                 array corresponding to the minimization boundary set\n",
    "                 by eps_stop parameter; and the same characteristics for the\n",
    "                 2nd bootstrap sample.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(ordered_data)\n",
    "    h_arr, xi_arr = get_biweight_kernel_estimates(ordered_data, hsteps,\n",
    "                                                  alpha = alpha)\n",
    "    if bootstrap:\n",
    "        results = kernel_type_dbs(ordered_data, hsteps,\n",
    "                                  t_bootstrap = t_bootstrap,\n",
    "                                  alpha = alpha, r_bootstrap = r_bootstrap,\n",
    "                                  verbose = verbose, diagn_plots = diagn_plots,\n",
    "                                  eps_stop = eps_stop)\n",
    "        h_star, x1_arr, n1_amse, h1, max_index1, x2_arr, n2_amse, h2, max_index2 = results\n",
    "        while h_star == None:\n",
    "            print(\"Resampling...\")\n",
    "            results = kernel_type_dbs(ordered_data, hsteps,\n",
    "                                  t_bootstrap = t_bootstrap,\n",
    "                                  alpha = alpha, r_bootstrap = r_bootstrap,\n",
    "                                  verbose = verbose, diagn_plots = diagn_plots,\n",
    "                                  eps_stop = eps_stop)\n",
    "            h_star, x1_arr, n1_amse, h1, max_index1, x2_arr, n2_amse, h2, max_index2 = results\n",
    "        \n",
    "        #get k index which corresponds to h_star\n",
    "        k_star = np.argmin(np.abs(h_arr - h_star))\n",
    "        xi_star = xi_arr[k_star]\n",
    "        k_arr = []\n",
    "        k_star = int(np.floor(h_arr[k_star]*n))-1\n",
    "        k_arr = np.floor((h_arr * n))\n",
    "#         if xi_star <= 0:\n",
    "#             print (\"Kernel-type estimated gamma: infinity (xi <= 0).\")\n",
    "#         else:\n",
    "#             print (\"Kernel-type estimated gamma:\", 1 + 1./xi_star)\n",
    "#         print(\"**********\")\n",
    "    else:\n",
    "        k_star, xi_star = None, None\n",
    "        x1_arr, n1_amse, h1, max_index1 = 4*[None]\n",
    "        x2_arr, n2_amse, h2, max_index2 = 4*[None]\n",
    "        k_arr = np.floor(h_arr * n)\n",
    "    results = [np.array(k_arr), xi_arr, k_star, xi_star, x1_arr, n1_amse, h1, max_index1,\\\n",
    "              x2_arr, n2_amse, h2, max_index2]\n",
    "    return 1 + 1./xi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad9a42-4865-467a-8eac-f928a721b6f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==================================================\n",
    "# ========== Plotting and Data Processing ==========\n",
    "# ==================================================\n",
    "\n",
    "def make_plots(ordered_data, output_file_path, number_of_bins,\n",
    "               r_smooth, alpha, hsteps, bootstrap_flag, t_bootstrap,\n",
    "               r_bootstrap, diagn_plots, eps_stop, theta1, theta2, \n",
    "               verbose, noise_flag, p_noise, savedata):\n",
    "    \"\"\" \n",
    "    Function to create plots and save tail index estimation data.\n",
    "\n",
    "    Args:\n",
    "        ordered_data:     numpy array for which tail index estimation\n",
    "                          is performed. Decreasing ordering is required.\n",
    "        output_file_path: file path to which plots should be saved.\n",
    "        number_of_bins:   number of log-bins for degree distribution.\n",
    "        r_smooth:         integer parameter controlling the width\n",
    "                          of smoothing window. Typically small\n",
    "                          value such as 2 or 3.\n",
    "        alpha:            parameter controlling the amount of \"smoothing\"\n",
    "                          for the kernel-type estimator. Should be greater\n",
    "                          than 0.5.\n",
    "        hsteps:           parameter controlling number of bandwidth steps\n",
    "                          of the kernel-type estimator.\n",
    "        bootstrap_flag:   flag to switch on/off double-bootstrap procedure.\n",
    "        t_bootstrap:      parameter controlling the size of the 2nd\n",
    "                          bootstrap. Defined from n2 = n*(t_bootstrap).\n",
    "        r_bootstrap:      number of bootstrap resamplings for the 1st and 2nd\n",
    "                          bootstraps.\n",
    "        diagn_plots:      flag to switch on/off generation of AMSE diagnostic\n",
    "                          plots.\n",
    "        eps_stop:         parameter controlling range of AMSE minimization.\n",
    "                          Defined as the fraction of order statistics to\n",
    "                          consider during the AMSE minimization step.\n",
    "        theta1:           Lower bound of plotting range, defined as\n",
    "                          k_min = ceil(n^theta1).\n",
    "                          Overwritten if plots behave badly within the range.\n",
    "        theta2:           Upper bound of plotting range, defined as\n",
    "                          k_max = floor(n^theta2).\n",
    "                          Overwritten if plots behave badly within the range.\n",
    "        verbose:          flag controlling bootstrap verbosity.\n",
    "        noise_flag:       Switch on/off uniform noise in range\n",
    "                          [-5*10^(-p), 5*10^(-p)] that is added to each\n",
    "                          data point. Used for integer-valued sequences\n",
    "                          with p = 1 (default = 1).\n",
    "        p_noise:          integer parameter controlling noise amplitude.\n",
    "        savedata:         Flag to save data files in the directory with plots.\n",
    "    \"\"\"\n",
    "    output_dir = os.path.dirname(os.path.realpath(output_file_path))\n",
    "    output_name = os.path.splitext(os.path.basename(output_file_path))[0]\n",
    "    # calculate log-binned PDF\n",
    "    if verbose:\n",
    "        print(\"Calculating PDF...\")\n",
    "    t1 =time.time()\n",
    "    x_pdf, y_pdf = get_distribution(ordered_data,\n",
    "                                    number_of_bins = number_of_bins)\n",
    "    t2 =time.time()\n",
    "    if verbose:\n",
    "        print(\"Elapsed time(PDF):\", t2-t1)\n",
    "    if savedata == 1:\n",
    "        with open(os.path.join(output_dir+\"/\"+output_name+\"_pdf.dat\"), \"w\") as f:\n",
    "            for i in range(len(x_pdf)):\n",
    "                f.write(str(x_pdf[i]) + \" \" + str(y_pdf[i]) + \"\\n\")\n",
    "\n",
    "    # calculate CCDF\n",
    "    if verbose:\n",
    "        print(\"Calculating CCDF...\")\n",
    "    t1 = time.time()\n",
    "    x_ccdf, y_ccdf = get_ccdf(ordered_data)\n",
    "    t2 = time.time()\n",
    "    if verbose:\n",
    "        print(\"Elapsed time:\", t2-t1)\n",
    "    if savedata == 1:\n",
    "        with open(os.path.join(output_dir+\"/\"+output_name+\"_ccdf.dat\"), \"w\") as f:\n",
    "            for i in range(len(x_ccdf)):\n",
    "                f.write(str(x_ccdf[i]) + \" \" + str(y_ccdf[i]) + \"\\n\")\n",
    "\n",
    "    # add noise if needed\n",
    "    if noise_flag:\n",
    "        original_discrete_data = ordered_data\n",
    "        discrete_ordered_data = ordered_data\n",
    "        discrete_ordered_data[::-1].sort()\n",
    "        ordered_data = add_uniform_noise(ordered_data, p = p_noise)\n",
    "    ordered_data[::-1].sort()\n",
    "    \n",
    "    # perform Pickands estimation\n",
    "    if verbose:\n",
    "        print(\"Calculating Pickands...\")\n",
    "    t1=time.time()\n",
    "    k_p_arr, xi_p_arr = pickands_estimator(ordered_data)\n",
    "    t2 =time.time()\n",
    "    if verbose:\n",
    "        print(\"Elapsed time (Pickands):\", t2-t1)\n",
    "    if savedata == 1:\n",
    "        with open(os.path.join(output_dir+\"/\"+output_name+\"_pickands.dat\"), \"w\") as f:\n",
    "            for i in range(len(k_p_arr)):\n",
    "                f.write(str(k_p_arr[i]) + \" \" + str(xi_p_arr[i]) + \"\\n\")\n",
    "    \n",
    "\n",
    "    # perform smooth Hill estimation\n",
    "    if verbose:\n",
    "        print(\"Calculating smooth Hill...\")\n",
    "    t1=time.time()\n",
    "    k_sh_arr, xi_sh_arr = smooth_hill_estimator(ordered_data,\n",
    "                                                r_smooth = r_smooth)\n",
    "    t2=time.time()\n",
    "    if verbose:\n",
    "        print(\"Elapsed time (smooth Hill):\", t2-t1)\n",
    "    if savedata == 1:\n",
    "        with open(os.path.join(output_dir+\"/\"+output_name+\"_sm_hill.dat\"), \"w\") as f:\n",
    "            for i in range(len(k_sh_arr)):\n",
    "                f.write(str(k_sh_arr[i]) + \" \" + str(xi_sh_arr[i]) + \"\\n\")\n",
    "    \n",
    "    \n",
    "    # perform adjusted Hill estimation\n",
    "    if verbose:\n",
    "        print(\"Calculating adjusted Hill...\")\n",
    "    t1 = time.time()\n",
    "    hill_results = hill_estimator(ordered_data,\n",
    "                                        bootstrap = bootstrap_flag,\n",
    "                                        t_bootstrap = t_bootstrap,\n",
    "                                        r_bootstrap = r_bootstrap,\n",
    "                                        diagn_plots = diagn_plots,\n",
    "                                        eps_stop = eps_stop, \n",
    "                                        verbose = verbose)\n",
    "    t2 =time.time()\n",
    "    if verbose:\n",
    "        print(\"Elapsed time (Hill):\", t2-t1)\n",
    "    k_h_arr = hill_results[0]\n",
    "    xi_h_arr = hill_results[1]\n",
    "    k_h_star = hill_results[2]\n",
    "    xi_h_star = hill_results[3]\n",
    "    x1_h_arr, n1_h_amse, k1_h, max_h_index1 = hill_results[4:8]\n",
    "    x2_h_arr, n2_h_amse, k2_h, max_h_index2 = hill_results[8:]\n",
    "    if savedata == 1:\n",
    "        with open(os.path.join(output_dir+\"/\"+output_name+\"_adj_hill_plot.dat\"), \"w\") as f:\n",
    "            for i in range(len(k_h_arr)):\n",
    "                f.write(str(k_h_arr[i]) + \" \" + str(xi_h_arr[i]) + \"\\n\")\n",
    "        with open(os.path.join(output_dir+\"/\"+output_name+\"_adj_hill_estimate.dat\"), \"w\") as f:\n",
    "            f.write(str(k_h_star) + \" \" + str(xi_h_star) + \"\\n\")\n",
    "\n",
    "    # perform moments estimation\n",
    "    if verbose:\n",
    "        print(\"Calculating moments...\")\n",
    "    t1 = time.time()\n",
    "    moments_results = moments_estimator(ordered_data,\n",
    "                                        bootstrap = bootstrap_flag,\n",
    "                                        t_bootstrap = t_bootstrap,\n",
    "                                        r_bootstrap = r_bootstrap,\n",
    "                                        diagn_plots = diagn_plots,\n",
    "                                        eps_stop = eps_stop, \n",
    "                                        verbose = verbose)\n",
    "    t2 = time.time()\n",
    "    if verbose:\n",
    "        print(\"Elapsed time (moments):\", t2-t1)\n",
    "    k_m_arr = moments_results[0]\n",
    "    xi_m_arr = moments_results[1]\n",
    "    k_m_star = moments_results[2]\n",
    "    xi_m_star = moments_results[3]\n",
    "    x1_m_arr, n1_m_amse, k1_m, max_m_index1 = moments_results[4:8]\n",
    "    x2_m_arr, n2_m_amse, k2_m, max_m_index2 = moments_results[8:]\n",
    "    if savedata == 1:\n",
    "        with open(os.path.join(output_dir+\"/\"+output_name+\"_mom_plot.dat\"), \"w\") as f:\n",
    "            for i in range(len(k_m_arr)):\n",
    "                f.write(str(k_m_arr[i]) + \" \" + str(xi_m_arr[i]) + \"\\n\")\n",
    "        with open(os.path.join(output_dir+\"/\"+output_name+\"_mom_estimate.dat\"), \"w\") as f:\n",
    "            f.write(str(k_m_star) + \" \" + str(xi_m_star) + \"\\n\")\n",
    "    # perform kernel-type estimation\n",
    "    if verbose:\n",
    "        print(\"Calculating kernel-type...\")\n",
    "    t1 = time.time()\n",
    "    kernel_type_results = kernel_type_estimator(ordered_data, hsteps,\n",
    "                                                alpha = alpha,\n",
    "                                                bootstrap = bootstrap_flag,\n",
    "                                                t_bootstrap = t_bootstrap,\n",
    "                                                r_bootstrap = r_bootstrap,\n",
    "                                                diagn_plots = diagn_plots,\n",
    "                                                eps_stop = eps_stop, \n",
    "                                                verbose = verbose)\n",
    "    t2 = time.time()\n",
    "    if verbose:\n",
    "        print(\"Elapsed time (kernel-type):\", t2-t1)\n",
    "    k_k_arr = kernel_type_results[0]\n",
    "    xi_k_arr = kernel_type_results[1]\n",
    "    k_k_star = kernel_type_results[2]\n",
    "    xi_k_star = kernel_type_results[3]\n",
    "    x1_k_arr, n1_k_amse, h1, max_k_index1 = kernel_type_results[4:8]\n",
    "    x2_k_arr, n2_k_amse, h2, max_k_index2 = kernel_type_results[8:]\n",
    "\n",
    "    if bootstrap_flag:\n",
    "        k_k1_star = np.argmin(np.abs(k_k_arr - k_k_star))\n",
    "    if savedata == 1:\n",
    "        with open(os.path.join(output_dir+\"/\"+output_name+\"_kern_plot.dat\"), \"w\") as f:\n",
    "            for i in range(len(k_k_arr)):\n",
    "                f.write(str(k_k_arr[i]) + \" \" + str(xi_k_arr[i]) + \"\\n\")\n",
    "        with open(os.path.join(output_dir+\"/\"+output_name+\"_kern_estimate.dat\"), \"w\") as f:\n",
    "            f.write(str(k_k_arr[k_k1_star]) + \" \" + str(xi_k_arr[k_k1_star]) + \"\\n\")\n",
    "    \n",
    "    # plotting part\n",
    "    if verbose:\n",
    "        print(\"Making plots...\")\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize = (12, 16))\n",
    "    for ax in axes.reshape(-1):\n",
    "        ax.tick_params(direction='out', length=6, width=1.5,\n",
    "                       labelsize = 12, which = 'major')\n",
    "        ax.tick_params(direction='out', length=3, width=1, which = 'minor')\n",
    "        [i.set_linewidth(1.5) for i in ax.spines.values()]\n",
    "\n",
    "    # plot PDF\n",
    "    axes[0,0].set_xlabel(r\"Degree $k$\", fontsize = 20)\n",
    "    axes[0,0].set_ylabel(r\"$P(k)$\", fontsize = 20)\n",
    "    axes[0,0].loglog(x_pdf, y_pdf, color = \"#386cb0\", marker = \"s\",\n",
    "                     lw = 1.5, markeredgecolor = \"black\")\n",
    "\n",
    "    # plot CCDF\n",
    "    axes[0,1].set_xlabel(r\"Degree $k$\", fontsize = 20)\n",
    "    axes[0,1].set_ylabel(r\"$\\bar{F}(k)$\", fontsize = 20)\n",
    "    axes[0,1].set_xscale(\"log\")\n",
    "    axes[0,1].set_yscale(\"log\")\n",
    "    axes[0,1].step(x_ccdf, y_ccdf, color = \"#386cb0\", lw = 1.5)\n",
    "    \n",
    "    # draw scalings\n",
    "    if noise_flag:\n",
    "        xmin = discrete_ordered_data[k_h_star]\n",
    "    else:\n",
    "        xmin = ordered_data[k_h_star]\n",
    "    x = x_ccdf[np.where(x_ccdf >= xmin)]\n",
    "    l = np.mean(y_ccdf[np.where(x == xmin)])\n",
    "    alpha = 1./xi_h_star\n",
    "    if xi_h_star > 0:\n",
    "        axes[0,1].plot(x, [l*(float(xmin)/k)**alpha for k in x],\n",
    "                       color = '#fb8072', ls = '--', lw = 2,\n",
    "                       label = r\"Adj. Hill Scaling $(\\alpha=\"+\\\n",
    "                       str(np.round(1./xi_h_star, decimals = 3))+r\")$\")\n",
    "        axes[0,1].plot((x[-1]), [l*(float(xmin)/x[-1])**(alpha)],\n",
    "                                   color = \"#fb8072\", ls = 'none', marker = 'o',\n",
    "                                   markerfacecolor = 'none', markeredgecolor = \"#fb8072\",\n",
    "                                   markeredgewidth = 3, markersize = 10)\n",
    "    if noise_flag:\n",
    "        xmin = discrete_ordered_data[k_m_star]\n",
    "    else:\n",
    "        xmin = ordered_data[k_m_star]\n",
    "    x = x_ccdf[np.where(x_ccdf >= xmin)]\n",
    "    l = np.mean(y_ccdf[np.where(x == xmin)])\n",
    "    alpha = 1./xi_m_star\n",
    "    if xi_m_star > 0:\n",
    "        axes[0,1].plot(x, [l*(float(xmin)/k)**alpha for k in x],\n",
    "                       color = '#8dd3c7', ls = '--', lw = 2,\n",
    "                       label = r\"Moments Scaling $(\\alpha=\"+\\\n",
    "                       str(np.round(1./xi_m_star, decimals = 3))+r\")$\")\n",
    "        axes[0,1].plot((x[-1]), [l*(float(xmin)/x[-1])**(alpha)],\n",
    "                                   color = \"#8dd3c7\", ls = 'none', marker = 'o',\n",
    "                                   markerfacecolor = 'none', markeredgecolor = \"#8dd3c7\",\n",
    "                                   markeredgewidth = 3, markersize = 10)\n",
    "    if noise_flag:\n",
    "        xmin = discrete_ordered_data[k_k_star]\n",
    "    else:\n",
    "        xmin = ordered_data[k_k_star]\n",
    "    \n",
    "    x = x_ccdf[np.where(x_ccdf >= xmin)]\n",
    "    l = np.mean(y_ccdf[np.where(x == xmin)])\n",
    "    alpha = 1./xi_k_star\n",
    "    if xi_k_star > 0:\n",
    "        axes[0,1].plot(x, [l*(float(xmin)/k)**alpha for k in x],\n",
    "                       color = '#fdb462', ls = '--', lw = 2,\n",
    "                       label = r\"Kernel Scaling $(\\alpha=\"+\\\n",
    "                       str(np.round(1./xi_k_star, decimals = 3))+r\")$\")\n",
    "        axes[0,1].plot((x[-1]), [l*(float(xmin)/x[-1])**(alpha)],\n",
    "                                   color = \"#8dd3c7\", ls = 'none', marker = 'o',\n",
    "                                   markerfacecolor = 'none', markeredgecolor = \"#fdb462\",\n",
    "                                   markeredgewidth = 3, markersize = 10)\n",
    "    axes[0,1].legend(loc = 'best')\n",
    "\n",
    "    # define min and max order statistics to plot\n",
    "    min_k = int(np.ceil(len(k_h_arr)**theta1)) - 1\n",
    "    max_k = int(np.floor(len(k_h_arr)**theta2)) - 1\n",
    "    # check if estimators' values are not too off in these bounds\n",
    "    min_k_index = (np.abs(k_sh_arr - min_k)).argmin()\n",
    "    max_k_index = (np.abs(k_sh_arr - max_k)).argmin()\n",
    "    if (xi_sh_arr[min_k_index] <= -3 or xi_sh_arr[min_k_index] >= 3):\n",
    "        indices_to_plot_sh = np.where((xi_sh_arr <= 3) & (xi_sh_arr >= -3))\n",
    "    elif (xi_sh_arr[max_k_index] <= -3 or xi_sh_arr[max_k_index] >= 3):\n",
    "        indices_to_plot_sh = np.where((xi_sh_arr <= 3) & (xi_sh_arr >= -3))\n",
    "    else:\n",
    "        indices_to_plot_sh = np.where((k_sh_arr <= max_k) & (k_sh_arr >= min_k))\n",
    "    axes[1,0].set_xlabel(r\"Number of Order Statistics $\\kappa$\", fontsize = 20)\n",
    "    axes[1,0].set_ylabel(r\"Estimated $\\hat{\\xi}$\", fontsize = 20)    \n",
    "    # plot smooth Hill\n",
    "    \n",
    "    axes[1,0].plot(k_sh_arr[indices_to_plot_sh], xi_sh_arr[indices_to_plot_sh],\n",
    "                   color = \"#b3de69\", alpha = 0.8, label = \"Smooth Hill\",\n",
    "                   zorder = 10)\n",
    "\n",
    "    # plot adjusted Hill\n",
    "    # check if estimators' values are not too off in these bounds\n",
    "    if (xi_h_arr[min_k-1] <= -3 or xi_h_arr[min_k-1] >= 3):\n",
    "        indices_to_plot_h = np.where((xi_h_arr <= 3) & (xi_h_arr >= -3))\n",
    "    elif (xi_h_arr[max_k-1] <= -3 or xi_h_arr[max_k-1] >= 3):\n",
    "        indices_to_plot_h = np.where((xi_h_arr <= 3) & (xi_h_arr >= -3))\n",
    "    else:\n",
    "        indices_to_plot_h = np.where((k_h_arr <= max_k) & (k_h_arr >= min_k))\n",
    "    axes[1,0].plot(k_h_arr[indices_to_plot_h], xi_h_arr[indices_to_plot_h],\n",
    "                   color = \"#fb8072\", alpha = 0.8, label = \"Adjusted Hill\",\n",
    "                   zorder = 10)\n",
    "    if bootstrap_flag:\n",
    "        axes[1,0].scatter([k_h_arr[k_h_star-1]], [xi_h_arr[k_h_star-1]],\n",
    "                       color = \"#fb8072\", marker = \"*\", s = 100,\n",
    "                       edgecolor = \"black\", zorder = 20, \n",
    "                       label = r\"$\\widehat{\\xi}^{Hill}=\"\\\n",
    "                           +str(np.round([xi_h_arr[k_h_star-1]][0], decimals = 3))\\\n",
    "                           +r\"$\")\n",
    "    axes[1,0].legend(loc = \"best\")\n",
    "\n",
    "    \n",
    "    axes[1,1].set_xlabel(r\"Number of Order Statistics $\\kappa$\", fontsize = 20)\n",
    "    axes[1,1].set_ylabel(r\"Estimated $\\hat{\\xi}$\", fontsize = 20) \n",
    "    axes[1,1].set_xscale(\"log\")   \n",
    "    \n",
    "    # plot smooth Hill\n",
    "    axes[1,1].plot(k_sh_arr[indices_to_plot_sh], xi_sh_arr[indices_to_plot_sh],\n",
    "                   color = \"#b3de69\", alpha = 0.8, label = \"Smooth Hill\",\n",
    "                   zorder = 10)\n",
    "    # plot adjusted Hill\n",
    "    indices_to_plot = np.where((k_h_arr <= max_k) & (k_h_arr >= min_k))\n",
    "    axes[1,1].plot(k_h_arr[indices_to_plot_h], xi_h_arr[indices_to_plot_h],\n",
    "                   color = \"#fb8072\", alpha = 0.8, label = \"Adjusted Hill\",\n",
    "                   zorder = 10)\n",
    "    if bootstrap_flag:\n",
    "        axes[1,1].scatter([k_h_arr[k_h_star-1]], [xi_h_arr[k_h_star-1]],\n",
    "                       color = \"#fb8072\", marker = \"*\", s = 100,\n",
    "                       edgecolor = \"black\", zorder = 20, \n",
    "                       label = r\"$\\widehat{\\xi}^{Hill}=\"\\\n",
    "                           +str(np.round([xi_h_arr[k_h_star-1]][0], decimals = 3))\\\n",
    "                           +r\"$\")\n",
    "    axes[1,1].legend(loc = \"best\")\n",
    "\n",
    "    axes[2,0].set_xlabel(r\"Number of Order Statistics $\\kappa$\", fontsize = 20)\n",
    "    axes[2,0].set_ylabel(r\"Estimated $\\hat{\\xi}$\", fontsize = 20)\n",
    "    #plot Pickands\n",
    "    min_k_index = (np.abs(k_p_arr - min_k)).argmin()\n",
    "    max_k_index = (np.abs(k_p_arr - max_k)).argmin()\n",
    "    if (xi_p_arr[min_k_index] <= -3 or xi_p_arr[min_k_index] >= 3):\n",
    "        indices_to_plot_p = np.where((xi_p_arr <= 3) & (xi_p_arr >= -3))\n",
    "    elif (xi_p_arr[max_k_index] <= -3 or xi_p_arr[max_k_index] >= 3):\n",
    "        indices_to_plot_p = np.where((xi_p_arr <= 3) & (xi_p_arr >= -3))\n",
    "    else:\n",
    "        indices_to_plot_p = np.where((k_p_arr <= max_k) & (k_p_arr >= min_k))\n",
    "    axes[2,0].plot(k_p_arr[indices_to_plot_p], xi_p_arr[indices_to_plot_p],\n",
    "                   color = \"#bc80bd\", alpha = 0.8, label = \"Pickands\",\n",
    "                   zorder = 10)\n",
    "    #plot moments\n",
    "    if (xi_m_arr[min_k-1] <= -3 or xi_m_arr[min_k-1] >= 3):\n",
    "        indices_to_plot_m = np.where((xi_m_arr <= 3) & (xi_m_arr >= -3))\n",
    "    elif (xi_m_arr[max_k-1] <= -3 or xi_m_arr[max_k-1] >= 3):\n",
    "        indices_to_plot_m = np.where((xi_m_arr <= 3) & (xi_m_arr >= -3))\n",
    "    else:\n",
    "        indices_to_plot_m = np.where((k_m_arr <= max_k) & (k_m_arr >= min_k))\n",
    "    \n",
    "    axes[2,0].plot(k_m_arr[indices_to_plot_m], xi_m_arr[indices_to_plot_m],\n",
    "                   color = \"#8dd3c7\", alpha = 0.8, label = \"Moments\",\n",
    "                   zorder = 10)\n",
    "    if bootstrap_flag:\n",
    "        axes[2,0].scatter([k_m_arr[k_m_star-1]], [xi_m_arr[k_m_star-1]],\n",
    "                       color = \"#8dd3c7\", marker = \"*\", s = 100,\n",
    "                       edgecolor = \"black\", zorder = 20, \n",
    "                       label = r\"$\\widehat{\\xi}^{Moments}=\"\\\n",
    "                           +str(np.round([xi_m_arr[k_m_star-1]][0], decimals = 3))\\\n",
    "                           +r\"$\")\n",
    "    #plot kernel-type\n",
    "    min_k_index = (np.abs(k_k_arr - min_k)).argmin()\n",
    "    max_k_index = (np.abs(k_k_arr - max_k)).argmin()\n",
    "    if (xi_k_arr[min_k_index] <= -3 or xi_k_arr[min_k_index] >= 3):\n",
    "        indices_to_plot_k = np.where((xi_k_arr <= 3) & (xi_k_arr >= -3))\n",
    "    elif (xi_k_arr[max_k_index] <= -3 or xi_k_arr[max_k_index] >= 3):\n",
    "        indices_to_plot_k = np.where((xi_k_arr <= 3) & (xi_k_arr >= -3))\n",
    "    else:\n",
    "        indices_to_plot_k = list(range(min_k_index, max_k_index))\n",
    "    #indices_to_plot_k = np.where((xi_k_arr <= 3) & (xi_k_arr >= -3))\n",
    "    axes[2,0].plot(k_k_arr[indices_to_plot_k], xi_k_arr[indices_to_plot_k],\n",
    "                   color = \"#fdb462\", alpha = 0.8, label = \"Kernel\",\n",
    "                   zorder = 10)\n",
    "    if bootstrap_flag:\n",
    "        axes[2,0].scatter([k_k_arr[k_k1_star-1]], [xi_k_arr[k_k1_star-1]],\n",
    "                       color = \"#fdb462\", marker = \"*\", s = 100,\n",
    "                       edgecolor = \"black\", zorder = 20, \n",
    "                       label = r\"$\\widehat{\\xi}^{Kernel}=\"\\\n",
    "                           +str(np.round([xi_k_arr[k_k1_star-1]][0], decimals = 3))\\\n",
    "                           +r\"$\")\n",
    "    axes[2,0].legend(loc = \"best\")\n",
    "    # for clarity purposes, display only xi region between -1 and 1\n",
    "    axes[2,0].set_ylim((-0.5,1.5))\n",
    "\n",
    "    axes[2,1].set_xlabel(r\"Number of Order Statistics $\\kappa$\", fontsize = 20)\n",
    "    axes[2,1].set_ylabel(r\"Estimated $\\hat{\\xi}$\", fontsize = 20)\n",
    "    axes[2,1].set_xscale(\"log\")\n",
    "\n",
    "    #plot Pickands\n",
    "    axes[2,1].plot(k_p_arr[indices_to_plot_p], xi_p_arr[indices_to_plot_p],\n",
    "                   color = \"#bc80bd\", alpha = 0.8, label = \"Pickands\",\n",
    "                   zorder = 10)\n",
    "    #plot moments\n",
    "    axes[2,1].plot(k_m_arr[indices_to_plot_m], xi_m_arr[indices_to_plot_m],\n",
    "                   color = \"#8dd3c7\", alpha = 0.8, label = \"Moments\",\n",
    "                   zorder = 10)\n",
    "    if bootstrap_flag:\n",
    "        axes[2,1].scatter([k_m_arr[k_m_star-1]], [xi_m_arr[k_m_star-1]],\n",
    "                       color = \"#8dd3c7\", marker = \"*\", s = 100,\n",
    "                       edgecolor = \"black\", zorder = 20, \n",
    "                       label = r\"$\\widehat{\\xi}^{Moments}=\"\\\n",
    "                           +str(np.round([xi_m_arr[k_m_star-1]][0], decimals = 3))\\\n",
    "                           +r\"$\")\n",
    "    #plot kernel-type\n",
    "    axes[2,1].plot(k_k_arr[indices_to_plot_k], xi_k_arr[indices_to_plot_k],\n",
    "                   color = \"#fdb462\", alpha = 0.8, label = \"Kernel\",\n",
    "                   zorder = 10)\n",
    "    if bootstrap_flag:\n",
    "        axes[2,1].scatter([k_k_arr[k_k1_star-1]], [xi_k_arr[k_k1_star-1]],\n",
    "                       color = \"#fdb462\", marker = \"*\", s = 100,\n",
    "                       edgecolor = \"black\", zorder = 20, \n",
    "                       label = r\"$\\widehat{\\xi}^{Kernel}=\"\\\n",
    "                           +str(np.round([xi_k_arr[k_k1_star-1]][0], decimals = 3))\\\n",
    "                           +r\"$\")\n",
    "    # for clarity purposes, display only xi region between -1 and 1\n",
    "    axes[2,1].set_ylim((-0.5,1.5))\n",
    "    axes[2,1].legend(loc = \"best\")\n",
    "\n",
    "    if diagn_plots:\n",
    "        fig_d, axes_d = plt.subplots(1, 3, figsize = (18, 6))\n",
    "\n",
    "        # filter out boundary values using theta parameters for Hill\n",
    "        min_k1 = 2\n",
    "        max_k1 = len(x1_h_arr) - 1\n",
    "        min_k2 = 2\n",
    "        max_k2 = len(x2_h_arr) - 1\n",
    "        axes_d[0].set_yscale(\"log\")\n",
    "        axes_d[0].set_xscale(\"log\")\n",
    "        axes_d[1].set_xscale(\"log\")\n",
    "        axes_d[2].set_xscale(\"log\")\n",
    "        n1_h_amse[np.where((n1_h_amse == np.inf) |\\\n",
    "                           (n1_h_amse == -np.inf))] = np.nan\n",
    "        axes_d[0].set_ylim((0.1*np.nanmin(n1_h_amse[min_k1:max_k1]), 1.0))\n",
    "        axes_d[0].set_xlabel(\"Fraction of Bootstrap Order Statistics\",\n",
    "                           fontsize = 20)\n",
    "        axes_d[0].set_ylabel(r\"$\\langle AMSE \\rangle$\", fontsize = 20)\n",
    "        axes_d[0].set_title(\"Adjusted Hill Estimator\", fontsize = 20)\n",
    "        # plot AMSE and corresponding minimum\n",
    "        axes_d[0].plot(x1_h_arr[min_k1:max_k1], n1_h_amse[min_k1:max_k1],\n",
    "                       alpha = 0.5, lw = 1.5,\n",
    "                       color = \"#d55e00\", label = r\"$n_1$ samples\")\n",
    "        axes_d[0].scatter([k1_h], [n1_h_amse[int(len(x1_h_arr)*k1_h)-1]],\n",
    "                          color = \"#d55e00\",\n",
    "                          marker = 'o', edgecolor = \"black\", alpha = 0.5,\n",
    "                          label = r\"Min for $n_1$ sample\")\n",
    "        axes_d[0].plot(x2_h_arr[min_k2:max_k2], n2_h_amse[min_k2:max_k2],\n",
    "                       alpha = 0.5, lw = 1.5,\n",
    "                       color = \"#0072b2\", label = r\"$n_2$ samples\")\n",
    "        axes_d[0].scatter([k2_h], [n2_h_amse[int(len(x2_h_arr)*k2_h)-1]],\n",
    "                          color = \"#0072b2\",\n",
    "                          marker = 'o', edgecolor = \"black\", alpha = 0.5,\n",
    "                          label = r\"Min for $n_2$ sample\")\n",
    "        axes_d[0].axvline(max_h_index1/float(len(x1_h_arr)), color = \"#d55e00\",\n",
    "                          ls = '--', alpha = 0.5,\n",
    "                          label = r\"Minimization boundary for $n_1$ sample\")\n",
    "        axes_d[0].axvline(max_h_index2/float(len(x2_h_arr)), color = \"#0072b2\",\n",
    "                          ls = '--', alpha = 0.5,\n",
    "                          label = r\"Minimization boundary for $n_2$ sample\")\n",
    "        \n",
    "\n",
    "        axes_d[0].legend(loc = \"best\")\n",
    "        if savedata == 1:\n",
    "            with open(os.path.join(output_dir+\"/\"+output_name+\"_adjhill_diagn1.dat\"), \"w\") as f:\n",
    "                for i in range(len(x1_h_arr[min_k1:max_k1])):\n",
    "                    f.write(str(x1_h_arr[min_k1:max_k1][i]) + \" \" + str(n1_h_amse[min_k1:max_k1][i]) + \"\\n\")\n",
    "            with open(os.path.join(output_dir+\"/\"+output_name+\"_adjhill_diagn2.dat\"), \"w\") as f:\n",
    "                for i in range(len(x2_h_arr[min_k2:max_k2])):\n",
    "                    f.write(str(x2_h_arr[min_k2:max_k2][i]) + \" \" + str(n2_h_amse[min_k2:max_k2][i]) + \"\\n\")\n",
    "            with open(os.path.join(output_dir+\"/\"+output_name+\"_adjhill_diagn_points.dat\"), \"w\") as f:\n",
    "                f.write(\"Min for n1 sample: \"+str(k1_h)+\" \"+str(n1_h_amse[int(len(x1_h_arr)*k1_h)-1])+\"\\n\")\n",
    "                f.write(\"Min for n2 sample: \"+str(k2_h)+\" \"+str(n2_h_amse[int(len(x2_h_arr)*k2_h)-1])+\"\\n\")\n",
    "                f.write(\"Minimization boundary for n1 sample: \"+str(max_h_index1/float(len(x1_h_arr)))+\"\\n\")\n",
    "                f.write(\"Minimization boundary for n2 sample: \"+str(max_h_index2/float(len(x2_h_arr)))+\"\\n\")\n",
    "\n",
    "        # filter out boundary values using theta parameters for moments\n",
    "        min_k1 = 2\n",
    "        max_k1 = len(x1_m_arr) - 1\n",
    "        min_k2 = 2\n",
    "        max_k2 = len(x2_m_arr) - 1\n",
    "        n1_m_amse[np.where((n1_m_amse == np.inf) |\\\n",
    "                           (n1_m_amse == -np.inf))] = np.nan\n",
    "        axes_d[1].set_yscale(\"log\")\n",
    "        axes_d[1].set_ylim((0.1*np.nanmin(n1_m_amse[min_k1:max_k1]), 1.0))\n",
    "        axes_d[1].set_xlabel(\"Fraction of Bootstrap Order Statistics\",\n",
    "                           fontsize = 20)\n",
    "        axes_d[1].set_ylabel(r\"$\\langle AMSE \\rangle$\", fontsize = 20)\n",
    "        axes_d[1].set_title(\"Moments Estimator\", fontsize = 20)\n",
    "        # plot AMSE and corresponding minimum\n",
    "        axes_d[1].plot(x1_m_arr[min_k1:max_k1], n1_m_amse[min_k1:max_k1],\n",
    "                       alpha = 0.5, lw = 1.5,\n",
    "                       color = \"#d55e00\", label = r\"$n_1$ samples\")\n",
    "        axes_d[1].scatter([k1_m], [n1_m_amse[int(len(x1_m_arr)*k1_m)-1]],\n",
    "                          color = \"#d55e00\",\n",
    "                          marker = 'o', edgecolor = \"black\", alpha = 0.5,\n",
    "                          label = r\"Min for $n_1$ sample\")\n",
    "        axes_d[1].plot(x2_m_arr[min_k2:max_k2], n2_m_amse[min_k2:max_k2],\n",
    "                       alpha = 0.5, lw = 1.5,\n",
    "                       color = \"#0072b2\", label = r\"$n_2$ samples\")\n",
    "        axes_d[1].scatter([k2_m], [n2_m_amse[int(len(x2_m_arr)*k2_m)-1]],\n",
    "                          color = \"#0072b2\",\n",
    "                          marker = 'o', edgecolor = \"black\", alpha = 0.5,\n",
    "                          label = r\"Min for $n_2$ sample\")\n",
    "        axes_d[1].axvline(max_m_index1/float(len(x1_m_arr)), color = \"#d55e00\",\n",
    "                          ls = '--', alpha = 0.5,\n",
    "                          label = r\"Minimization boundary for $n_1$ sample\")\n",
    "        axes_d[1].axvline(max_m_index2/float(len(x2_m_arr)), color = \"#0072b2\",\n",
    "                          ls = '--', alpha = 0.5,\n",
    "                          label = r\"Minimization boundary for $n_2$ sample\")\n",
    "        axes_d[1].legend(loc = \"best\")\n",
    "        if savedata == 1:\n",
    "            with open(os.path.join(output_dir+\"/\"+output_name+\"_mom_diagn1.dat\"), \"w\") as f:\n",
    "                for i in range(len(x1_m_arr[min_k1:max_k1])):\n",
    "                    f.write(str(x1_m_arr[min_k1:max_k1][i]) + \" \" + str(n1_m_amse[min_k1:max_k1][i]) + \"\\n\")\n",
    "            with open(os.path.join(output_dir+\"/\"+output_name+\"_mom_diagn2.dat\"), \"w\") as f:\n",
    "                for i in range(len(x2_m_arr[min_k2:max_k2])):\n",
    "                    f.write(str(x2_m_arr[min_k2:max_k2][i]) + \" \" + str(n2_m_amse[min_k2:max_k2][i]) + \"\\n\")\n",
    "            with open(os.path.join(output_dir+\"/\"+output_name+\"_mom_diagn_points.dat\"), \"w\") as f:\n",
    "                f.write(\"Min for n1 sample: \"+str(k1_m)+\" \"+str(n1_m_amse[int(len(x1_m_arr)*k1_m)-1])+\"\\n\")\n",
    "                f.write(\"Min for n2 sample: \"+str(k2_m)+\" \"+str(n2_m_amse[int(len(x2_m_arr)*k2_m)-1])+\"\\n\")\n",
    "                f.write(\"Minimization boundary for n1 sample: \"+str(max_m_index1/float(len(x1_m_arr)))+\"\\n\")\n",
    "                f.write(\"Minimization boundary for n2 sample: \"+str(max_m_index2/float(len(x2_m_arr)))+\"\\n\")        \n",
    "\n",
    "\n",
    "        min_k1 = 2\n",
    "        max_k1 = len(x1_k_arr)\n",
    "        min_k2 = 2\n",
    "        max_k2 = len(x2_k_arr)\n",
    "        n1_k_amse[np.where((n1_k_amse == np.inf) |\\\n",
    "                           (n1_k_amse == -np.inf))] = np.nan\n",
    "        axes_d[2].set_yscale(\"log\")\n",
    "        axes_d[2].set_ylim((0.1*np.nanmin(n1_k_amse[min_k1:max_k1]), 1.0))\n",
    "        axes_d[2].set_xlabel(\"Fraction of Bootstrap Order Statistics\",\n",
    "                           fontsize = 20)\n",
    "        axes_d[2].set_ylabel(r\"$\\langle AMSE \\rangle$\", fontsize = 20)\n",
    "        axes_d[2].set_title(\"Kernel-type Estimator\", fontsize = 20)\n",
    "        # plot AMSE and corresponding minimum\n",
    "        axes_d[2].plot(x1_k_arr[min_k1:max_k1], n1_k_amse[min_k1:max_k1],\n",
    "                       alpha = 0.5, lw = 1.5,\n",
    "                       color = \"#d55e00\", label = r\"$n_1$ samples\")\n",
    "        axes_d[2].scatter([h1], [n1_k_amse[np.where(x1_k_arr == h1)]], color = \"#d55e00\",\n",
    "                          marker = 'o', edgecolor = \"black\", alpha = 0.5,\n",
    "                          label = r\"Min for $n_1$ sample\")\n",
    "        # plot boundary of minimization\n",
    "        axes_d[2].axvline(max_k_index1, color = \"#d55e00\",\n",
    "                          ls = '--', alpha = 0.5,\n",
    "                          label = r\"Minimization boundary for $n_2$ sample\")\n",
    "        axes_d[2].plot(x2_k_arr[min_k2:max_k2], n2_k_amse[min_k2:max_k2],\n",
    "                       alpha = 0.5, lw = 1.5,\n",
    "                       color = \"#0072b2\", label = r\"$n_2$ samples\")\n",
    "        axes_d[2].scatter([h2], [n2_k_amse[np.where(x2_k_arr == h2)]], color = \"#0072b2\",\n",
    "                          marker = 'o', edgecolor = \"black\", alpha = 0.5,\n",
    "                          label = r\"Min for $n_2$ sample\")\n",
    "        axes_d[2].axvline(max_k_index2, color = \"#0072b2\",\n",
    "                          ls = '--', alpha = 0.5,\n",
    "                          label = r\"Minimization boundary for $n_2$ sample\")\n",
    "        axes_d[2].legend(loc = \"best\")\n",
    "        if savedata == 1:\n",
    "            with open(os.path.join(output_dir+\"/\"+output_name+\"_kern_diagn1.dat\"), \"w\") as f:\n",
    "                for i in range(len(x1_k_arr[min_k1:max_k1])):\n",
    "                    f.write(str(x1_k_arr[min_k1:max_k1][i]) + \" \" + str(n1_k_amse[min_k1:max_k1][i]) + \"\\n\")\n",
    "            with open(os.path.join(output_dir+\"/\"+output_name+\"_kern_diagn2.dat\"), \"w\") as f:\n",
    "                for i in range(len(x2_m_arr[min_k2:max_k2])):\n",
    "                    f.write(str(x2_k_arr[min_k2:max_k2][i]) + \" \" + str(n2_k_amse[min_k2:max_k2][i]) + \"\\n\")\n",
    "            with open(os.path.join(output_dir+\"/\"+output_name+\"_kern_diagn_points.dat\"), \"w\") as f:\n",
    "                f.write(\"Min for n1 sample: \"+str(h1)+\" \"+str(n1_k_amse[np.where(x1_k_arr == h1)][0])+\"\\n\")\n",
    "                f.write(\"Min for n2 sample: \"+str(h2)+\" \"+str(n2_k_amse[np.where(x2_k_arr == h2)][0])+\"\\n\")\n",
    "                f.write(\"Minimization boundary for n1 sample: \"+str(n1_k_amse[int(max_k_index1*hsteps)-1])+\"\\n\")\n",
    "                f.write(\"Minimization boundary for n2 sample: \"+str(n2_k_amse[int(max_k_index2*hsteps)-1])+\"\\n\")\n",
    "\n",
    "        fig_d.tight_layout()\n",
    "        diag_plots_path = output_dir+\"/\"+output_name+\"_diag.pdf\"\n",
    "        fig_d.savefig(diag_plots_path)\n",
    "\n",
    "    fig.tight_layout(pad = 0.2)\n",
    "    fig.savefig(output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
